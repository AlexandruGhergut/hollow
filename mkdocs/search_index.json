{
    "docs": [
        {
            "location": "/", 
            "text": "The Problem\n\n\nSoftware engineers often encounter problems which require the dissemination of small or moderately sized data sets which don\u2019t fit the label \u201cbig data\u201d.  To solve these problems, we often send the data to an RDBMS or nosql data store and query it at runtime, or serialize the data as json or xml, distribute it, and keep a local copy on each consumer.\n\n\nScaling each of these solutions presents different challenges.  Sending the data to an RDBMS, nosql data store, or even a memcached cluster may allow your dataset to grow indefinitely large, but there are limitations on the latency and frequency with which you can interact with that dataset.  Serializing and keeping a local copy (if in RAM) can allow many orders of magnitude lower latency and higher frequency access, but this approach has many scaling challenges:\n\n\n\n\nThe dataset size is limited by available RAM.\n\n\nThe full dataset may need to be re-downloaded each time it is updated.\n\n\nUpdating the dataset may require significant CPU resources or impact GC behavior.\n\n\n\n\nNetflix, serving many billions of personalized requests each day, has a few use cases for which the latency of a remote datastore would be highly undesirable given the frequency with which those datasets are accessed.\n\n\nThe Solution\n\n\nNetflix Hollow is a java library and comprehensive toolset for harnessing small to moderately sized in-memory datasets which are disseminated from a single producer to many consumers for read-only access.  Hollow aggressively addresses the scaling challenges of in-memory datasets, and is built with servers busily serving requests at or near maximum capacity in mind.\n\n\nDue to its performance characteristics, Hollow shifts the scale in terms of appropriate dataset sizes for an in-memory solution.  Datasets for which such liberation may never previously have been considered can be candidates for Hollow.  \n\n\n\n\nSmall to Medium Datasets\n\n\nHollow may be entirely appropriate for datasets which, if represented with json or XML, might require in excess of 100GB.  A good rule of thumb in 2017: KB, MB, and often GB, but not TB or PB.\n\n\n\n\nHollow simultaneously targets three goals:\n\n\n\n\nMaximum development agility\n\n\nHighly optimized performance and resource management\n\n\nExtreme stability and reliability\n\n\n\n\nAgility\n\n\nHollow provides the capability to automatically generate a custom API based on a specific data model, so that consumers can intuitively interact with the data, with the benefit of IDE code completion.\n\n\nHollow provides insight tools, which will help users understand their dataset, and how it changes over time, more deeply than ever before:\n\n\n\n\nComprehensive change history\n\n\nDiffing entire datasets between arbitrary states\n\n\nHeap usage analysis\n\n\nUsage tracking\n\n\n\n\nThe toolset available for working with Hollow datasets allows for a surprising variety of operations to be performed with ease, including:\n\n\n\n\nIndexing / Querying for individual records in a dataset\n\n\nSplitting / Combining entire datasets in many different ways\n\n\nFiltering individual record types at the consumer to reduce heap footprint\n\n\n\n\nPerformance\n\n\nHollow is hyper-optimized with a few performance metrics at top-of-mind:\n\n\n\n\nHeap footprint\n\n\nComputational cost of access\n\n\nGC impact of updates\n\n\nComputational cost of updates\n\n\nNetwork cost of updates\n\n\n\n\nOver time, Hollow automatically calculates the changes in a dataset on the producer.  Instead of retransmitting the entire snapshot of the data for each update, only the changes are disseminated to consumers to keep them up to date.\n\n\nOn consumers, Hollow keeps a compact encoding of the dataset in RAM.  This representation is optimized for both minimizing heap footprint and minimizing access CPU cost.  To retain and keep the dataset updated, Hollow pools and reuses heap memory to avoid GC tenuring.\n\n\nStability\n\n\nHollow has been battle-hardened over more than two years of continuous use at Netflix.  Hollow is used to represent crucial datasets, essential to the fulfillment of the Netflix experience, on servers answering live customer requests.  So although Hollow goes to extraordinary lengths to squeeze every last bit of performance out of servers\u2019 hardware, enormous attention to detail has gone into solidifying this foundational piece of our infrastructure.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#the-problem", 
            "text": "Software engineers often encounter problems which require the dissemination of small or moderately sized data sets which don\u2019t fit the label \u201cbig data\u201d.  To solve these problems, we often send the data to an RDBMS or nosql data store and query it at runtime, or serialize the data as json or xml, distribute it, and keep a local copy on each consumer.  Scaling each of these solutions presents different challenges.  Sending the data to an RDBMS, nosql data store, or even a memcached cluster may allow your dataset to grow indefinitely large, but there are limitations on the latency and frequency with which you can interact with that dataset.  Serializing and keeping a local copy (if in RAM) can allow many orders of magnitude lower latency and higher frequency access, but this approach has many scaling challenges:   The dataset size is limited by available RAM.  The full dataset may need to be re-downloaded each time it is updated.  Updating the dataset may require significant CPU resources or impact GC behavior.   Netflix, serving many billions of personalized requests each day, has a few use cases for which the latency of a remote datastore would be highly undesirable given the frequency with which those datasets are accessed.", 
            "title": "The Problem"
        }, 
        {
            "location": "/#the-solution", 
            "text": "Netflix Hollow is a java library and comprehensive toolset for harnessing small to moderately sized in-memory datasets which are disseminated from a single producer to many consumers for read-only access.  Hollow aggressively addresses the scaling challenges of in-memory datasets, and is built with servers busily serving requests at or near maximum capacity in mind.  Due to its performance characteristics, Hollow shifts the scale in terms of appropriate dataset sizes for an in-memory solution.  Datasets for which such liberation may never previously have been considered can be candidates for Hollow.     Small to Medium Datasets  Hollow may be entirely appropriate for datasets which, if represented with json or XML, might require in excess of 100GB.  A good rule of thumb in 2017: KB, MB, and often GB, but not TB or PB.   Hollow simultaneously targets three goals:   Maximum development agility  Highly optimized performance and resource management  Extreme stability and reliability", 
            "title": "The Solution"
        }, 
        {
            "location": "/#agility", 
            "text": "Hollow provides the capability to automatically generate a custom API based on a specific data model, so that consumers can intuitively interact with the data, with the benefit of IDE code completion.  Hollow provides insight tools, which will help users understand their dataset, and how it changes over time, more deeply than ever before:   Comprehensive change history  Diffing entire datasets between arbitrary states  Heap usage analysis  Usage tracking   The toolset available for working with Hollow datasets allows for a surprising variety of operations to be performed with ease, including:   Indexing / Querying for individual records in a dataset  Splitting / Combining entire datasets in many different ways  Filtering individual record types at the consumer to reduce heap footprint", 
            "title": "Agility"
        }, 
        {
            "location": "/#performance", 
            "text": "Hollow is hyper-optimized with a few performance metrics at top-of-mind:   Heap footprint  Computational cost of access  GC impact of updates  Computational cost of updates  Network cost of updates   Over time, Hollow automatically calculates the changes in a dataset on the producer.  Instead of retransmitting the entire snapshot of the data for each update, only the changes are disseminated to consumers to keep them up to date.  On consumers, Hollow keeps a compact encoding of the dataset in RAM.  This representation is optimized for both minimizing heap footprint and minimizing access CPU cost.  To retain and keep the dataset updated, Hollow pools and reuses heap memory to avoid GC tenuring.", 
            "title": "Performance"
        }, 
        {
            "location": "/#stability", 
            "text": "Hollow has been battle-hardened over more than two years of continuous use at Netflix.  Hollow is used to represent crucial datasets, essential to the fulfillment of the Netflix experience, on servers answering live customer requests.  So although Hollow goes to extraordinary lengths to squeeze every last bit of performance out of servers\u2019 hardware, enormous attention to detail has gone into solidifying this foundational piece of our infrastructure.", 
            "title": "Stability"
        }, 
        {
            "location": "/quick-start/", 
            "text": "Hollow has an available \nreference implementation\n, which is designed to get you up and running with a demo in minutes.  Then, we'll walk through swapping in a fully functional, production scalable, AWS-based infrastructure implementation in about an hour.\n\n\nThe reference implementation is a great starting point to integrate Hollow for your use case; it contains a simple mocked-up data model, which you can easily modify to suit your needs.\n\n\n\n\nLearn by Doing\n\n\nThis \nQuick Start Guide\n is placed at the beginning of the documentation as a starting point for those who prefer to \"learn by doing\".  If you'd prefer to gain a greater understanding of how everything fits together prior to jumping in, skip ahead to the \nGetting Started\n guide, but do come back later.  \n\n\n\n\nRunning the Demo\n\n\nClone the Reference Implementation\n\n\nStart by cloning the \nnetflix-hollow-reference-implementation\n repo on GitHub:\n\n\ngit clone https://github.com/Netflix/hollow-reference-implementation.git\n\n\n\n\nImport into your IDE\n\n\nImport the project into your IDE.  This project ships with both a \nbuild.gradle\n file and a \npom.xml\n file, so you can either use a gradle plugin or a standard maven plugin to import the dependencies.\n\n\n\n\nDependencies in Hollow\n\n\nThe core Hollow jar does not require or include \nany\n third party dependencies.  The dependencies in the \nreference implementation\n are required for infrastructure demonstration purposes.\n\n\n\n\nStart a Producer\n\n\nThe class \ncom.netflix.hollow.example.Producer\n contains a main method.  Run it.  This will be our data \nproducer\n, and will write data to a directory \npublish-dir\n underneath the temp directory.\n\n\nYou should see output like this:\n\n\nI AM THE PRODUCER.  I WILL PUBLISH TO /tmp/publish-dir\nATTEMPTING TO RESTORE PRIOR STATE...\nRESTORE NOT AVAILABLE\nBeginning cycle 20161110185218001\nBeginning cycle 20161110185228002\nBeginning cycle 20161110185238003\n\n\n\n\nAnd you should have a folder \n/tmp/publish-dir\n with contents like this:\n\n\n    17 announced.version\n   370 delta-20161110185218001-20161110185228002\n   604 delta-20161110185228002-20161110185238003\n   385 reversedelta-20161110185228002-20161110185218001\n   551 reversedelta-20161110185238003-20161110185228002\n597567 snapshot-20161110185218001\n597688 snapshot-20161110185228002\n597813 snapshot-20161110185238003\n\n\n\n\nStart a Consumer\n\n\nThe class \ncom.netflix.hollow.example.Consumer\n also contains a main method.  Run it.  This will be our data \nconsumer\n, and will read data from the directory \npublish-dir\n underneath the temp directory.  \n\n\nYou should see output like this:\n\n\nI AM THE CONSUMER.  I WILL READ FROM /tmp/publish-dir\nSNAPSHOT COMPLETED IN 45ms\nTYPES: [Actor, Movie, SetOfActor, String]\nDELTA COMPLETED IN 20ms\nTYPES: [Movie, SetOfActor, String]\nSNAPSHOT COMPLETED IN 1ms\nTYPES: [Actor, Movie]\nDELTA COMPLETED IN 12ms\nTYPES: [Actor, Movie, SetOfActor, String]\nDELTA COMPLETED IN 0ms\nTYPES: [Movie]\n\n\n\n\n\nInspecting the Data\n\n\nThat's it!  You now have a producer, which is reading data from a data source and publishing Hollow data to \n/tmp/publish-dir\n, and a consumer which is reading that data and keeping its internal state up to date.  Let's take a look at what the data actually looks like, and how it's changing over time.  The consumer started a \nhistory\n server, so open up a browser and visit \nhttp://localhost:7777\n.\n\n\n\n\nFake Data\n\n\nIn this demo case, there is no data source.  Instead, the producer is manufacturing some randomized fake data on the first cycle, and making some minor random modifications to that data for each cycle.  Inspect the package \ncom.netflix.hollow.example.producer.datamodel\n to see the mock data model and how it is randomized.\n\n\n\n\nPlugging in Infrastructure\n\n\nThe demo we have so far is fully functional, but writing to and reading from a local disk directory is probably not realistic for a production implementation.  In the following few sections, we'll actually set up a simple AWS-based infrastructure which could easily scale to any production deployment.\n\n\n\n\nUsing AWS\n\n\nThis demonstration uses AWS because it is accessible to anyone, easy to set up, and extremely reliable.  In order to proceed, you'll need \nan AWS account\n, which is free to sign up for -- you only pay for the services you use.\n\n\nEven if you're not able to use the prescribed AWS-based infrastructure, running through the following steps will be useful to gain an understanding of how to swap in different infrastructure implementations for use with Hollow.\n\n\n\n\nCreate a User\n\n\nOnce you've logged into your AWS account, select \nIdentity \n Access Management\n from the AWS console:\n\n\n\n\nSelect \nUsers\n, then \nCreate New Users\n.  Enter a name in the first box (e.g. \nHollowReference\n).\n\n\n\n\nMake sure the checkbox to \nGenerate an access key for each user\n is selected.  Click \nCreate\n in the bottom right corner of the screen.\n\n\nOn the next page, Click \nShow User Security Credentials\n.  Here you'll see an \nAccess Key ID\n and a \nSecret Access Key\n.  Copy both of these strings and set them off to the side, we'll need them soon.\n\n\nAssign Permissions To User\n\n\nWe'll need \nS3\n and \nDynamoDB\n access for this user.  Back on the \nIdentity \n Access Management: Users\n page, select the user we just created.  Under the \nPermissions\n tab, select \nAttach Policy\n:\n\n\n\n\nFor now, let's give this user full access to S3.  Select the policy named \nAmazonS3FullAccess\n:\n\n\n\n\nClick \nAttach Policy\n in the bottom right corner of the screen.\n\n\nRepeat these steps to add the policy \nAmazonDynamoDBFullAccess\n to this user.\n\n\nCreate an S3 Bucket\n\n\nBack on the AWS console landing page, select S3:\n\n\n\n\nClick \nCreate Bucket\n, then create a bucket.  Select a unique name for your bucket.  Your bucket can be in any region, here we're using the \nUS Standard\n region:\n\n\n\n\nClick \nCreate\n.\n\n\nPlug the Producer into S3\n\n\nNow that we've set up our user and our S3 bucket, we can plug the producer into S3.  Open up the \nProducer\n class in the Hollow reference implementation project.  Modify the \nmain\n method: swap the \nPublisher\n and \nAnnouncer\n implementations as follows, replacing \nzzz-hollow-reference-implementation\n with the bucket name you chose in the prior step:\n\n\nAWSCredentials credentials = \n                new BasicAWSCredentials(\nAccess Key ID\n, \nSecret Access Key\n);\n\nPublisher publisher = \n                new S3Publisher(credentials, \nzzz-hollow-reference-implementation\n, \ndemo\n);\nAnnouncer announcer = \n                new S3Announcer(credentials, \nzzz-hollow-reference-implementation\n, \ndemo\n);\n\nProducer producer = new Producer(publisher, announcer);\n\n\n\n\nStart the producer.  After a few cycles, inspect the \ndemo\n folder in your S3 bucket to see what the producer is writing.\n\n\n\n\nWatch your AWS Usage\n\n\nThe AWS meter is running.  Leaving this version of the producer running overnight could run up an AWS bill.  Shut the demo down once you're done!\n\n\n\n\n\n\nClean Up After Yourself\n\n\nIf you do ultimately choose to use S3 as your \nblob storage\n infrastructure, be aware that this implementation does not automatically clean up data for you, which can result in increasing AWS bills.  You should have a cleanup strategy, which can be as simple as adding a \nLifecycle Rule\n to your bucket which will, for example, delete old data after 30 days.\n\n\n\n\nPlug the Consumer into S3\n\n\nNow that our producer is writing data into S3, we need our consumers to read that data.  Open up the \nConsumer\n class in the Hollow reference implementation project.  Modify the \nmain\n method: swap the \nHollowBlobRetriever\n and \nHollowAnnouncementWatcher\n implementations as follows, replacing \nzzz-hollow-reference-implementation\n with the bucket name you chose in the prior step:\n\n\nAWSCredentials credentials = \n                new BasicAWSCredentials(\nAccess Key ID\n, \nSecret Access Key\n);\n\nHollowBlobRetriever blobRetriever = \n        new S3BlobRetriever(credentials, \nzzz-hollow-reference-implementation\n, \ndemo\n);\nHollowAnnouncementWatcher announcementWatcher = \n        new S3AnnouncementWatcher(credentials, \nzzz-hollow-reference-implementation\n, \ndemo\n);\n\nConsumer consumer = new Consumer(blobRetriever, announcementWatcher);\n\n\n\n\n\nStart the consumer.  You'll see the same output from the demo step, except now the Hollow input data is coming from S3.  \n\n\nAt this point, we have a fully distributed infrastructure.  You can start the producer on one machine, and start a consumer on many other machines, anywhere in the world.  Each of the consumers will update in lock-step each time the producer publishes changes.\n\n\n\n\nAddress already in use Exception\n\n\nNote that the \nConsumer\n attempts to start a \nhistory\n server on port 7777.  Because of this, only one \nConsumer\n can be running on a single machine at one time.  If you get a \njava.net.BindException\n, shut down the other \nConsumer\n and try again.\n\n\n\n\n\n\nPublishing Closer to the Consumer\n\n\nOur implementation currently publishes to a single S3 bucket.  This is OK, but S3 buckets reside in specific AWS regions, and it is often beneficial to publish data closer to your consumers.  For example, if you have some consumers in the AWS region \nUS Standard\n, and some in the region \nSydney\n, then it makes sense to simultaneously publish to one bucket in each region, and have consumers read from the bucket closest to them.\n\n\n\n\nA Better Announcement Infrastructure\n\n\nOur distributed infrastructure thus far leverages S3 for both a \nblob store\n mechanism, and an \nannouncement\n mechanism.  Although S3 is perfectly suitable for blob storage, it's less well suited for announcement.  Instead, we can leverage DynamoDB for the announcement infrastructure, and achieve both improved scalability and better economics.\n\n\nFirst, we need to create a DynamoDB table.  Back on the AWS console landing page, select DynamoDB:\n\n\n\n\nSelect \nCreate table\n and enter a Table name (e.g. \nHollowAnnouncement\n) and use \nnamespace\n as the \nPartition key\n:\n\n\n\n\nSelect \nCreate\n.\n\n\nPlug the Producer into DynamoDB\n\n\nNow that we've set up our DynamoDB table, let's swap it into our producer as our announcement mechanism.  Go back to the \nProducer\n class and modify the \nmain\n method, swapping the \nAnnouncer\n implementation as follows:\n\n\nAnnouncer announcer = new DynamoDBAnnouncer(credentials, \nHollowAnnouncement\n, \ndemo\n);\n\n\n\n\nStart the producer.  After at least one cycle, you'll be able to scan the table and see a record indicating the currently announced version for our \ndemo\n namespace:\n\n\n\n\nPlug the Consumer into DynamoDB\n\n\nNow that our producer is announcing to DynamoDB, of course our consumer needs to look there for update directions.  Go back to the \nConsumer\n class and modify the \nmain\n method, swapping the \nHollowAnnouncementWatcher\n implementation as follows:\n\n\nHollowAnnouncementWatcher announcementWatcher = \n            new DynamoDBAnnouncementWatcher(credentials, \nHollowAnnouncement\n, \ndemo\n);\n\n\n\n\nRun the consumer.  You'll see the same output from prior steps, except now the Hollow input data is coming from S3, and the state version announcement is read from DynamoDB.\n\n\n\n\nPinning The State\n\n\nThe provided \nDynamoDBAnnouncementWatcher\n can be \npinned\n.  If the announcement item in DynamoDB contains a field \npin_version\n, then the consumer will go to the version indicated in that field, instead of the version the producer announces.  See \nPinning Consumers\n for more details about pinning data.\n\n\n\n\nPlug In Your Data Model\n\n\nCongratulations!  You now have a living, breathing, fully production scalable implementation of Hollow in your hands.  All that's left to do is substitute your data model for the example provided.  The data model can be defined, as in this example, with a set of POJO classes.  Take this starting point and make it your own -- remove the classes underneath \ncom.netflix.hollow.example.producer.datamodel\n and replace them with your data model.\n\n\n\n\nModeling Data with POJOs\n\n\nThe \nHollowObjectMapper\n section of this document provides details about how to convert your POJO data model into Hollow.\n\n\n\n\nContinue on to the \nGetting Started\n guide to learn more about how the fundamental pieces of Hollow fit together, how to create a consumer API custom-tailored to your data model, and how to index your data for easy, efficient retrieval by consumers.", 
            "title": "Quick Start"
        }, 
        {
            "location": "/quick-start/#running-the-demo", 
            "text": "", 
            "title": "Running the Demo"
        }, 
        {
            "location": "/quick-start/#clone-the-reference-implementation", 
            "text": "Start by cloning the  netflix-hollow-reference-implementation  repo on GitHub:  git clone https://github.com/Netflix/hollow-reference-implementation.git", 
            "title": "Clone the Reference Implementation"
        }, 
        {
            "location": "/quick-start/#import-into-your-ide", 
            "text": "Import the project into your IDE.  This project ships with both a  build.gradle  file and a  pom.xml  file, so you can either use a gradle plugin or a standard maven plugin to import the dependencies.   Dependencies in Hollow  The core Hollow jar does not require or include  any  third party dependencies.  The dependencies in the  reference implementation  are required for infrastructure demonstration purposes.", 
            "title": "Import into your IDE"
        }, 
        {
            "location": "/quick-start/#start-a-producer", 
            "text": "The class  com.netflix.hollow.example.Producer  contains a main method.  Run it.  This will be our data  producer , and will write data to a directory  publish-dir  underneath the temp directory.  You should see output like this:  I AM THE PRODUCER.  I WILL PUBLISH TO /tmp/publish-dir\nATTEMPTING TO RESTORE PRIOR STATE...\nRESTORE NOT AVAILABLE\nBeginning cycle 20161110185218001\nBeginning cycle 20161110185228002\nBeginning cycle 20161110185238003  And you should have a folder  /tmp/publish-dir  with contents like this:      17 announced.version\n   370 delta-20161110185218001-20161110185228002\n   604 delta-20161110185228002-20161110185238003\n   385 reversedelta-20161110185228002-20161110185218001\n   551 reversedelta-20161110185238003-20161110185228002\n597567 snapshot-20161110185218001\n597688 snapshot-20161110185228002\n597813 snapshot-20161110185238003", 
            "title": "Start a Producer"
        }, 
        {
            "location": "/quick-start/#start-a-consumer", 
            "text": "The class  com.netflix.hollow.example.Consumer  also contains a main method.  Run it.  This will be our data  consumer , and will read data from the directory  publish-dir  underneath the temp directory.    You should see output like this:  I AM THE CONSUMER.  I WILL READ FROM /tmp/publish-dir\nSNAPSHOT COMPLETED IN 45ms\nTYPES: [Actor, Movie, SetOfActor, String]\nDELTA COMPLETED IN 20ms\nTYPES: [Movie, SetOfActor, String]\nSNAPSHOT COMPLETED IN 1ms\nTYPES: [Actor, Movie]\nDELTA COMPLETED IN 12ms\nTYPES: [Actor, Movie, SetOfActor, String]\nDELTA COMPLETED IN 0ms\nTYPES: [Movie]", 
            "title": "Start a Consumer"
        }, 
        {
            "location": "/quick-start/#inspecting-the-data", 
            "text": "That's it!  You now have a producer, which is reading data from a data source and publishing Hollow data to  /tmp/publish-dir , and a consumer which is reading that data and keeping its internal state up to date.  Let's take a look at what the data actually looks like, and how it's changing over time.  The consumer started a  history  server, so open up a browser and visit  http://localhost:7777 .   Fake Data  In this demo case, there is no data source.  Instead, the producer is manufacturing some randomized fake data on the first cycle, and making some minor random modifications to that data for each cycle.  Inspect the package  com.netflix.hollow.example.producer.datamodel  to see the mock data model and how it is randomized.", 
            "title": "Inspecting the Data"
        }, 
        {
            "location": "/quick-start/#plugging-in-infrastructure", 
            "text": "The demo we have so far is fully functional, but writing to and reading from a local disk directory is probably not realistic for a production implementation.  In the following few sections, we'll actually set up a simple AWS-based infrastructure which could easily scale to any production deployment.   Using AWS  This demonstration uses AWS because it is accessible to anyone, easy to set up, and extremely reliable.  In order to proceed, you'll need  an AWS account , which is free to sign up for -- you only pay for the services you use.  Even if you're not able to use the prescribed AWS-based infrastructure, running through the following steps will be useful to gain an understanding of how to swap in different infrastructure implementations for use with Hollow.", 
            "title": "Plugging in Infrastructure"
        }, 
        {
            "location": "/quick-start/#create-a-user", 
            "text": "Once you've logged into your AWS account, select  Identity   Access Management  from the AWS console:   Select  Users , then  Create New Users .  Enter a name in the first box (e.g.  HollowReference ).   Make sure the checkbox to  Generate an access key for each user  is selected.  Click  Create  in the bottom right corner of the screen.  On the next page, Click  Show User Security Credentials .  Here you'll see an  Access Key ID  and a  Secret Access Key .  Copy both of these strings and set them off to the side, we'll need them soon.", 
            "title": "Create a User"
        }, 
        {
            "location": "/quick-start/#assign-permissions-to-user", 
            "text": "We'll need  S3  and  DynamoDB  access for this user.  Back on the  Identity   Access Management: Users  page, select the user we just created.  Under the  Permissions  tab, select  Attach Policy :   For now, let's give this user full access to S3.  Select the policy named  AmazonS3FullAccess :   Click  Attach Policy  in the bottom right corner of the screen.  Repeat these steps to add the policy  AmazonDynamoDBFullAccess  to this user.", 
            "title": "Assign Permissions To User"
        }, 
        {
            "location": "/quick-start/#create-an-s3-bucket", 
            "text": "Back on the AWS console landing page, select S3:   Click  Create Bucket , then create a bucket.  Select a unique name for your bucket.  Your bucket can be in any region, here we're using the  US Standard  region:   Click  Create .", 
            "title": "Create an S3 Bucket"
        }, 
        {
            "location": "/quick-start/#plug-the-producer-into-s3", 
            "text": "Now that we've set up our user and our S3 bucket, we can plug the producer into S3.  Open up the  Producer  class in the Hollow reference implementation project.  Modify the  main  method: swap the  Publisher  and  Announcer  implementations as follows, replacing  zzz-hollow-reference-implementation  with the bucket name you chose in the prior step:  AWSCredentials credentials = \n                new BasicAWSCredentials( Access Key ID ,  Secret Access Key );\n\nPublisher publisher = \n                new S3Publisher(credentials,  zzz-hollow-reference-implementation ,  demo );\nAnnouncer announcer = \n                new S3Announcer(credentials,  zzz-hollow-reference-implementation ,  demo );\n\nProducer producer = new Producer(publisher, announcer);  Start the producer.  After a few cycles, inspect the  demo  folder in your S3 bucket to see what the producer is writing.   Watch your AWS Usage  The AWS meter is running.  Leaving this version of the producer running overnight could run up an AWS bill.  Shut the demo down once you're done!    Clean Up After Yourself  If you do ultimately choose to use S3 as your  blob storage  infrastructure, be aware that this implementation does not automatically clean up data for you, which can result in increasing AWS bills.  You should have a cleanup strategy, which can be as simple as adding a  Lifecycle Rule  to your bucket which will, for example, delete old data after 30 days.", 
            "title": "Plug the Producer into S3"
        }, 
        {
            "location": "/quick-start/#plug-the-consumer-into-s3", 
            "text": "Now that our producer is writing data into S3, we need our consumers to read that data.  Open up the  Consumer  class in the Hollow reference implementation project.  Modify the  main  method: swap the  HollowBlobRetriever  and  HollowAnnouncementWatcher  implementations as follows, replacing  zzz-hollow-reference-implementation  with the bucket name you chose in the prior step:  AWSCredentials credentials = \n                new BasicAWSCredentials( Access Key ID ,  Secret Access Key );\n\nHollowBlobRetriever blobRetriever = \n        new S3BlobRetriever(credentials,  zzz-hollow-reference-implementation ,  demo );\nHollowAnnouncementWatcher announcementWatcher = \n        new S3AnnouncementWatcher(credentials,  zzz-hollow-reference-implementation ,  demo );\n\nConsumer consumer = new Consumer(blobRetriever, announcementWatcher);  Start the consumer.  You'll see the same output from the demo step, except now the Hollow input data is coming from S3.    At this point, we have a fully distributed infrastructure.  You can start the producer on one machine, and start a consumer on many other machines, anywhere in the world.  Each of the consumers will update in lock-step each time the producer publishes changes.   Address already in use Exception  Note that the  Consumer  attempts to start a  history  server on port 7777.  Because of this, only one  Consumer  can be running on a single machine at one time.  If you get a  java.net.BindException , shut down the other  Consumer  and try again.    Publishing Closer to the Consumer  Our implementation currently publishes to a single S3 bucket.  This is OK, but S3 buckets reside in specific AWS regions, and it is often beneficial to publish data closer to your consumers.  For example, if you have some consumers in the AWS region  US Standard , and some in the region  Sydney , then it makes sense to simultaneously publish to one bucket in each region, and have consumers read from the bucket closest to them.", 
            "title": "Plug the Consumer into S3"
        }, 
        {
            "location": "/quick-start/#a-better-announcement-infrastructure", 
            "text": "Our distributed infrastructure thus far leverages S3 for both a  blob store  mechanism, and an  announcement  mechanism.  Although S3 is perfectly suitable for blob storage, it's less well suited for announcement.  Instead, we can leverage DynamoDB for the announcement infrastructure, and achieve both improved scalability and better economics.  First, we need to create a DynamoDB table.  Back on the AWS console landing page, select DynamoDB:   Select  Create table  and enter a Table name (e.g.  HollowAnnouncement ) and use  namespace  as the  Partition key :   Select  Create .", 
            "title": "A Better Announcement Infrastructure"
        }, 
        {
            "location": "/quick-start/#plug-the-producer-into-dynamodb", 
            "text": "Now that we've set up our DynamoDB table, let's swap it into our producer as our announcement mechanism.  Go back to the  Producer  class and modify the  main  method, swapping the  Announcer  implementation as follows:  Announcer announcer = new DynamoDBAnnouncer(credentials,  HollowAnnouncement ,  demo );  Start the producer.  After at least one cycle, you'll be able to scan the table and see a record indicating the currently announced version for our  demo  namespace:", 
            "title": "Plug the Producer into DynamoDB"
        }, 
        {
            "location": "/quick-start/#plug-the-consumer-into-dynamodb", 
            "text": "Now that our producer is announcing to DynamoDB, of course our consumer needs to look there for update directions.  Go back to the  Consumer  class and modify the  main  method, swapping the  HollowAnnouncementWatcher  implementation as follows:  HollowAnnouncementWatcher announcementWatcher = \n            new DynamoDBAnnouncementWatcher(credentials,  HollowAnnouncement ,  demo );  Run the consumer.  You'll see the same output from prior steps, except now the Hollow input data is coming from S3, and the state version announcement is read from DynamoDB.   Pinning The State  The provided  DynamoDBAnnouncementWatcher  can be  pinned .  If the announcement item in DynamoDB contains a field  pin_version , then the consumer will go to the version indicated in that field, instead of the version the producer announces.  See  Pinning Consumers  for more details about pinning data.", 
            "title": "Plug the Consumer into DynamoDB"
        }, 
        {
            "location": "/quick-start/#plug-in-your-data-model", 
            "text": "Congratulations!  You now have a living, breathing, fully production scalable implementation of Hollow in your hands.  All that's left to do is substitute your data model for the example provided.  The data model can be defined, as in this example, with a set of POJO classes.  Take this starting point and make it your own -- remove the classes underneath  com.netflix.hollow.example.producer.datamodel  and replace them with your data model.   Modeling Data with POJOs  The  HollowObjectMapper  section of this document provides details about how to convert your POJO data model into Hollow.   Continue on to the  Getting Started  guide to learn more about how the fundamental pieces of Hollow fit together, how to create a consumer API custom-tailored to your data model, and how to index your data for easy, efficient retrieval by consumers.", 
            "title": "Plug In Your Data Model"
        }, 
        {
            "location": "/getting-started/", 
            "text": "In the \nQuick Start\n guide, we got a reference implementation of Hollow up and running, with a mock data model that can be easily modified to suit any use case.  After reading this section, you'll have an understanding of the basic usage patterns for Hollow, and how each of the core pieces fit together.\n\n\nCore Concepts\n\n\nHollow manages datasets which are built by a single \nproducer\n, and disseminated to one or many \nconsumers\n for read-only access.  A dataset changes over time.  The timeline for a changing dataset can be broken down into discrete \ndata states\n, each of which is a complete snapshot of the data at a particular point in time.\n\n\nBoth the producer and consumers handle datasets with a \nstate engine\n.  A state engine can be transitioned between data states.  A producer uses a \nwrite state engine\n and a consumer uses a \nread state engine\n.\n\n\nProducing a Data Snapshot\n\n\nLet's assume we have a POJO class \nMovie\n:\n\n\npublic class Movie {\n    long id;\n    String title;\n    int releaseYear;\n\n    public Movie(long id, String title, int releaseYear) {\n        this.id = id;\n        this.title = title;\n        this.releaseYear = releaseYear;\n    }\n}\n\n\n\n\nAnd that many \nMovie\ns exist which comprise a dataset that needs to be disseminated:\n\n\nList\nMovie\n movies = Arrays.asList(\n        new Movie(1, \nThe Matrix\n, 1999),\n        new Movie(2, \nBeasts of No Nation\n, 2015),\n        new Movie(3, \nPulp Fiction\n, 1994)\n);\n\n\n\n\nWe'll need a data \nproducer\n to create a data state which will be transmitted to consumers:\n\n\nHollowWriteStateEngine writeEngine = new HollowWriteStateEngine();\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\n\nfor(Movie movie : movies)\n    mapper.addObject(movie);\n\nOutputStream os = ...; /// where to write the blob\nHollowBlobWriter writer = new HollowBlobWriter(writeEngine);\nwriter.writeSnapshot(os);\n\n\n\n\nA \nHollowWriteStateEngine\n is the main handle to a Hollow dataset for a data producer.  A \nHollowObjectMapper\n is one of a few different ways to populate a \nHollowWriteStateEngine\n with data.  When starting with POJOs, it's the easiest way.\n\n\nWe'll use a \nHollowBlobWriter\n to write the current state of a \nHollowWriteStateEngine\n to an \nOutputStream\n.  We call the data which gets written to the \nOutputStream\n a \nblob\n.  \n\n\n\n\nPublishing Blobs\n\n\nFor the purposes of testing, this blob can be written to local disk.  In a production scenario, it can be written to a remote file store such as Amazon S3 for retrieval by consumers.\n\n\n\n\nConsumer API Generation\n\n\nOnce the data has been populated into a state engine, that state engine is aware of the data model, and can be used to automatically produce a client API:\n\n\nHollowAPIGenerator generator = \n       new HollowAPIGenerator(\n           \nMovieAPI\n,                    /// A name for the API\n           \ncom.netflix.hollow.example\n,  /// A package where the API will live\n           writeEngine                    /// our state engine\n       );\n\ngenerator.generateFiles(\n/path/to/files\n);\n\n\n\n\nAfter this code executes, an set of Java files will be written to the location \n/path/to/files\n.  These java files will be a generated API based on the data model defined by the schemas in our state engine, and will provide convenient methods to access that data.\n\n\nConsuming a Data Snapshot\n\n\nA data consumer can load a snapshot created by the producer into memory:\n\n\nHollowReadStateEngine readEngine = new HollowReadStateEngine();\nHollowBlobReader reader = new HollowBlobReader(readEngine);\n\nInputStream is = /// where to load the snapshot from\nreader.readSnapshot(is);\n\n\n\n\nA \nHollowReadStateEngine\n is our main handle to a Hollow dataset as a consumer.  A \nHollowBlobReader\n is used to consume blobs into a \nHollowReadStateEngine\n.  Above, we're consuming a snapshot blob in order to initialize our state engine.  \n\n\nOnce this dataset is loaded into memory, we can access the data for any records using our generated API:\n\n\nMovieAPI movieApi = new MovieAPI(readEngine);\n\nfor(MovieHollow movie : movieApi.getAllMovieHollow()) {\n    System.out.println(movie._getId() + \n, \n + \n                       movie._getTitle()._getValue() + \n, \n + \n                       movie._getReleaseYear());\n}\n\n\n\n\nThe output of the above code will be:\n\n\n1, The Matrix, 1999\n2, Beasts of No Nation, 2015\n3, Pulp Fiction, 1994\n\n\n\n\nProducing a Delta\n\n\nSome time has passed and the dataset has evolved.  It now contains these records:\n\n\nList\nMovie\n movies = Arrays.asList(\n        new Movie(1, \nThe Matrix\n, 1999),\n        new Movie(2, \nBeasts of No Nation\n, 2015),\n        new Movie(4, \nGoodfellas\n, 1990),\n        new Movie(5, \nInception\n, 2010)\n);\n\n\n\n\nThe producer, with the same \nHollowWriteStateEngine\n in memory, needs to communicate this updated dataset to consumers.  The data for the new state must be added to the state engine, after which a transition from the previous state to the new state can be written as a \ndelta\n blob:\n\n\nwriteEngine.prepareForNextCycle();\n\nfor(Movie movie : movies)\n    mapper.addObject(movie);\n\nOutputStream os = ....; /// where to write the delta blob\nwriter.writeDelta(os);\n\n\n\n\nLet's take a closer look at what the above code does.  The same \nHollowWriteStateEngine\n which was used to produce the \nsnapshot\n blob is used -- it already knows everything about the prior state and can be transitioned to the next state.  We call \nprepareForNextCycle()\n to inform the state engine that the writing of blobs from the prior state is complete, and populating data into the next state is about to begin.  When creating a new state, all of the movies currently in our dataset are re-added again.  It's not necessary to figure out which records were added, removed, or modified -- that's Hollow's job.\n\n\nWe can (but don't have to) use the same \nHollowObjectMapper\n and/or \nHollowBlobWriter\n as we used in the prior \ncycle\n to create the initial snapshot.  \n\n\nThe call to \nwriteDelta()\n records a \ndelta\n blob to the \nOutputStream\n.  Encoded into the delta is a set of instructions to update a consumer\u2019s read state engine from the previous state to the current state.\n\n\n\n\nProducer Cycles\n\n\nWe call what the producer does to create a data state a \ncycle\n.  During each \ncycle\n, we \n\n\n\n\nadd all of the records from our dataset into the state engine, then \n\n\nwrite blobs (usually each of a \nsnapshot\n, a \ndelta\n, and a \nreverse delta\n blob) to our blob file store.\n\n\n\n\n\n\nConsuming a Delta\n\n\nOnce a delta is announced the HollowReadStateEngine can be updated on the client:\n\n\nInputStream is = /// where to load the delta from\nHollowBlobReader reader = new HollowBlobReader(readEngine);\nreader.applyDelta(is);\n\n\n\n\nThe same \nHollowReadStateEngine\n into which our snapshot was consumed must be reused to consume a \ndelta\n blob.  This state engine knows everything about the current state and can use the instructions in a delta to transition to the next state.  We can (but don't have to) reuse the same \nHollowBlobReader\n.\n\n\nAfter this delta has been applied, the read state engine is at the new state.  If the generated API is used to iterate over the movies again as shown in the prior consumer example, the new output will be:\n\n\n1, The Matrix, 1999\n2, Beasts of No Nation, 2015\n4, Goodfellas, 1990\n5, Inception, 2010\n\n\n\n\n\n\nThread Safety\n\n\nIt is safe to use the HollowReadStateEngine to retrieve data while a delta transition is in progress.\n\n\n\n\n\n\nAdjacent States\n\n\nWe refer to states which are directly connected via single delta transitions as \nadjacent\n states, and a continuous set of adjacent states as a \ndelta chain\n\n\n\n\n\n\nDelta Mismatch\n\n\nIf a delta application is attempted onto a \nHollowReadStateEngine\n which is at a state from which the delta did not originate, then an exception is thrown and the state engine remains safely unchanged.\n\n\n\n\nIndexing Data for Retrieval\n\n\nIn prior examples the generated Hollow API was used by the data consumer to iterate over all \nMovie\n records in the dataset.  Most often, however, it isn\u2019t desirable to iterate over the entire dataset \u2014 instead, specific records will be accessed based on some known key.  Let\u2019s assume that the \nMovie\n\u2019s id is a known key.\n\n\nAfter consumers have populated a \nHollowReadStateEngine\n, the data can be indexed:\n\n\nHollowPrimaryKeyIndex idx = \n                      new HollowPrimaryKeyIndex(readEngine, \nMovie\n, \nid\n);\n\nidx.listenForDeltaUpdates();\n\n\n\n\nThis index can be held in memory and then used in conjunction with the generated Hollow API to retrieve Movie records by id:\n\n\nint movieOrdinal = idx.getMatchingOrdinal(2);\nif(movieOrdinal != -1) {\n    MovieHollow movie = movieApi.getMovieHollow(movieOrdinal);\n    System.out.println(\nFound Movie: \n + movie._getTitle()._getValue());\n}\n\n\n\n\nWhich outputs:\n\n\nFound Movie: Beasts of No Nation\n\n\n\n\n\n\nKeeping an Index Up To Date\n\n\nThe call to \nlistenForDeltaUpdates()\n will cause a \nHollowPrimaryKeyIndex\n to automatically stay updated when deltas are applied to the indexed \nHollowReadStateEngine\n, but this should only be called if you intend to keep the index around.  See the Indexing / Querying section for usage details.\n\n\n\n\n\n\nThread Safety\n\n\nRetrievals from a \nHollowPrimaryKeyIndex\n are thread-safe.  It is safe to use a \nHollowPrimaryKeyIndex\n from multiple threads, and it is safe to query while a transition is in progress.\n\n\n\n\n\n\nOrdinals\n\n\nEach record is assigned to a specific \nordinal\n, which is an integer value. An \nordinal\n:\n\n\n\n\nis a unique identifier of the record within a type.\n\n\nis sufficient to locate the record within a type.\n\n\n\n\nOrdinals are automatically assigned by Hollow. They lie in the range of 0-n, where n is generally not much larger than the total number of records for the type.\n\n\n\n\nHierarchical Data Models\n\n\nOur data models can be much richer than in the prior example.  Assume an updated \nMovie\n class:\n\n\npublic class Movie {\n    long id;\n    String title;\n    int releaseYear;\n    List\nActor\n actors;\n\n    public Movie(long id, String title, int year, List\nActor\n actors) {\n        this.id = id;\n        this.title = title;\n        this.releaseYear = year;\n        this.actors = actors;\n    }\n}\n\n\n\n\nWhich references \nActor\n records:\n\n\npublic class Actor {\n    long actorId;\n    String actorName;\n\n    public Actor(long actorId, String actorName) {\n        this.actorId = actorId;\n        this.actorName = actorName;\n    }\n}\n\n\n\n\nSome records are added to a \nHollowWriteStateEngine\n on the producer:\n\n\nList\nMovie\n movies = Arrays.asList(\n        new Movie(1, \nThe Matrix\n, 1999, Arrays.asList(\n                new Actor(101, \nKeanu Reeves\n),\n                new Actor(102, \nLaurence Fishburne\n),\n                new Actor(103, \nCarrie-Ann Moss\n),\n                new Actor(104, \nHugo Weaving\n)\n        )),\n        new Movie(6, \nEvent Horizon\n, 1997, Arrays.asList(\n                new Actor(102, \nLaurence Fishburne\n),\n                new Actor(105, \nSam Neill\n)\n        ))\n);\n\n\nfor(Movie movie : movies)\n    mapper.addObject(movie);\n\n\n\n\nWhen we add these movies to the dataset, the \nHollowObjectMapper\n will traverse everything referenced by the provided records and add them to the state as well.  Consequently, both a type \nMovie\n and a type \nActor\n will exist in the data model after the above code runs.  \n\n\n\n\nDeduplication\n\n\nLaurence Fishburne starred in both of these films.  Rather than creating two \nActor\n records for Mr. Fishburne, a single record will be created and assigned to both of our \nMovie\n records.  This \ndeduplication\n happens automatically by virtue of having the exact same data contained in both Actor inputs.\n\n\n\n\nConsumers of this dataset may want to also create an index for \nActor\n records:\n\n\nHollowPrimaryKeyIndex actorIdx = \n                    new HollowPrimaryKeyIndex(readEngine, \nActor\n, \nactorId\n);\nactorIdx.listenForDeltaUpdates();\n\n\n\n\nThis index can be used in the same way as the \nMovie\n index to retrieve \nActor\n records:\n\n\nint actorOrdinal = actorIdx.getMatchingOrdinal(102);\nif(actorOrdinal != -1) {\n    ActorHollow actor = movieApi.getActorHollow(actorOrdinal);\n    System.out.println(\nFound Actor: \n + actor._getActorName()._getValue());\n}\n\n\n\n\nWhich outputs:\n\n\nFound Actor: Laurence Fishburne", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting-started/#core-concepts", 
            "text": "Hollow manages datasets which are built by a single  producer , and disseminated to one or many  consumers  for read-only access.  A dataset changes over time.  The timeline for a changing dataset can be broken down into discrete  data states , each of which is a complete snapshot of the data at a particular point in time.  Both the producer and consumers handle datasets with a  state engine .  A state engine can be transitioned between data states.  A producer uses a  write state engine  and a consumer uses a  read state engine .", 
            "title": "Core Concepts"
        }, 
        {
            "location": "/getting-started/#producing-a-data-snapshot", 
            "text": "Let's assume we have a POJO class  Movie :  public class Movie {\n    long id;\n    String title;\n    int releaseYear;\n\n    public Movie(long id, String title, int releaseYear) {\n        this.id = id;\n        this.title = title;\n        this.releaseYear = releaseYear;\n    }\n}  And that many  Movie s exist which comprise a dataset that needs to be disseminated:  List Movie  movies = Arrays.asList(\n        new Movie(1,  The Matrix , 1999),\n        new Movie(2,  Beasts of No Nation , 2015),\n        new Movie(3,  Pulp Fiction , 1994)\n);  We'll need a data  producer  to create a data state which will be transmitted to consumers:  HollowWriteStateEngine writeEngine = new HollowWriteStateEngine();\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\n\nfor(Movie movie : movies)\n    mapper.addObject(movie);\n\nOutputStream os = ...; /// where to write the blob\nHollowBlobWriter writer = new HollowBlobWriter(writeEngine);\nwriter.writeSnapshot(os);  A  HollowWriteStateEngine  is the main handle to a Hollow dataset for a data producer.  A  HollowObjectMapper  is one of a few different ways to populate a  HollowWriteStateEngine  with data.  When starting with POJOs, it's the easiest way.  We'll use a  HollowBlobWriter  to write the current state of a  HollowWriteStateEngine  to an  OutputStream .  We call the data which gets written to the  OutputStream  a  blob .     Publishing Blobs  For the purposes of testing, this blob can be written to local disk.  In a production scenario, it can be written to a remote file store such as Amazon S3 for retrieval by consumers.", 
            "title": "Producing a Data Snapshot"
        }, 
        {
            "location": "/getting-started/#consumer-api-generation", 
            "text": "Once the data has been populated into a state engine, that state engine is aware of the data model, and can be used to automatically produce a client API:  HollowAPIGenerator generator = \n       new HollowAPIGenerator(\n            MovieAPI ,                    /// A name for the API\n            com.netflix.hollow.example ,  /// A package where the API will live\n           writeEngine                    /// our state engine\n       );\n\ngenerator.generateFiles( /path/to/files );  After this code executes, an set of Java files will be written to the location  /path/to/files .  These java files will be a generated API based on the data model defined by the schemas in our state engine, and will provide convenient methods to access that data.", 
            "title": "Consumer API Generation"
        }, 
        {
            "location": "/getting-started/#consuming-a-data-snapshot", 
            "text": "A data consumer can load a snapshot created by the producer into memory:  HollowReadStateEngine readEngine = new HollowReadStateEngine();\nHollowBlobReader reader = new HollowBlobReader(readEngine);\n\nInputStream is = /// where to load the snapshot from\nreader.readSnapshot(is);  A  HollowReadStateEngine  is our main handle to a Hollow dataset as a consumer.  A  HollowBlobReader  is used to consume blobs into a  HollowReadStateEngine .  Above, we're consuming a snapshot blob in order to initialize our state engine.    Once this dataset is loaded into memory, we can access the data for any records using our generated API:  MovieAPI movieApi = new MovieAPI(readEngine);\n\nfor(MovieHollow movie : movieApi.getAllMovieHollow()) {\n    System.out.println(movie._getId() +  ,   + \n                       movie._getTitle()._getValue() +  ,   + \n                       movie._getReleaseYear());\n}  The output of the above code will be:  1, The Matrix, 1999\n2, Beasts of No Nation, 2015\n3, Pulp Fiction, 1994", 
            "title": "Consuming a Data Snapshot"
        }, 
        {
            "location": "/getting-started/#producing-a-delta", 
            "text": "Some time has passed and the dataset has evolved.  It now contains these records:  List Movie  movies = Arrays.asList(\n        new Movie(1,  The Matrix , 1999),\n        new Movie(2,  Beasts of No Nation , 2015),\n        new Movie(4,  Goodfellas , 1990),\n        new Movie(5,  Inception , 2010)\n);  The producer, with the same  HollowWriteStateEngine  in memory, needs to communicate this updated dataset to consumers.  The data for the new state must be added to the state engine, after which a transition from the previous state to the new state can be written as a  delta  blob:  writeEngine.prepareForNextCycle();\n\nfor(Movie movie : movies)\n    mapper.addObject(movie);\n\nOutputStream os = ....; /// where to write the delta blob\nwriter.writeDelta(os);  Let's take a closer look at what the above code does.  The same  HollowWriteStateEngine  which was used to produce the  snapshot  blob is used -- it already knows everything about the prior state and can be transitioned to the next state.  We call  prepareForNextCycle()  to inform the state engine that the writing of blobs from the prior state is complete, and populating data into the next state is about to begin.  When creating a new state, all of the movies currently in our dataset are re-added again.  It's not necessary to figure out which records were added, removed, or modified -- that's Hollow's job.  We can (but don't have to) use the same  HollowObjectMapper  and/or  HollowBlobWriter  as we used in the prior  cycle  to create the initial snapshot.    The call to  writeDelta()  records a  delta  blob to the  OutputStream .  Encoded into the delta is a set of instructions to update a consumer\u2019s read state engine from the previous state to the current state.   Producer Cycles  We call what the producer does to create a data state a  cycle .  During each  cycle , we    add all of the records from our dataset into the state engine, then   write blobs (usually each of a  snapshot , a  delta , and a  reverse delta  blob) to our blob file store.", 
            "title": "Producing a Delta"
        }, 
        {
            "location": "/getting-started/#consuming-a-delta", 
            "text": "Once a delta is announced the HollowReadStateEngine can be updated on the client:  InputStream is = /// where to load the delta from\nHollowBlobReader reader = new HollowBlobReader(readEngine);\nreader.applyDelta(is);  The same  HollowReadStateEngine  into which our snapshot was consumed must be reused to consume a  delta  blob.  This state engine knows everything about the current state and can use the instructions in a delta to transition to the next state.  We can (but don't have to) reuse the same  HollowBlobReader .  After this delta has been applied, the read state engine is at the new state.  If the generated API is used to iterate over the movies again as shown in the prior consumer example, the new output will be:  1, The Matrix, 1999\n2, Beasts of No Nation, 2015\n4, Goodfellas, 1990\n5, Inception, 2010   Thread Safety  It is safe to use the HollowReadStateEngine to retrieve data while a delta transition is in progress.    Adjacent States  We refer to states which are directly connected via single delta transitions as  adjacent  states, and a continuous set of adjacent states as a  delta chain    Delta Mismatch  If a delta application is attempted onto a  HollowReadStateEngine  which is at a state from which the delta did not originate, then an exception is thrown and the state engine remains safely unchanged.", 
            "title": "Consuming a Delta"
        }, 
        {
            "location": "/getting-started/#indexing-data-for-retrieval", 
            "text": "In prior examples the generated Hollow API was used by the data consumer to iterate over all  Movie  records in the dataset.  Most often, however, it isn\u2019t desirable to iterate over the entire dataset \u2014 instead, specific records will be accessed based on some known key.  Let\u2019s assume that the  Movie \u2019s id is a known key.  After consumers have populated a  HollowReadStateEngine , the data can be indexed:  HollowPrimaryKeyIndex idx = \n                      new HollowPrimaryKeyIndex(readEngine,  Movie ,  id );\n\nidx.listenForDeltaUpdates();  This index can be held in memory and then used in conjunction with the generated Hollow API to retrieve Movie records by id:  int movieOrdinal = idx.getMatchingOrdinal(2);\nif(movieOrdinal != -1) {\n    MovieHollow movie = movieApi.getMovieHollow(movieOrdinal);\n    System.out.println( Found Movie:   + movie._getTitle()._getValue());\n}  Which outputs:  Found Movie: Beasts of No Nation   Keeping an Index Up To Date  The call to  listenForDeltaUpdates()  will cause a  HollowPrimaryKeyIndex  to automatically stay updated when deltas are applied to the indexed  HollowReadStateEngine , but this should only be called if you intend to keep the index around.  See the Indexing / Querying section for usage details.    Thread Safety  Retrievals from a  HollowPrimaryKeyIndex  are thread-safe.  It is safe to use a  HollowPrimaryKeyIndex  from multiple threads, and it is safe to query while a transition is in progress.    Ordinals  Each record is assigned to a specific  ordinal , which is an integer value. An  ordinal :   is a unique identifier of the record within a type.  is sufficient to locate the record within a type.   Ordinals are automatically assigned by Hollow. They lie in the range of 0-n, where n is generally not much larger than the total number of records for the type.", 
            "title": "Indexing Data for Retrieval"
        }, 
        {
            "location": "/getting-started/#hierarchical-data-models", 
            "text": "Our data models can be much richer than in the prior example.  Assume an updated  Movie  class:  public class Movie {\n    long id;\n    String title;\n    int releaseYear;\n    List Actor  actors;\n\n    public Movie(long id, String title, int year, List Actor  actors) {\n        this.id = id;\n        this.title = title;\n        this.releaseYear = year;\n        this.actors = actors;\n    }\n}  Which references  Actor  records:  public class Actor {\n    long actorId;\n    String actorName;\n\n    public Actor(long actorId, String actorName) {\n        this.actorId = actorId;\n        this.actorName = actorName;\n    }\n}  Some records are added to a  HollowWriteStateEngine  on the producer:  List Movie  movies = Arrays.asList(\n        new Movie(1,  The Matrix , 1999, Arrays.asList(\n                new Actor(101,  Keanu Reeves ),\n                new Actor(102,  Laurence Fishburne ),\n                new Actor(103,  Carrie-Ann Moss ),\n                new Actor(104,  Hugo Weaving )\n        )),\n        new Movie(6,  Event Horizon , 1997, Arrays.asList(\n                new Actor(102,  Laurence Fishburne ),\n                new Actor(105,  Sam Neill )\n        ))\n);\n\n\nfor(Movie movie : movies)\n    mapper.addObject(movie);  When we add these movies to the dataset, the  HollowObjectMapper  will traverse everything referenced by the provided records and add them to the state as well.  Consequently, both a type  Movie  and a type  Actor  will exist in the data model after the above code runs.     Deduplication  Laurence Fishburne starred in both of these films.  Rather than creating two  Actor  records for Mr. Fishburne, a single record will be created and assigned to both of our  Movie  records.  This  deduplication  happens automatically by virtue of having the exact same data contained in both Actor inputs.   Consumers of this dataset may want to also create an index for  Actor  records:  HollowPrimaryKeyIndex actorIdx = \n                    new HollowPrimaryKeyIndex(readEngine,  Actor ,  actorId );\nactorIdx.listenForDeltaUpdates();  This index can be used in the same way as the  Movie  index to retrieve  Actor  records:  int actorOrdinal = actorIdx.getMatchingOrdinal(102);\nif(actorOrdinal != -1) {\n    ActorHollow actor = movieApi.getActorHollow(actorOrdinal);\n    System.out.println( Found Actor:   + actor._getActorName()._getValue());\n}  Which outputs:  Found Actor: Laurence Fishburne", 
            "title": "Hierarchical Data Models"
        }, 
        {
            "location": "/interacting-with-a-dataset/", 
            "text": "Generated Object API\n\n\nEach of the examples provided thus far have focused on interaction with the Hollow data set via the generated Hollow Objects API.  Interacting with this API starts with an ordinal (likely obtained from a \nHollowPrimaryKeyIndex\n):\n\n\nint movieOrdinal = movieIdx.getMatchingOrdinal(6);\n\nMovieHollow movie = movieApi.getMovieHollow(movieOrdinal);\n\nfor(ActorHollow actor : movie._getActors()) {\n    System.out.println(\nStarring \n + actor._getActorName()._getValue());\n}\n\n\n\n\nHollow Objects\n are instantiated at the time they are requested.  Each Hollow Object references a single record of a specific type, and holds two things:\n\n\n\n\nA reference to the Hollow data store for the type\n\n\nAn ordinal\n\n\n\n\nA Hollow Object contains methods to retrieve each of its type's fields in the data model from which the API was generated.  Each field retrieval method is prefixed with an underscore.  Each time a field retrieval method is called on a Hollow Object, the data is retrieved directly from the Hollow data store.\n\n\n\n\nHollow Objects are 'hollow'\n\n\nThe name Hollow is derived from the fact that these objects are 'hollow' -- they \nappear\n to contain field accessors, but those are just facades which access the underlying data store.\n\n\n\n\nGenerated Type API\n\n\nAt times, using the Hollow Object API can result in a high rate of Object allocation.  All generated Hollow APIs also provide a way to interact with the data without creating Objects.  This is accomplished by using record ordinals to query the data store directly.  For example:\n\n\nMovieTypeAPI movieTypeAPI = movieAPI.getMovieTypeAPI();\nListOfActorTypeAPI listOfActorTypeAPI = movieAPI.getListOfActorTypeAPI();\nActorTypeAPI actorTypeAPI = movieAPI.getActorTypeAPI();\nStringTypeAPI stringTypeAPI = movieAPI.getStringTypeAPI();\n\nint movieOrdinal = movieIdx.getMatchingOrdinal(6);\nint listOfActorsOrdinal = movieTypeAPI.getActors(movieOrdinal);\n\nint numActors = listOfActorTypeAPI.size(listOfActorsOrdinal);\n\nfor(int i=0; i\nnumActors; i++) {\n   int actorOrdinal = \n                 listOfActorTypeAPI.getElementOrdinal(listOfActorsOrdinal, i);\n   int stringOrdinal = actorTypeAPI.getActorNameOrdinal(actorOrdinal);\n   System.out.println(\nStarring \n + stringAPI.getValue(stringOrdinal));\n}\n\n\n\n\nIn extremely tight loops, it may be more efficient to use the Type API rather than the Object API.\n\n\n\n\nAvoid Premature Optimization\n\n\nIn all but the tightest, most frequently executed loops, usage of the Type API will be unnecessary.  Its usage should be applied judiciously, since the pattern can be more difficult to maintain.\n\n\n\n\nGeneric Object API\n\n\nHollow also includes a generic Hollow Object API which, if sufficient for consumers, obviates the need to provide generated code:\n\n\nint movieOrdinal = movieIdx.getMatchingOrdinal(1);\n\nGenericHollowObject movie = new GenericHollowObject(readEngine, \nMovie\n, movieOrdinal);\n\nString title = movie.getObject(\ntitle\n).getString(\nvalue\n);\n\nfor(GenericHollowObject actor : movie.getList(\nactors\n).objects()) {\n    String actorName = actor.getObject(\nactorName\n).getString(\nvalue\n);\n    System.out.println(\nStarring \n + actorName);\n}\n\n\n\n\nWorking with the Generic Object API can become cumbersome \u2014 unlike a generated Hollow API, the IDE type assist cannot provide a guide to the data model.  However, for simple data models and explorational tasks the Generic Object API can be useful.", 
            "title": "Interacting with a Hollow Dataset"
        }, 
        {
            "location": "/interacting-with-a-dataset/#generated-object-api", 
            "text": "Each of the examples provided thus far have focused on interaction with the Hollow data set via the generated Hollow Objects API.  Interacting with this API starts with an ordinal (likely obtained from a  HollowPrimaryKeyIndex ):  int movieOrdinal = movieIdx.getMatchingOrdinal(6);\n\nMovieHollow movie = movieApi.getMovieHollow(movieOrdinal);\n\nfor(ActorHollow actor : movie._getActors()) {\n    System.out.println( Starring   + actor._getActorName()._getValue());\n}  Hollow Objects  are instantiated at the time they are requested.  Each Hollow Object references a single record of a specific type, and holds two things:   A reference to the Hollow data store for the type  An ordinal   A Hollow Object contains methods to retrieve each of its type's fields in the data model from which the API was generated.  Each field retrieval method is prefixed with an underscore.  Each time a field retrieval method is called on a Hollow Object, the data is retrieved directly from the Hollow data store.   Hollow Objects are 'hollow'  The name Hollow is derived from the fact that these objects are 'hollow' -- they  appear  to contain field accessors, but those are just facades which access the underlying data store.", 
            "title": "Generated Object API"
        }, 
        {
            "location": "/interacting-with-a-dataset/#generated-type-api", 
            "text": "At times, using the Hollow Object API can result in a high rate of Object allocation.  All generated Hollow APIs also provide a way to interact with the data without creating Objects.  This is accomplished by using record ordinals to query the data store directly.  For example:  MovieTypeAPI movieTypeAPI = movieAPI.getMovieTypeAPI();\nListOfActorTypeAPI listOfActorTypeAPI = movieAPI.getListOfActorTypeAPI();\nActorTypeAPI actorTypeAPI = movieAPI.getActorTypeAPI();\nStringTypeAPI stringTypeAPI = movieAPI.getStringTypeAPI();\n\nint movieOrdinal = movieIdx.getMatchingOrdinal(6);\nint listOfActorsOrdinal = movieTypeAPI.getActors(movieOrdinal);\n\nint numActors = listOfActorTypeAPI.size(listOfActorsOrdinal);\n\nfor(int i=0; i numActors; i++) {\n   int actorOrdinal = \n                 listOfActorTypeAPI.getElementOrdinal(listOfActorsOrdinal, i);\n   int stringOrdinal = actorTypeAPI.getActorNameOrdinal(actorOrdinal);\n   System.out.println( Starring   + stringAPI.getValue(stringOrdinal));\n}  In extremely tight loops, it may be more efficient to use the Type API rather than the Object API.   Avoid Premature Optimization  In all but the tightest, most frequently executed loops, usage of the Type API will be unnecessary.  Its usage should be applied judiciously, since the pattern can be more difficult to maintain.", 
            "title": "Generated Type API"
        }, 
        {
            "location": "/interacting-with-a-dataset/#generic-object-api", 
            "text": "Hollow also includes a generic Hollow Object API which, if sufficient for consumers, obviates the need to provide generated code:  int movieOrdinal = movieIdx.getMatchingOrdinal(1);\n\nGenericHollowObject movie = new GenericHollowObject(readEngine,  Movie , movieOrdinal);\n\nString title = movie.getObject( title ).getString( value );\n\nfor(GenericHollowObject actor : movie.getList( actors ).objects()) {\n    String actorName = actor.getObject( actorName ).getString( value );\n    System.out.println( Starring   + actorName);\n}  Working with the Generic Object API can become cumbersome \u2014 unlike a generated Hollow API, the IDE type assist cannot provide a guide to the data model.  However, for simple data models and explorational tasks the Generic Object API can be useful.", 
            "title": "Generic Object API"
        }, 
        {
            "location": "/data-ingestion/", 
            "text": "Hollow includes a few ready-made data ingestion mechanisms.  Additionally, custom data ingestion mechanisms can be created relatively easily using the \nLow Level Input API\n.\n\n\nHollowObjectMapper\n\n\nThe \nHollowObjectMapper\n will add POJOs into a \nHollowWriteStateEngine\n:\n\n\nHollowWriteStateEngine engine = /// a state engine\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\n\nfor(Movie movie : movies)\n    writeEngine.add(movie);\n\n\n\n\nThe \nHollowObjectMapper\n can also be used to initialize the data model of a \nHollowWriteStateEngine\n without adding any actual data:\n\n\nHollowWriteStateEngine engine = /// a state engine\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\n\nmapper.initializeTypeState(Movie.class);\nmapper.initializeTypeState(Show.class);\n\n\n\n\nThe schemas will be assumed based on the field and type names in the POJOs.  Any referenced types will also be traversed and included in the derived data model.  The name of a type may be explicitly defined as something other than the class name using the \n@HollowTypeName\n annotation.\n\n\nIf an integer field named \n__assigned_ordinal\n is defined in POJOs, the \nHollowObjectMapper\n will use this field to memoize the assigned ordinal for instances.  Upon addition of an object which already has this value memoized, the \nHollowObjectMapper\n will short-circuit retrieving the assigned ordinal.  The field should be initialized to -1.  The field may be, but does not have to be, private and/or final.\n\n\n\n\nWarning\n\n\nIf the \n__assigned_ordinal\n optimization is used, and field values inside POJOs with this optimization are modified subsequent to their addition to a state engine, then the subsequent modifications will be ignored and any references to these POJOs will always point to the \noriginally\n added record.\n\n\nAdditionally, if this optimization is used then POJOs should not be reused between cycles.\n\n\n\n\nThe following example \nAward\n class will reference a type \nAwardName\n, which will contain a single string value.  The class will also employ the \n__assigned_ordinal\n shortcut:\n\n\npublic class Award {\n    long id;\n\n    @HollowTypeName(name=\nAwardName\n)\n    String name;\n\n    private final int __assigned_ordinal = -1;\n}\n\n\n\n\n\n\nThread Safety\n\n\nThe \nHollowObjectMapper\n is thread-safe; multiple threads may add Objects at the same time.\n\n\n\n\nJSON to Hollow Adapter\n\n\nThe project \nhollow-jsonadapter\n contains a component which will automatically parse json into a \nHollowWriteStateEngine\n.  The expected format of the json will be defined by the schemas in the \nHollowWriteStateEngine\n.  The data model must be pre-initialized.  See the \nSchema Parser\n topic in this document for an easy way to configure the schemas with a text document.\n\n\nThe \nHollowJsonAdapter\n class is used to populate records of a single type from a json file.  A single record:\n\n\n{ \n  \nid\n: 1,\n  \nreleaseYear\n: 1999,\n  \nactors\n: [\n     {\n        \nid\n: 101,\n        \nactorName\n: \nKeanu Reeves\n\n     },\n     {\n        \nid\n: 102,\n        \nactorName\n: \nLaurence Fishburne\n\n     }\n  ]\n}\n\n\n\n\nCan be parsed with the following code:\n\n\nString json = /// the record above\n\nHollowJsonAdapter jsonAdapter = new HollowJsonAdapter(writeEngine, \nMovie\n);\n\njsonAdapter.processRecord(json);\n\n\n\n\nIf a field defined in the schema is not encountered in the json data, the value will be null in the corresponding Hollow record.  If a field is encountered in the json data which is not defined in the schema, the field will be ignored.\n\n\nA large number of records in a single file can also be processed:\n\n\nReader reader = /// a reader for the json file \n\nHollowJsonAdapter jsonAdapter = new HollowJsonAdapter(writeEngine, \nMovie\n);\n\njsonAdapter.populate(reader);\n\n\n\n\nWhen processing an entire file, it is expected that the file contains only a single json array of records of the expected type.  The records will be processed in parallel.\n\n\nZeno to Hollow Adapter\n\n\nThe project \nhollow-zenoadapter\n has an adapter which can be used with Hollow\u2019s predecessor, Zeno.  We used this as part of our migration path from Zeno to Hollow, and it is provided for current users of Zeno who would like to migrate to Hollow as well.  Start with the \nHollowStateEngineCreator\n.", 
            "title": "Data Ingestion"
        }, 
        {
            "location": "/data-ingestion/#hollowobjectmapper", 
            "text": "The  HollowObjectMapper  will add POJOs into a  HollowWriteStateEngine :  HollowWriteStateEngine engine = /// a state engine\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\n\nfor(Movie movie : movies)\n    writeEngine.add(movie);  The  HollowObjectMapper  can also be used to initialize the data model of a  HollowWriteStateEngine  without adding any actual data:  HollowWriteStateEngine engine = /// a state engine\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\n\nmapper.initializeTypeState(Movie.class);\nmapper.initializeTypeState(Show.class);  The schemas will be assumed based on the field and type names in the POJOs.  Any referenced types will also be traversed and included in the derived data model.  The name of a type may be explicitly defined as something other than the class name using the  @HollowTypeName  annotation.  If an integer field named  __assigned_ordinal  is defined in POJOs, the  HollowObjectMapper  will use this field to memoize the assigned ordinal for instances.  Upon addition of an object which already has this value memoized, the  HollowObjectMapper  will short-circuit retrieving the assigned ordinal.  The field should be initialized to -1.  The field may be, but does not have to be, private and/or final.   Warning  If the  __assigned_ordinal  optimization is used, and field values inside POJOs with this optimization are modified subsequent to their addition to a state engine, then the subsequent modifications will be ignored and any references to these POJOs will always point to the  originally  added record.  Additionally, if this optimization is used then POJOs should not be reused between cycles.   The following example  Award  class will reference a type  AwardName , which will contain a single string value.  The class will also employ the  __assigned_ordinal  shortcut:  public class Award {\n    long id;\n\n    @HollowTypeName(name= AwardName )\n    String name;\n\n    private final int __assigned_ordinal = -1;\n}   Thread Safety  The  HollowObjectMapper  is thread-safe; multiple threads may add Objects at the same time.", 
            "title": "HollowObjectMapper"
        }, 
        {
            "location": "/data-ingestion/#json-to-hollow-adapter", 
            "text": "The project  hollow-jsonadapter  contains a component which will automatically parse json into a  HollowWriteStateEngine .  The expected format of the json will be defined by the schemas in the  HollowWriteStateEngine .  The data model must be pre-initialized.  See the  Schema Parser  topic in this document for an easy way to configure the schemas with a text document.  The  HollowJsonAdapter  class is used to populate records of a single type from a json file.  A single record:  { \n   id : 1,\n   releaseYear : 1999,\n   actors : [\n     {\n         id : 101,\n         actorName :  Keanu Reeves \n     },\n     {\n         id : 102,\n         actorName :  Laurence Fishburne \n     }\n  ]\n}  Can be parsed with the following code:  String json = /// the record above\n\nHollowJsonAdapter jsonAdapter = new HollowJsonAdapter(writeEngine,  Movie );\n\njsonAdapter.processRecord(json);  If a field defined in the schema is not encountered in the json data, the value will be null in the corresponding Hollow record.  If a field is encountered in the json data which is not defined in the schema, the field will be ignored.  A large number of records in a single file can also be processed:  Reader reader = /// a reader for the json file \n\nHollowJsonAdapter jsonAdapter = new HollowJsonAdapter(writeEngine,  Movie );\n\njsonAdapter.populate(reader);  When processing an entire file, it is expected that the file contains only a single json array of records of the expected type.  The records will be processed in parallel.", 
            "title": "JSON to Hollow Adapter"
        }, 
        {
            "location": "/data-ingestion/#zeno-to-hollow-adapter", 
            "text": "The project  hollow-zenoadapter  has an adapter which can be used with Hollow\u2019s predecessor, Zeno.  We used this as part of our migration path from Zeno to Hollow, and it is provided for current users of Zeno who would like to migrate to Hollow as well.  Start with the  HollowStateEngineCreator .", 
            "title": "Zeno to Hollow Adapter"
        }, 
        {
            "location": "/data-modeling/", 
            "text": "Schema Types\n\n\nHollow records are strongly typed.  The structure of each type is defined by a schema.  A schema will be one of the following:\n\n\n\n\nObject\n: A fixed set of strongly typed fields.\n\n\nList\n: An ordered collection of references to records of a specific type.\n\n\nSet\n: An unordered collection of references to records of a specific type.  \n\n\nMap\n: A key/value mapping between references to records of a specific key type and records of a specific value type.\n\n\n\n\n\n\nSchemas Define the Data Model\n\n\nA hollow dataset is comprised of one or more data \ntypes\n.  The \ndata model\n for a dataset is defined by the schemas describing those types.\n\n\n\n\nObject Schemas\n\n\nObject\n schemas have one or more fields.  Each field is one of the following types:\n\n\n\n\nINT\n: An integer value up to 32-bits.\n\n\nLONG\n: An integer value up to 64-bits\n\n\nFLOAT\n: A 32-bit floating-point value\n\n\nDOUBLE\n: A 64-bit floating-point value\n\n\nBOOLEAN\n: \ntrue\n or \nfalse\n\n\nSTRING\n: An array of characters.  \n\n\nBYTES\n: An array of bytes.  \n\n\nREFERENCE\n: A reference to another specific type.  The referenced type must be defined by the schema.\n\n\n\n\nAdditionally, an \nObject\n schema may optionally specify a primary key, which can be used as a default indexing mechanism in many of the tools provided with Hollow.\n\n\nOn consumers, \nINT\n and \nLONG\n fields are each represented by a number of bits exactly sufficient to represent the maximum value for the field across all records.  \nFLOAT\n, \nDOUBLE\n, and \nBOOLEAN\n fields are represented by 32, 64, and 2 bits, respectively.  \nSTRING\n and \nBYTES\n fields use a variable number of bytes for each record.  \nREFERENCE\n fields encode the \nordinal\n of referenced records, and are represented by a number of bits exactly sufficient to encode the maximum ordinal of the referenced type.\n\n\n\n\nDesigning for Efficiency\n\n\nTry to model your data such that there aren't any outlier values for \nINT\n and \nLONG\n fields.  Also, avoid \nFLOAT\n and \nDOUBLE\n fields where possible, since these field types are relatively expensive.\n\n\n\n\nList Schemas\n\n\nA \nList\n schema indicates a record type which is an ordered collection of \nREFERENCE\n fields.  Each record will have a variable number of references.  The referenced type must be defined by the schema, and all references in all records will encode only the \nordinals\n of the referenced records as the values for these references.\n\n\nSet Schemas\n\n\nA \nSet\n schema indicates a record type which is an unordered collection of \nREFERENCE\n fields.  Each record will have a variable number of references, and the referenced type must be defined by the schema.  Within a single set record, each reference must be unique.  \n\n\nReferences in \nSet\n records can be hashed by some specific element fields for O(1) retrieval.  In order to enable this feature, a \nSet\n schema will define an optional \nhash key\n, which defines how its elements are hashed/indexed.\n\n\nMap Schemas\n\n\nA \nMap\n schema indicates a record type which is an unordered collection of pairs of \nREFERENCE\n fields, used to represent a key/value mapping.  Each record will have a variable number of key/value pairs.  Both the key reference type and the value reference type must be defined by the schema.  The key reference type does not have to be the same as the value reference type.  Within a single map record, each key reference must be unique.  \n\n\nEntries in \nMap\n records can be hashed by some specific key fields for O(1) retrieval of the keys, values, and/or entries.  In order to enable this feature, a \nMap\n schema will define an optional \nhash key\n, which defines how its entries are hashed/indexed.\n\n\nHash Keys\n\n\nEach \nMap\n and \nSet\n schema may optionally define a \nhash key\n.  A \nhash key\n specifies one or more user-defined fields used to hash entries into the collection.  When a hash key is defined on a \nSet\n, each set record becomes like a primary key index; records in the set can be efficiently retrieved by matching the specified \nhash key\n fields.  Similarly, when a hash key is defined on a \nMap\n, each map record becomes like an index over the keys in the key/value pairs contained in the map record.\n\n\nIf using the \nHollowObjectMapper\n, hash keys will be automatically selected if an element or key type contain a single non-reference field.  Addionally, if a \nSet\n or \nMap\n references \nObject\n elements with a defined \nprimary key\n, then the \nhash key\n will default to the \nprimary key\n of the element type.  \n\n\nAlternatively, \nhash keys\n can be explicitly defined using the \n@HollowHashKey\n annotation in POJOs for \nSet\n schemas by specifying one or more fields from the element type, or for \nMap\n schemas by specifying one or more fields from the key type:\n\n\npublic class Movie {\n    long id;\n    String title;\n    @HollowHashKey(fields=\nactorId\n)\n    Set\nActor\n actors;\n\n    ...\n}\n\n\n\n\nThe consumers, via the generated API, will have the ability to retrieve elements from \nSet\n records by the \nhash key\n:\n\n\nMovieHollow movie = api.getMovieHollow(ordinal);\nHollowSet\nActorHollow\n actors = movie._getActors();\nActorHollow actor = actors.findElement(104);\n\nSystem.out.println(\nActor with ID 104: \n + actor._getActorName()._getValue());\n\n\n\n\nThe consumers will have the ability to retrieve keys, values, and entries from the map by the \nhash key\n:\n\n\nMovieHollow movie = api.getMovieHollow(ordinal);\nHollowMap\nActorHollow, CharacterHollow\n actors = movie._getActors();\n\nActorHollow actor = actors.findKey(104);\nCharacteroHollow character = actors.findValue(104);\n/// alternatively: Map.Entry\nActorHollow, CharacterHollow\n entry = actors.findEntry(104);\n\nSystem.out.println(\nActor with ID 104: \n + actor._getActorName()._getValue() + \n         \n played character \n + character._getCharacterName()._getValue());\n\n\n\n\nWe can define more than one field in the \n@HollowHashKey\n declaration if our key spans multiple fields.  Each field may be multiple levels deep in a hierarchical data model, expressed via dot notation (e.g. \nactorName.value\n).\n\n\nCircular References\n\n\nCircular references are not allowed in Hollow.  A type may not reference itself, either directly or transitively.\n\n\nInlined vs Referenced Fields\n\n\nWhile modeling data, a choice sometimes must be made whether to define an Object field as a non-reference type (e.g. \nSTRING\n), or as a \nREFERENCE\n to a separate type which has a single \nSTRING\n field.  Consider the following type:\n\n\nAward {\n    String awardName;\n    long movieId;\n    long actorId;\n}\n\n\n\n\nIn this case, imagine \nawardName\n is something like \u201cBest Supporting Actress\u201d.  Over the years, many such awards will be given, so we\u2019ll have a lot of records which share that value.  If we use a \nSTRING\n field, then that character string will be repeated for every such award record.  However, if we reference a separate record type, all such awards will reference the same record with that value.  If the \nawardName\n values have a lot of repetition, then this can result in a significant savings.\n\n\n\n\nDeduplication\n\n\nRecord deduplication happens automatically at the \nrecord\n granularity in Hollow.  Try to model your data such that when there is a lot of repetition in records, the repetitive fields are encapsulated into their own types.\n\n\n\n\nTo consider the opposite case, let\u2019s examine the \nActor\n type:\n\n\nActor {\n    long id;\n    String actorName;\n}\n\n\n\n\nThe \nactorName\n is unlikely to be repeated often.  In this case, if we reference a separate record type, we have to retain roughly the same number of character strings, plus, we need to retain references to those records.  In this case, we end up saving space by using a \nSTRING\n field instead of a reference to a separate type.\n\n\n\n\nReference Costs\n\n\nA \nREFERENCE\n field isn't free, and therefore we shouldn't necessarily try to encapsulate fields inside their own record types where we won't benefit from deduplication.  These fields should instead be \ninlined\n.\n\n\n\n\nWe refer to fields which are defined with native Hollow types as \ninlined\n fields, and fields which are defined as references to types with a single field as \nreferenced\n fields.\n\n\nIn order to be very efficient, referenced fields sometimes need to be \nnamespaced\n so that fields with like values may use the same type, but referenced fields of the same type elsewhere in the data model use different types.  For example, consider our Award type again, but this time, we\u2019ll reference a type called \nAwardName\n, instead of \nString\n:\n\n\nAward {\n    AwardName awardName;\n    long movieId;\n    long actorId;\n}\n\n\nAwardName {\n    string value;\n}\n\n\n\n\nOther types in our data model which reference award names can reuse the \nAwardName\n type.  Other referenced string fields in our data model, which are unrelated to award names, should use different types corresponding to the semantics of their values.  \n\n\nNamespacing fields appropriately saves space because references to types with a lower cardinality use fewer bits than references to types with a higher cardinality.  The reason for this can be gleaned from the \nIn-Memory Data Layout\n topic underneath the \nAdvanced Topics\n section.\n\n\n\n\nNamespacing Reduces Reference Costs\n\n\nUsing an appropriately \nnamespaced\n type reduces the heap footprint of \nREFERENCE\n fields.\n\n\n\n\nGrouping Associated Fields\n\n\nReferencing fields can save space because the same field values do not have to be repeated for every record in which they occur.  Similarly, we can group fields which have covarying values, and pull these out from larger objects as their own types.  For example, imagine we started with a \nMovie\n record which included the following fields:\n\n\nMovie {\n    long id;\n    String title;\n    String maturityRating;\n    String advisories;\n}\n\n\n\n\nWe might notice that the \nmaturityRating\n and \nadvisories\n fields vary together, and are often the repeated across many Movies.  We can pull out a separate type for these fields:\n\n\nMovie {\n    long id;\n    String title;\n    MaturityRating maturityRating;\n}\n\nMaturityRating {\n    string rating;\n    string advisories;\n}\n\n\n\n\nWe could have referenced these fields separately.  If we had done so, each \nMovie\n record, of which there are probably many, would have had to contain two separate references for these fields.  Instead, by recognizing that these fields were associated and pulling them together, space is saved because each \nMovie\n record now only contains one reference for this data.\n\n\nMaintaining Backwards Compatibility\n\n\nA data model will evolve over time.  The following operations will not impact the interoperability between existing clients and new data:\n\n\n\n\nAdding a new type\n\n\nRemoving an existing type\n\n\nAdding a new field to an existing type\n\n\nRemoving an existing field from an existing type.\n\n\n\n\nWhen adding new fields or types, existing generated client APIs will ignore the new fields, and all of the fields which existed at the time of API generation will still be visible using the same methods.  When removing fields, existing generated client APIs will see null values if the methods corresponding to the removed fields are called.  When removing types, existing generated client APIs will see removed types as having no records.\n\n\nIt is not backwards compatible to change the type of an existing field.  The client behavior when calling a method corresponding to a field with a changed type is undefined.\n\n\nBackwards compatibility often has a lot to do with the use case and semantics of the data. Hollow will always behave in the stated way for evolving data models, but it\u2019s possible that consumers require a field which starts returning null once it gets removed.  For this reason, additional caution should be exercised when removing types and fields.\n\n\nMovie/Actor Example\n\n\nLet's examine the \nMovie\n / \nActor\n data model from our Getting Started Guide:\n\n\npublic class Movie {\n    long id;\n    String title;\n    int releaseYear;\n    List\nActor\n actors;\n\n    public Movie(long id, String title, int year, List\nActor\n actors) {\n        this.id = id;\n        this.title = title;\n        this.releaseYear = year;\n        this.actors = actors;\n    }\n}\n\npublic class Actor {\n    long actorId;\n    String actorName;\n\n    public Actor(long actorId, String actorName) {\n        this.actorId = actorId;\n        this.actorName = actorName;\n    }\n}\n\n\n\n\nThere are four type schemas in this data model:  \nMovie\n, \nListOfActor\n, \nActor\n, and \nString\n.\n\n\nUpon observing the \nMovie\n and \nActor\n classes, the \nHollowObjectMapper\n will add a type for each into its \nHollowWriteStateEngine\n, each with an \nObject\n schema type.  Each of these references a \nString\n field.  Just as in Java, a \nString\n references a separate \nObject\n, and the \nHollowObjectMapper\n will add an \nObject\n schema for a type named \nString\n to assign to these references.\n\n\n\n\nInlined Strings in the HollowObjectMapper\n\n\nTo define an inlined String with the \nHollowObjectMapper\n, define string references as \nchar[]\n.", 
            "title": "Data Modeling"
        }, 
        {
            "location": "/data-modeling/#schema-types", 
            "text": "Hollow records are strongly typed.  The structure of each type is defined by a schema.  A schema will be one of the following:   Object : A fixed set of strongly typed fields.  List : An ordered collection of references to records of a specific type.  Set : An unordered collection of references to records of a specific type.    Map : A key/value mapping between references to records of a specific key type and records of a specific value type.    Schemas Define the Data Model  A hollow dataset is comprised of one or more data  types .  The  data model  for a dataset is defined by the schemas describing those types.", 
            "title": "Schema Types"
        }, 
        {
            "location": "/data-modeling/#object-schemas", 
            "text": "Object  schemas have one or more fields.  Each field is one of the following types:   INT : An integer value up to 32-bits.  LONG : An integer value up to 64-bits  FLOAT : A 32-bit floating-point value  DOUBLE : A 64-bit floating-point value  BOOLEAN :  true  or  false  STRING : An array of characters.    BYTES : An array of bytes.    REFERENCE : A reference to another specific type.  The referenced type must be defined by the schema.   Additionally, an  Object  schema may optionally specify a primary key, which can be used as a default indexing mechanism in many of the tools provided with Hollow.  On consumers,  INT  and  LONG  fields are each represented by a number of bits exactly sufficient to represent the maximum value for the field across all records.   FLOAT ,  DOUBLE , and  BOOLEAN  fields are represented by 32, 64, and 2 bits, respectively.   STRING  and  BYTES  fields use a variable number of bytes for each record.   REFERENCE  fields encode the  ordinal  of referenced records, and are represented by a number of bits exactly sufficient to encode the maximum ordinal of the referenced type.   Designing for Efficiency  Try to model your data such that there aren't any outlier values for  INT  and  LONG  fields.  Also, avoid  FLOAT  and  DOUBLE  fields where possible, since these field types are relatively expensive.", 
            "title": "Object Schemas"
        }, 
        {
            "location": "/data-modeling/#list-schemas", 
            "text": "A  List  schema indicates a record type which is an ordered collection of  REFERENCE  fields.  Each record will have a variable number of references.  The referenced type must be defined by the schema, and all references in all records will encode only the  ordinals  of the referenced records as the values for these references.", 
            "title": "List Schemas"
        }, 
        {
            "location": "/data-modeling/#set-schemas", 
            "text": "A  Set  schema indicates a record type which is an unordered collection of  REFERENCE  fields.  Each record will have a variable number of references, and the referenced type must be defined by the schema.  Within a single set record, each reference must be unique.    References in  Set  records can be hashed by some specific element fields for O(1) retrieval.  In order to enable this feature, a  Set  schema will define an optional  hash key , which defines how its elements are hashed/indexed.", 
            "title": "Set Schemas"
        }, 
        {
            "location": "/data-modeling/#map-schemas", 
            "text": "A  Map  schema indicates a record type which is an unordered collection of pairs of  REFERENCE  fields, used to represent a key/value mapping.  Each record will have a variable number of key/value pairs.  Both the key reference type and the value reference type must be defined by the schema.  The key reference type does not have to be the same as the value reference type.  Within a single map record, each key reference must be unique.    Entries in  Map  records can be hashed by some specific key fields for O(1) retrieval of the keys, values, and/or entries.  In order to enable this feature, a  Map  schema will define an optional  hash key , which defines how its entries are hashed/indexed.", 
            "title": "Map Schemas"
        }, 
        {
            "location": "/data-modeling/#hash-keys", 
            "text": "Each  Map  and  Set  schema may optionally define a  hash key .  A  hash key  specifies one or more user-defined fields used to hash entries into the collection.  When a hash key is defined on a  Set , each set record becomes like a primary key index; records in the set can be efficiently retrieved by matching the specified  hash key  fields.  Similarly, when a hash key is defined on a  Map , each map record becomes like an index over the keys in the key/value pairs contained in the map record.  If using the  HollowObjectMapper , hash keys will be automatically selected if an element or key type contain a single non-reference field.  Addionally, if a  Set  or  Map  references  Object  elements with a defined  primary key , then the  hash key  will default to the  primary key  of the element type.    Alternatively,  hash keys  can be explicitly defined using the  @HollowHashKey  annotation in POJOs for  Set  schemas by specifying one or more fields from the element type, or for  Map  schemas by specifying one or more fields from the key type:  public class Movie {\n    long id;\n    String title;\n    @HollowHashKey(fields= actorId )\n    Set Actor  actors;\n\n    ...\n}  The consumers, via the generated API, will have the ability to retrieve elements from  Set  records by the  hash key :  MovieHollow movie = api.getMovieHollow(ordinal);\nHollowSet ActorHollow  actors = movie._getActors();\nActorHollow actor = actors.findElement(104);\n\nSystem.out.println( Actor with ID 104:   + actor._getActorName()._getValue());  The consumers will have the ability to retrieve keys, values, and entries from the map by the  hash key :  MovieHollow movie = api.getMovieHollow(ordinal);\nHollowMap ActorHollow, CharacterHollow  actors = movie._getActors();\n\nActorHollow actor = actors.findKey(104);\nCharacteroHollow character = actors.findValue(104);\n/// alternatively: Map.Entry ActorHollow, CharacterHollow  entry = actors.findEntry(104);\n\nSystem.out.println( Actor with ID 104:   + actor._getActorName()._getValue() + \n           played character   + character._getCharacterName()._getValue());  We can define more than one field in the  @HollowHashKey  declaration if our key spans multiple fields.  Each field may be multiple levels deep in a hierarchical data model, expressed via dot notation (e.g.  actorName.value ).", 
            "title": "Hash Keys"
        }, 
        {
            "location": "/data-modeling/#circular-references", 
            "text": "Circular references are not allowed in Hollow.  A type may not reference itself, either directly or transitively.", 
            "title": "Circular References"
        }, 
        {
            "location": "/data-modeling/#inlined-vs-referenced-fields", 
            "text": "While modeling data, a choice sometimes must be made whether to define an Object field as a non-reference type (e.g.  STRING ), or as a  REFERENCE  to a separate type which has a single  STRING  field.  Consider the following type:  Award {\n    String awardName;\n    long movieId;\n    long actorId;\n}  In this case, imagine  awardName  is something like \u201cBest Supporting Actress\u201d.  Over the years, many such awards will be given, so we\u2019ll have a lot of records which share that value.  If we use a  STRING  field, then that character string will be repeated for every such award record.  However, if we reference a separate record type, all such awards will reference the same record with that value.  If the  awardName  values have a lot of repetition, then this can result in a significant savings.   Deduplication  Record deduplication happens automatically at the  record  granularity in Hollow.  Try to model your data such that when there is a lot of repetition in records, the repetitive fields are encapsulated into their own types.   To consider the opposite case, let\u2019s examine the  Actor  type:  Actor {\n    long id;\n    String actorName;\n}  The  actorName  is unlikely to be repeated often.  In this case, if we reference a separate record type, we have to retain roughly the same number of character strings, plus, we need to retain references to those records.  In this case, we end up saving space by using a  STRING  field instead of a reference to a separate type.   Reference Costs  A  REFERENCE  field isn't free, and therefore we shouldn't necessarily try to encapsulate fields inside their own record types where we won't benefit from deduplication.  These fields should instead be  inlined .   We refer to fields which are defined with native Hollow types as  inlined  fields, and fields which are defined as references to types with a single field as  referenced  fields.  In order to be very efficient, referenced fields sometimes need to be  namespaced  so that fields with like values may use the same type, but referenced fields of the same type elsewhere in the data model use different types.  For example, consider our Award type again, but this time, we\u2019ll reference a type called  AwardName , instead of  String :  Award {\n    AwardName awardName;\n    long movieId;\n    long actorId;\n}\n\n\nAwardName {\n    string value;\n}  Other types in our data model which reference award names can reuse the  AwardName  type.  Other referenced string fields in our data model, which are unrelated to award names, should use different types corresponding to the semantics of their values.    Namespacing fields appropriately saves space because references to types with a lower cardinality use fewer bits than references to types with a higher cardinality.  The reason for this can be gleaned from the  In-Memory Data Layout  topic underneath the  Advanced Topics  section.   Namespacing Reduces Reference Costs  Using an appropriately  namespaced  type reduces the heap footprint of  REFERENCE  fields.", 
            "title": "Inlined vs Referenced Fields"
        }, 
        {
            "location": "/data-modeling/#grouping-associated-fields", 
            "text": "Referencing fields can save space because the same field values do not have to be repeated for every record in which they occur.  Similarly, we can group fields which have covarying values, and pull these out from larger objects as their own types.  For example, imagine we started with a  Movie  record which included the following fields:  Movie {\n    long id;\n    String title;\n    String maturityRating;\n    String advisories;\n}  We might notice that the  maturityRating  and  advisories  fields vary together, and are often the repeated across many Movies.  We can pull out a separate type for these fields:  Movie {\n    long id;\n    String title;\n    MaturityRating maturityRating;\n}\n\nMaturityRating {\n    string rating;\n    string advisories;\n}  We could have referenced these fields separately.  If we had done so, each  Movie  record, of which there are probably many, would have had to contain two separate references for these fields.  Instead, by recognizing that these fields were associated and pulling them together, space is saved because each  Movie  record now only contains one reference for this data.", 
            "title": "Grouping Associated Fields"
        }, 
        {
            "location": "/data-modeling/#maintaining-backwards-compatibility", 
            "text": "A data model will evolve over time.  The following operations will not impact the interoperability between existing clients and new data:   Adding a new type  Removing an existing type  Adding a new field to an existing type  Removing an existing field from an existing type.   When adding new fields or types, existing generated client APIs will ignore the new fields, and all of the fields which existed at the time of API generation will still be visible using the same methods.  When removing fields, existing generated client APIs will see null values if the methods corresponding to the removed fields are called.  When removing types, existing generated client APIs will see removed types as having no records.  It is not backwards compatible to change the type of an existing field.  The client behavior when calling a method corresponding to a field with a changed type is undefined.  Backwards compatibility often has a lot to do with the use case and semantics of the data. Hollow will always behave in the stated way for evolving data models, but it\u2019s possible that consumers require a field which starts returning null once it gets removed.  For this reason, additional caution should be exercised when removing types and fields.", 
            "title": "Maintaining Backwards Compatibility"
        }, 
        {
            "location": "/data-modeling/#movieactor-example", 
            "text": "Let's examine the  Movie  /  Actor  data model from our Getting Started Guide:  public class Movie {\n    long id;\n    String title;\n    int releaseYear;\n    List Actor  actors;\n\n    public Movie(long id, String title, int year, List Actor  actors) {\n        this.id = id;\n        this.title = title;\n        this.releaseYear = year;\n        this.actors = actors;\n    }\n}\n\npublic class Actor {\n    long actorId;\n    String actorName;\n\n    public Actor(long actorId, String actorName) {\n        this.actorId = actorId;\n        this.actorName = actorName;\n    }\n}  There are four type schemas in this data model:   Movie ,  ListOfActor ,  Actor , and  String .  Upon observing the  Movie  and  Actor  classes, the  HollowObjectMapper  will add a type for each into its  HollowWriteStateEngine , each with an  Object  schema type.  Each of these references a  String  field.  Just as in Java, a  String  references a separate  Object , and the  HollowObjectMapper  will add an  Object  schema for a type named  String  to assign to these references.   Inlined Strings in the HollowObjectMapper  To define an inlined String with the  HollowObjectMapper , define string references as  char[] .", 
            "title": "Movie/Actor Example"
        }, 
        {
            "location": "/producer-consumer/", 
            "text": "Hollow provides the mechanism for maintaining the state of data, serializing data, reading and accessing data, and a comprehensive set of tooling for manipulating and investigating data.  \n\n\nHollow does \nnot\n provide or specify the infrastructure for actually disseminating the produced blobs from the producer to the consumers.  This section will describe the infrastructure needed for a usual production deployment of Hollow.  It\u2019s entirely possible to wield the Hollow framework in ways which differ from the usage described here.  \n\n\nThe Producer Cycle\n\n\nGenerally, a producer runs a repeating \ncycle\n.  During each cycle, the producer goes through two distinct phases:\n\n\n\n\nAdding all of the data for a state to a \nHollowWriteStateEngine\n\n\nWriting \nblobs\n from a \nHollowWriteStateEngine\n\n\n\n\nTo transition back and forth between these phases, two method calls on the \nHollowWriteStateEngine\n are used.  To transition to the first phase, \nprepareForNextCycle()\n must be called.  To transition to the second phase, \nprepareForWrite()\n must be called.\n\n\nEach new generated state is assigned a unique, monotonically increasing 64-bit identifier.  State identifiers impose an ordering over states.  Later states have greater identifiers than earlier states.  The identifier is used to both identify the state and index the blobs in the published blob store.  \n\n\n\n\nState Identifiers\n\n\nTip: it is useful to derive these identifiers based on the current timestamp.\n\n\n\n\nAt the end of each cycle, the producer publishes up to three types of blobs for the resulting state -- a snapshot, a delta, and a \nreverse delta\n.\n\n\n\n\nReverse Deltas\n\n\nJust as a delta transitions between an earlier state and an adjacent later state, a reverse delta transitions between a later state and an earlier adjacent state.  A reverse delta is created by simply calling \nwriteReverseDelta(OutputStream)\n on a \nHollowBlobWriter\n.\n\n\n\n\nStoring the Blobs\n\n\nBlobs are published to a file store which is accessible by consumers.  From this blob store, consumers must be able to query for and retrieve blobs in the following ways:\n\n\n\n\nSnapshots\n: Must be queryable based on the state identifier.  If a blob store is queried for a snapshot with an identifier which does not exist, the snapshot with the greatest identifier prior to the queried identifier should be retrieved.\n\n\nDeltas\n: Must be queryable based on the state identifier \nto which\n a delta should be applied.\n\n\nReverse Deltas\n: Must be queryable based on the state identifier \nto which\n a reverse delta should be applied.\n\n\n\n\nAnnouncing the State\n\n\nOnce the necessary transitions to bring clients up to date have been written to the blob store, the availability of the state must be \nannounced\n to clients.  This simply means that a centralized location must be maintained and updated by the producer which indicates the version of the currently available state.  \n\n\nWhen this announced state is updated, usually it is desirable to have consumers realize this update as quickly as possible.  This can be accomplished either via a push notification to all consumers, or via frequent polling by consumers.\n\n\nRestoring At Startup\n\n\nThe examples of writing blobs thus far have assumed that the same \nHollowWriteStateEngine\n is held in memory for the duration of a dataset\u2019s delta chain.  However, this isn\u2019t always possible; the producer will need to be restarted from time to time due to deployment or other operational circumstances.\n\n\nIn order to produce a delta between states produced by one \nHollowWriteStateEngine\n and another, the producer can \nrestore\n the prior state upon restart, which will allow a delta and reverse delta to be produced:\n\n\nHollowReadStateEngine readEngine = /// the most recent state\n\nHollowWriteStateEngine writeEngine = new HollowWriteStateEngine();\n\n\n/// initialize the data model\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\nmapper.initialize(Movie.class);\n\nwriteEngine.restoreFrom(readEngine);\n\n\n\n\nOnce we have \nrestored\n the prior state, we can produce a delta from our producer's first cycle.  The delta will be applicable to any consumers which are on the state from which we restored.  \n\n\nNote that prior to restore, a \nHollowWriteStateEngine\n must be initialized with the schemas for its data model.  This is because the restore operation does not require that schemas exactly match between the restored state and the new state; it is legal to add or remove types and fields.\n\n\nWhen a delta is produced from a restored state after schemas have been updated, records for which all of the common fields between both schemas are unchanged will be assigned to the same ordinal and will be considered unmodified. If multiple such record pairs exist, the records to be considered unmodified will be selected arbitrarily among those pairs.\n\n\n\n\nInitializing Before Restore\n\n\nA \nHollowWriteStateEngine\n's data model may be initialized:\n\n\n\n\nvia the \nHollowObjectMapper\n by calling \ninitTypeState()\n with all top-level classes\n\n\nvia a set of schemas \nloaded from a text file\n using the \nHollowSchemaParser\n and \nHollowWriteStateCreator\n\n\n\n\n\n\nRolling Back\n\n\nWhile producing a new state, it is possible to roll back a \nHollowWriteStateEngine\n to the previous time \nprepareForNextCycle()\n was called:\n\n\npublic void runTheCycle(HollowWriteStateEngine writeEngine) {\n    writeEngine.prepareForNextCycle();\n    try {\n        addAllData(writeEngine);\n    } catch(Throwable unexpected) {\n        writeEngine.resetToLastPrepareForNextCycle();\n    }\n}\n\n\n\n\nWhen this method is called, it\u2019s as if none of the additions/removals since the last call to \nprepareForNextCycle()\n ever happened.  This action is available right up until the next call to \nprepareForNextCycle()\n is called.\n\n\nIt\u2019s best practice to wrap the code which adds data to a state engine with a try/catch block shown above.  This will cover any scenario in which a producer runs into an unexpected Exception due to an unforeseen bug in the code.  \n\n\nValidating Data\n\n\nIt likely makes sense to perform some basic \nvalidation\n on your produced data states before announcing them to clients.  This usually takes the form of loading the data into a \nHollowReadStateEngine\n on the producer, then gathering and checking some heuristics-based metrics on the data before announcement.  These validation rules will be specific to the semantics of the dataset.  If a problem is detected, send an alert and roll back the write engine, rather than announcing.  This way a delta may be produced from the previous good state.  \n\n\nConsumer Framework\n\n\nData consumers keep their local copy of a dataset current by ensuring that their state engine is always at the latest \nannounced\n data state. Consumers can arrive at a particular data state in a couple of different ways:\n\n\n\n\nAt initialization time, they will load a snapshot, which is an entire copy of the dataset to be forklifted into memory.\n\n\nAfter initialization time, they will keep their local copy of the dataset current by applying delta transitions, which are the differences between adjacent data states.\n\n\n\n\nThe \nHollowClient\n encapsulates the details of initializing and keeping a dataset up to date.  In order to accomplish this task, a few infrastructure hooks must be injected:\n\n\npublic HollowClient(HollowBlobRetriever blobRetriever,\n                    HollowAnnouncementWatcher announcementWatcher,\n                    HollowUpdateListener updateListener,\n                    HollowAPIFactory apiFactory,\n                    HollowObjectHashCodeFinder hashCodeFinder,\n                    HollowClientMemoryConfig memoryConfig)\n\n\n\n\nLet's examine each the injected hooks to the \nHollowClient\n:\n\n\n\n\nHollowBlobRetriever\n: The interface to the blob store.  This is the only hook for which a custom implementation is required.  Each of the other hooks have default implementations which may be used.\n\n\nHollowAnnouncementWatcher\n: Provides an interface to the state announcement mechanism.  Often, announcement polling logic is encapsulated inside implementations.\n\n\nHollowUpdateListener\n: Provides hooks so that actions may be taken during and after updates (e.g. indexing).\n\n\nHollowAPIFactory\n: Allows users to specify a custom-generated Hollow API to use.\n\n\nHollowClientMemoryConfig\n: Defines advanced settings related to object longevity and double snapshots.\n\n\n\n\nEach time the identifier of the currently announced state changes, \ntriggerRefresh()\n should be called on the \nHollowClient\n.  This will bring the data up to date.\n\n\nPinning Consumers\n\n\nMistakes happen.  What's important is that we can recover from them quickly.  If you accidentally publish bad data, you should be able to revert those changes quickly.  If you give your \nHollowAnnouncementWatcher\n implementation an alternate location to read the announcement from, which \noverrides\n the announcement from the consumer, then you can use this to quickly force clients to go back to any arbitrary state in the past.  We call setting a state version in this alternate location \npinning\n the consumers.\n\n\nImplementing a pinning mechanism is extremely useful and highly recommended.  You can operationally reverse data issues immediately upon discovery, so that symptoms go away while you diagnose exactly what went wrong.  This can save an enormous amount of stress and money.\n\n\n\n\nUnpinning\n\n\nIf you've pinned consumers due to a data issue, it's probably not desirable to simply 'unpin' them after the root cause is addressed.  Instead, restart the producer and instruct it to \nrestore\n from the pinned state.  It should then produce a delta which skips over all of the bad states.  Only unpin after the delta from the pinned version to a bad version is overwritten with a delta from the pinned version to the good version.\n\n\n\n\nBlob Namespaces\n\n\nEvery so often, it may be required to make changes to the data model which is incompatible with prior versions.  In this case, an older producer, which produces the older data model, should run in parallel with the newer producer, producing the newer, incompatible data model. \n\n\n\n\nIncompatible Data Model Changes\n\n\nFor details about changes which are and are not backwards compatible, see \nMaintaining Backwards Compatibility\n\n\n\n\nEach producer should write its blobs to a different \nnamespace\n, so that older consumers can read from the old data model, and newer consumers can read from the newer data model.  This will result in parallel delta chains created in these separate namespaces.  Once all consumers are upgraded and reading from the newer data model, the older producer can be shut down.\n\n\nThe method of namespacing will vary with the chosen data persistence technology.", 
            "title": "Producers and Consumers"
        }, 
        {
            "location": "/producer-consumer/#the-producer-cycle", 
            "text": "Generally, a producer runs a repeating  cycle .  During each cycle, the producer goes through two distinct phases:   Adding all of the data for a state to a  HollowWriteStateEngine  Writing  blobs  from a  HollowWriteStateEngine   To transition back and forth between these phases, two method calls on the  HollowWriteStateEngine  are used.  To transition to the first phase,  prepareForNextCycle()  must be called.  To transition to the second phase,  prepareForWrite()  must be called.  Each new generated state is assigned a unique, monotonically increasing 64-bit identifier.  State identifiers impose an ordering over states.  Later states have greater identifiers than earlier states.  The identifier is used to both identify the state and index the blobs in the published blob store.     State Identifiers  Tip: it is useful to derive these identifiers based on the current timestamp.   At the end of each cycle, the producer publishes up to three types of blobs for the resulting state -- a snapshot, a delta, and a  reverse delta .   Reverse Deltas  Just as a delta transitions between an earlier state and an adjacent later state, a reverse delta transitions between a later state and an earlier adjacent state.  A reverse delta is created by simply calling  writeReverseDelta(OutputStream)  on a  HollowBlobWriter .", 
            "title": "The Producer Cycle"
        }, 
        {
            "location": "/producer-consumer/#storing-the-blobs", 
            "text": "Blobs are published to a file store which is accessible by consumers.  From this blob store, consumers must be able to query for and retrieve blobs in the following ways:   Snapshots : Must be queryable based on the state identifier.  If a blob store is queried for a snapshot with an identifier which does not exist, the snapshot with the greatest identifier prior to the queried identifier should be retrieved.  Deltas : Must be queryable based on the state identifier  to which  a delta should be applied.  Reverse Deltas : Must be queryable based on the state identifier  to which  a reverse delta should be applied.", 
            "title": "Storing the Blobs"
        }, 
        {
            "location": "/producer-consumer/#announcing-the-state", 
            "text": "Once the necessary transitions to bring clients up to date have been written to the blob store, the availability of the state must be  announced  to clients.  This simply means that a centralized location must be maintained and updated by the producer which indicates the version of the currently available state.    When this announced state is updated, usually it is desirable to have consumers realize this update as quickly as possible.  This can be accomplished either via a push notification to all consumers, or via frequent polling by consumers.", 
            "title": "Announcing the State"
        }, 
        {
            "location": "/producer-consumer/#restoring-at-startup", 
            "text": "The examples of writing blobs thus far have assumed that the same  HollowWriteStateEngine  is held in memory for the duration of a dataset\u2019s delta chain.  However, this isn\u2019t always possible; the producer will need to be restarted from time to time due to deployment or other operational circumstances.  In order to produce a delta between states produced by one  HollowWriteStateEngine  and another, the producer can  restore  the prior state upon restart, which will allow a delta and reverse delta to be produced:  HollowReadStateEngine readEngine = /// the most recent state\n\nHollowWriteStateEngine writeEngine = new HollowWriteStateEngine();\n\n\n/// initialize the data model\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\nmapper.initialize(Movie.class);\n\nwriteEngine.restoreFrom(readEngine);  Once we have  restored  the prior state, we can produce a delta from our producer's first cycle.  The delta will be applicable to any consumers which are on the state from which we restored.    Note that prior to restore, a  HollowWriteStateEngine  must be initialized with the schemas for its data model.  This is because the restore operation does not require that schemas exactly match between the restored state and the new state; it is legal to add or remove types and fields.  When a delta is produced from a restored state after schemas have been updated, records for which all of the common fields between both schemas are unchanged will be assigned to the same ordinal and will be considered unmodified. If multiple such record pairs exist, the records to be considered unmodified will be selected arbitrarily among those pairs.   Initializing Before Restore  A  HollowWriteStateEngine 's data model may be initialized:   via the  HollowObjectMapper  by calling  initTypeState()  with all top-level classes  via a set of schemas  loaded from a text file  using the  HollowSchemaParser  and  HollowWriteStateCreator", 
            "title": "Restoring At Startup"
        }, 
        {
            "location": "/producer-consumer/#rolling-back", 
            "text": "While producing a new state, it is possible to roll back a  HollowWriteStateEngine  to the previous time  prepareForNextCycle()  was called:  public void runTheCycle(HollowWriteStateEngine writeEngine) {\n    writeEngine.prepareForNextCycle();\n    try {\n        addAllData(writeEngine);\n    } catch(Throwable unexpected) {\n        writeEngine.resetToLastPrepareForNextCycle();\n    }\n}  When this method is called, it\u2019s as if none of the additions/removals since the last call to  prepareForNextCycle()  ever happened.  This action is available right up until the next call to  prepareForNextCycle()  is called.  It\u2019s best practice to wrap the code which adds data to a state engine with a try/catch block shown above.  This will cover any scenario in which a producer runs into an unexpected Exception due to an unforeseen bug in the code.", 
            "title": "Rolling Back"
        }, 
        {
            "location": "/producer-consumer/#validating-data", 
            "text": "It likely makes sense to perform some basic  validation  on your produced data states before announcing them to clients.  This usually takes the form of loading the data into a  HollowReadStateEngine  on the producer, then gathering and checking some heuristics-based metrics on the data before announcement.  These validation rules will be specific to the semantics of the dataset.  If a problem is detected, send an alert and roll back the write engine, rather than announcing.  This way a delta may be produced from the previous good state.", 
            "title": "Validating Data"
        }, 
        {
            "location": "/producer-consumer/#consumer-framework", 
            "text": "Data consumers keep their local copy of a dataset current by ensuring that their state engine is always at the latest  announced  data state. Consumers can arrive at a particular data state in a couple of different ways:   At initialization time, they will load a snapshot, which is an entire copy of the dataset to be forklifted into memory.  After initialization time, they will keep their local copy of the dataset current by applying delta transitions, which are the differences between adjacent data states.   The  HollowClient  encapsulates the details of initializing and keeping a dataset up to date.  In order to accomplish this task, a few infrastructure hooks must be injected:  public HollowClient(HollowBlobRetriever blobRetriever,\n                    HollowAnnouncementWatcher announcementWatcher,\n                    HollowUpdateListener updateListener,\n                    HollowAPIFactory apiFactory,\n                    HollowObjectHashCodeFinder hashCodeFinder,\n                    HollowClientMemoryConfig memoryConfig)  Let's examine each the injected hooks to the  HollowClient :   HollowBlobRetriever : The interface to the blob store.  This is the only hook for which a custom implementation is required.  Each of the other hooks have default implementations which may be used.  HollowAnnouncementWatcher : Provides an interface to the state announcement mechanism.  Often, announcement polling logic is encapsulated inside implementations.  HollowUpdateListener : Provides hooks so that actions may be taken during and after updates (e.g. indexing).  HollowAPIFactory : Allows users to specify a custom-generated Hollow API to use.  HollowClientMemoryConfig : Defines advanced settings related to object longevity and double snapshots.   Each time the identifier of the currently announced state changes,  triggerRefresh()  should be called on the  HollowClient .  This will bring the data up to date.", 
            "title": "Consumer Framework"
        }, 
        {
            "location": "/producer-consumer/#pinning-consumers", 
            "text": "Mistakes happen.  What's important is that we can recover from them quickly.  If you accidentally publish bad data, you should be able to revert those changes quickly.  If you give your  HollowAnnouncementWatcher  implementation an alternate location to read the announcement from, which  overrides  the announcement from the consumer, then you can use this to quickly force clients to go back to any arbitrary state in the past.  We call setting a state version in this alternate location  pinning  the consumers.  Implementing a pinning mechanism is extremely useful and highly recommended.  You can operationally reverse data issues immediately upon discovery, so that symptoms go away while you diagnose exactly what went wrong.  This can save an enormous amount of stress and money.   Unpinning  If you've pinned consumers due to a data issue, it's probably not desirable to simply 'unpin' them after the root cause is addressed.  Instead, restart the producer and instruct it to  restore  from the pinned state.  It should then produce a delta which skips over all of the bad states.  Only unpin after the delta from the pinned version to a bad version is overwritten with a delta from the pinned version to the good version.", 
            "title": "Pinning Consumers"
        }, 
        {
            "location": "/producer-consumer/#blob-namespaces", 
            "text": "Every so often, it may be required to make changes to the data model which is incompatible with prior versions.  In this case, an older producer, which produces the older data model, should run in parallel with the newer producer, producing the newer, incompatible data model.    Incompatible Data Model Changes  For details about changes which are and are not backwards compatible, see  Maintaining Backwards Compatibility   Each producer should write its blobs to a different  namespace , so that older consumers can read from the old data model, and newer consumers can read from the newer data model.  This will result in parallel delta chains created in these separate namespaces.  Once all consumers are upgraded and reading from the newer data model, the older producer can be shut down.  The method of namespacing will vary with the chosen data persistence technology.", 
            "title": "Blob Namespaces"
        }, 
        {
            "location": "/tooling/", 
            "text": "This section describes the usage of some of the tooling which ships with Hollow, but the tools described here are by no means a comprehensive accounting of the things you can do with your data once it's Hollow.  We hope that you'll find it straightforward to use the basic building blocks provided by the Hollow framework in different ways to create new tooling \u2014 and then contribute back any tools which you develop for your use case and find useful.\n\n\nInsight Tools\n\n\nHistory tool\n\n\nHollow provides the ability to retain, in memory, the changes in a dataset over many states, and to easily access historical data.  This is accomplished via the  \nHollowHistory\n class:\n\n\npublic HollowHistory(HollowReadStateEngine initialHollowStateEngine, \n                     long initialVersion, \n                     int maxHistoricalStatesToKeep)\n\n\n\n\n\n\nState Versioning\n\n\nThe \ninitialVersion\n parameter above should be a unique value identifying the state.\n\n\n\n\nThe \nHollowHistory\n should be configured with the primary keys of records for which we are interested in tracking history.  For example, using our \nMovie\n/\nActor\n example from the Getting Started guide, we may specify the following configuration:\n\n\nHollowHistory history = new HollowHistory(readEngine, 1, 1000);\nHollowHistoryKeyIndex historyIdx = history.getKeyIndex();\n\nhistoryIdx.addTypeIndex(\nMovie\n, \nid\n);\nhistoryIdx.indexTypeField(\nMovie\n, \nid\n);\n\nhistoryIdx.addTypeIndex(\nActor\n, \nactorId\n);\n\n\n\n\nNotice there are two types of calls available to the \nHollowHistoryKeyIndex\n:\n\n\n\n\nThe \naddTypeIndex()\n call specifies the primary key for a type which we want to be able to view historical changes over time.  Primary keys may be defined over multiple fields.  The final parameter in the \naddTypeIndex()\n call is a vararg.  \n\n\nThe \nindexTypeField()\n call specifies an individual primary key field over which we want to be able to \nsearch\n for historical changes over time.\n\n\n\n\n\n\nPrimary Keys\n\n\nThe \nHollowHistory\n will, by default, automatically configure any primary keys which are defined in the \nObject\n schemas of your dataset.  However, the calls to \nindexTypeField()\n will not be automatically configured.\n\n\n\n\nOnce instantiated and configured, the \nHollowHistory\n should be notified each time the state engine is transitioned via the \ndeltaOccurred(long newVersion)\n method.  The \nHollowHistory\n will track the entire dataset for each state which through which the state engine is transitioned.  \n\n\nThis historical data is maintained by retaining and indexing all of the changes for the delta chain in memory.  Because only changes over time are retained, rather than complete states, a great length of history can often be held in memory.\n\n\nHollow includes a ready-made UI which can be applied to a \nHollowHistory\n for any dataset.  The included UI clearly displays the changes which occur between adjacent states as the state engine transitions through a delta chain.  This will allow users to quickly realize all of the benefits of indexed, historical data retention at their fingertips.  \n\n\nThe \nHollowHistoryUI\n class in the \nhollow-diff-ui\n project is instantiated using a \nHollowHistory\n and a base URL path.  Incoming requests should be sent to the handle method:\n\n\npublic boolean handle(String target, \n                      HttpServletRequest req, \n                      HttpServletResponse resp) throws IOException\n\n\n\n\nThe \nHollowHistoryUI\n can be used in the context of an existing web container, or can be invoked via the included \nHollowHistoryUIServer\n, which uses the Jetty HTTP Servlet Server:\n\n\nHollowHistory history = /// set up the history;\n\nHollowHistoryUI ui = new HollowHistoryUI(\n, history);\nHollowHistoryUIServer server = new HollowHistoryUIServer(ui, 8080);\n\nserver.start();\nserver.join();\n\n\n\n\nWhile the above code is running, you can point a browser to \nhttp://localhost:8080\n to explore the history.\n\n\nRight out of the box, the history tool provides the ability to get a bird\u2019s eye view of all of the changes a dataset goes through over time, while simultaneously allowing for specific queries to see exactly how individual records change as the dataset transitions between states.  The history tool has proven to be enormously beneficial when investigating data issues in production scenarios.  When something looks incorrect, it\u2019s easy to pinpoint exactly what changed when, which can vastly expedite data corrections and eliminate hours of potential detective work.\n\n\nDiff Tool\n\n\nJust as the Hollow history tool UI makes the differences between any two \nadjacent\n states in a delta chain readily accessible, the Hollow diff tool is used to investigate the differences between any two \narbitrary\n data states, even those which may exist in different delta chains. \n\n\nThis is especially useful as a step in a regular release cadence, as the differences between data states produced, for example, in a test environment and production environment can be evaluated at a glance.  Sometimes, unintended consequences of code updates may be discovered this way, which prevents production issues before they happen.\n\n\nInitiating a diff between two data states is accomplished by loading both states into separate \nHollowReadStateEngines\n in memory, and then instantiating a \nHollowDiff\n and configuring it with the primary keys of types to diff.  For our \nMovie\n/\nActor\n example:\n\n\nHollowReadStateEngine testData = /// load test data\nHollowReadStateEngine prodData = /// load test data\n\nHollowDiff diff = new HollowDiff(testData, prodData);\ndiff.addTypeDiff(\nMovie\n, \nid\n);\ndiff.addTypeDiff(\nActor\n, \nactorId\n);\n\ndiff.calculateDiffs();\n\n\n\n\nA diff is calculated by matching records of the same type based on defined primary keys.  The unmatched records in both states are tracked, and detailed differences between field values in matching pairs are also tracked.\n\n\n\n\nPrimary Keys\n\n\nThe \nHollowDiff\n will, by default, automatically configure any primary keys which are defined in the \nObject\n schemas of your dataset. \n\n\n\n\nHollow includes a ready-made UI which can be applied to a \nHollowDiff\n.    The \nHollowDiffUI\n class can be used in the context of an existing web container, or can be invoked via the \nHollowDiffUIServer\n, which uses the Jetty HTTP Servlet Server:\n\n\nHollowDiff diff = /// build the diff\n\nHollowDiffUIServer server = new HollowDiffUIServer(8080);\nserver.start();\n\nserver.addDiff(\ndiff\n, diff);\n\nserver.join();\n\n\n\n\nWhile the above code is running, you can point a browser to \nhttp://localhost:8080\n to explore the diff.\n\n\nHeap Usage Analysis\n\n\nOne of the most important considerations when dealing with in-memory datasets is the heap utilization of that dataset on consumer machines.  Hollow provides a number of methods to analyze this metric.\n\n\nGiven a loaded \nHollowReadStateEngine\n, it is possible to iterate over each type and gather statistics about its approximate heap usage.  This is done in the following example:\n\n\nHollowReadStateEngine stateEngine = /// a populated state engine\n\nlong totalApproximateHeapFootprint = 0;\n\nfor(HollowTypeReadState typeState : stateEngine.getTypeStates()) {\n    String typeName = typeState.getSchema().getName();\n    long heapCost = typeState.getApproximateHeapFootprintInBytes();\n    System.out.println(typeName + \n: \n + heapCost);\n    totalApproximateHeapFootprint += heapCost;\n}\n\nSystem.out.println(\nTOTAL: \n + totalApproximateHeapFootprint);\n\n\n\n\nAs shown above, information can be gathered about the total heap footprint, and also about the heap footprint of individual types.  This information can be helpful in identifying optimization targets.  This technique can also be used to identify how the heap cost of individual types changes over time, which can provide early warning signs about optimizations which should be targeted proactively.\n\n\nUsage Tracking\n\n\nHollow tracks usage, which can be investigated at runtime.  By default, this functionality is turned off, but it can be enabled by injecting a HollowSamplingDirector into a Hollow API in a running instance.  You can use the TimeSliceSamplingDirector implementation, which will by default record every access which happens during 1ms out of every second:\n\n\nMovieAPI api = /// a custom-generated API\n\nTimeSliceSamplingDirector samplingDirector = new TimeSliceSamplingDirector();\nsamplingDirector.startSampling();\n\napi.setSamplingDirector(samplingDirector);\n\n\n\n\nOnce this is enabled, and some time has passed for samples to be gathered, the results can be collected for analysis:\n\n\nfor(SampleResult result : api.getAccessSampleResults()) {\n    if(result.getNumSamples() \n 0)\n        System.out.println(result.getIdentifier() + \n: \n + \n                                                  result.getNumSamples());\n}\n\n\n\n\nTransitive Set Traverser\n\n\nThe \nTransitiveSetTraverser\n can be used to find children and parent references for a selected set of records.  We start with an initial set of selected records by ordinal, represented with a \nMap\nString, BitSet\n.  Entries in this map will indicate a type, plus the ordinals of the selected records:\n\n\nMap\nString, BitSet\n selection = new HashMap\nString, BitSet\n();\n\n/// select the movies with IDs 1 and 6.\nBitSet selectedMovies = new BitSet();\nselectedMovies.set(movieIdx.getMatchingOrdinal(1));\nselectedMovies.set(movieIdx.getMatchingOrdinal(6));\n\nselection.put(\nMovie\n, movies);\n\n\n\n\nWe can add the references, and the \ntransitive references\n of our selection.  After the following call returns, our selection will be augmented with these matches:\n\n\nTransitiveSetTraverser.addTransitiveMatches(readEngine, selection);\n\n\n\n\n\n\nTransitive References\n\n\nIf A references B, and B references C, then A transitively references C\n\n\n\n\nGiven a selection, we can also add any records which reference anything in the selection.  This is essentially the opposite operation as above; it can be said that \naddTransitiveMatches\n traverses down, while \naddReferencingOutsideClosure\n traverses up.  After the following call returns, our selection will be augmented with this selection:\n\n\nTransitiveSetTraverser.removedReferencedOutsideClosure(readEngine, selection);\n\n\n\n\nDataset Manipulation Tools\n\n\nFiltering\n\n\nSometimes, a dataset will be of interest to multiple different types of consumers, but not all consumers may be interested in all aspects of a dataset.  In these cases, it\u2019s possible to omit certain types and fields from a client\u2019s view of the data.  This is typically done to tailor a consumer\u2019s heap footprint and startup time costs based on their data needs.\n\n\nUsing our \nMovie\n/\nActor\n example above, if there was a consumer which was interested in \nMovie\n records, but not \nActor\n records, that consumer might construct a consumer-side data filter configuration in the following way:\n\n\nHollowFilterConfig config = new HollowFilterConfig(true);\nconfig.addField(\nMovie\n, \nactors\n);\nconfig.addType(\nListOfActor\n);\nconfig.addType(\nActor\n);\n\n\n\n\nThe boolean \ntrue\n parameter in the constructor above indicates that this is an exclusion filter.  We could accomplish the same goal using an inclusion filter:\n\n\nHollowFilterConfig config = new HollowFilterConfig(false);\nconfig.addField(\nMovie\n, \nid\n);\nconfig.addField(\nMovie\n, \ntitle\n);\nconfig.addField(\nMovie\n, \nreleaseYear\n);\nconfig.addType(\nString\n);\n\n\n\n\nThe difference between these two configurations is how the filter behaves as new types and fields are added to the data model.  The exclusion filter will not exclude them by default, whereas the inclusion filter will.\n\n\nA filter configuration is applied to a state engine at read time:\n\n\nHollowBlobReader reader = /// a blob reader\nInputStream stream = /// a stream of the snapshot\nHollowFilterConfig config = /// the filter configuration\n\nreader.readSnapshot(inputStream, filter);\n\n\n\n\nCombining\n\n\nThe \nHollowCombiner\n is used to copy data from one or more copies of hollow datasets in \nHollowReadStateEngine\ns into a single \nHollowWriteStateEngine\n.  If each of the inputs contain the same data model, the following is sufficient to combine them:\n\n\nHollowReadStateEngine input1 = /// an input\nHollowReadStateEngine input2 = /// another input\n\nHollowCombiner combiner = new HollowCombiner(input1, input2);\ncombiner.combine();\n\nHollowWriteStateEngine combined = combiner.getCombinedStateEngine();\n\n\n\n\nBy default, the combiner will copy all records from all types from the inputs to the output.  We can direct the combiner to exclude certain records from copying using a \nHollowCombinerCopyDirector\n.  The interface for a \nHollowCombinerCopyDirector\n allows for making decisions about copying individual records during a combine operation by implementing the following method:\n\n\npublic boolean shouldCopy(HollowTypeReadState typeState, int ordinal);\n\n\n\n\nIf this method returns false, then the copier will not attempt to directly copy the matching record.  However, if the matching record is referenced via another record for which this method returns true, then it will still be copied regardless of the return value of this method.\n\n\nThe most broadly useful provided implementation of the \nHollowCombinerCopyDirector\n is the \nHollowCombinerExcludePrimaryKeysCopyDirector\n, which can be used to specify record exclusions by primary key.  For example, if we wanted to create a copy of a state engine with the \nMovie\n records with ids 100 and 125 excluded:\n\n\nHollowReadStateEngine input = /// an input\nHollowPrimaryKeyIndex idx = new HollowPrimaryKeyIndex(input, \nMovie\n, \nid\n);\n\nHollowCombinerExcludePrimaryKeysCopyDirector director = \n                          new HollowCombinerExcludePrimaryKeysCopyDirector();\n\ndirector.excludeKey(idx, 100);\ndirector.excludeKey(idx, 125);\n\nHollowCombiner combiner = new HollowCombiner(director, input);\ncombiner.combine();\n\nHollowWriteStateEngine result = combiner.getCombineStateEngine();\n\n\n\n\nIt\u2019s possible that while combining two inputs, both may have a record of the same type with the same primary key.  This violation of the uniqueness constraint of a primary key can be avoided by informing the combiner of the primary keys in a data model prior to the combine operation:\n\n\nHollowCombiner combiner = new HollowCombiner(input1, input2);\n\ncombiner.setPrimaryKeys(\n        new PrimaryKey(\nMovie\n, \nid\n),\n        new PrimaryKey(\nActor\n, \nactorId\n)\n);\n\ncombiner.combine();\n\n\n\n\nIf multiple records exist in the inputs matching a single value for any of the supplied primary keys, then only one such record will be copied to the output.  The specific record which is copied will be the record from the input was supplied earliest in the constructor of the \nHollowCombiner\n.  Further, if any record references another record which was omitted because it would have been duplicate based on this rule, then that reference is remapped in the output state to the matching record which was chosen to be included.\n\n\nSplitting\n\n\nA single dataset can be sharded into multiple datasets using a \nHollowSplitter\n.  The \nHollowSplitter\n takes a \nHollowSplitterCopyDirector\n, which indicates:\n\n\n\n\ntop level\n types to split,\n\n\nthe number of shards to create, and \n\n\nwhich shard to send individual records.\n\n\n\n\n\n\nTop Level Types\n\n\nTop level types are those which are not referenced by any other types.  In our \nMovie\n/\nActor\n example, \nMovie\n is a top-level type, but \nActor\n is not.\n\n\n\n\nTwo default implementations of \nHollowSplitterCopyDirector\n are available: \n\n\n\n\nHollowSplitterOrdinalCopyDirector\n\n\nHollowSplitterPrimaryKeyCopyDirector\n.  \n\n\n\n\nThese directors will split top-level types among a specified number of shards either by ordinals or primary keys, respectively.  When splitting by ordinal, a record with a specific primary key may jump between shards when it is modified, while with the primary key director a specific primary key will consistently hash to the same shard.\n\n\nOur \nMovie\n/\nActor\n example may use the splitter to split a dataset into four shards with the following invocation:\n\n\nHollowReadStateEngine stateEngine = /// a state engine\n\nHollowSplitterCopyDirector director = \n                            new HollowSplitterOrdinalCopyDirector(4, \nMovie\n);\n\nHollowSplitter splitter = new HollowSplitter(director, stateEngine);\nsplitter.split();\n\n\nfor(int i=0; i\n4; i++) {\n    HollowWriteStateEngine shard = splitter.getOutputShardStateEngine(i);\n}\n\n\n\n\nState Manipulation Tools\n\n\nPatching\n\n\nUsing the \nHollowWriteStateEngine\n\u2019s restore capability, it\u2019s possible to produce deltas forever, so that consumers never have to load a snapshot after initialization.  However, if environmental hiccups cause a producer to fail to publish a delta, or if a delta is lost, or if it\u2019s desired to publish a delta between non-adjacent states, then the \nHollowStateDeltaPatcher\n may be used to produce deltas between two arbitrary states within the same delta chain.\n\n\nThe \nHollowStateDeltaPatcher\n must produce \ntwo\n delta transitions to create a transition between arbitrary states.  This is because non-adjacent states may have different records occupying the same ordinals.  Since no ordinal may be removed and added in adjacent states, the state patcher must create an intermediate state in which modified records do not share any ordinals.\n\n\nSee the \nHollowStateDeltaPatcher\n javadocs for usage details.\n\n\nCompacting\n\n\nIt is possible to produce delta chains which extend over many thousands of states.  If during this delta chain an especially large delta happens for a specific type, it\u2019s possible that many ordinal holes will be present in that type.  If over time multiple types go through especially large deltas, this can have an impact on a dataset\u2019s heap footprint.\n\n\nTo reclaim heap space occupied by ordinal holes, the \nHollowCompactor\n may be used to move records off of the high end of the ordinal space into these holes.  This is accomplished by producing deltas which only include removals and additions of identical records allocated to more optimal ordinals.  See the \nHollowCompactor\n javadocs for usage details.\n\n\nIndexing / Querying\n\n\nPrimary Keys\n\n\nIn our \nMovie\n/\nActor\n example from the Getting Started guide, we saw that we can easily create a \nHollowPrimaryKeyIndex\n which will allow us to query for \nMovie\n records by id:\n\n\nHollowPrimaryKeyIndex idx = \n                      new HollowPrimaryKeyIndex(readEngine, \nMovie\n, \nid\n);\nidx.listenForDeltaUpdates();\n\n\n\n\nIn that example, the primary key was defined for \nMovie\n as its \nid\n field.  A primary key can also be defined over multiple and/or hierarchical fields.  Imagine that \nMovie\n additionally had a \ncountry\n field defined in its schema, and that across countries, \nMovie\n \nid\ns may be duplicated, but that there will never exist two \nMovie\n records with the same id and country:\n\n\npublic class Movie {\n    long id;\n    Country country;\n    String title;\n    int releaseYear;\n}\n\n\npublic class Country {\n    String id;\n    String name;\n}\n\n\n\n\nA \nHollowPrimaryKeyIndex\n can be defined with a primary key consisting of both fields:\n\n\nHollowPrimaryKeyIndex idx = \n            new HollowPrimaryKeyIndex(readEngine, \nMovie\n, \nid\n, \ncountry.id.value\n);\nidx.listenForDeltaUpdates();\n\n\n\n\nAnd to query for a \nMovie\n based on its id and country:\n\n\nint movieOrdinal = idx.getMatchingOrdinal(2, \nUS\n);\nif(movieOrdinal != -1) {\n    MovieHollow movie = movieApi.getMovieHollow(movieOrdinal);\n    System.out.println(\nFound Movie: \n + movie._getTitle()._getValue());\n}\n\n\n\n\nNotice that \nMovie\n\u2019s country field in the above example is actually a \nREFERENCE\n field.  The defined key includes the id of the movie, and the value of the id String of the referenced country.  We denote this traversal using dot notation in the primary key definition.  The field definitions can be multiple references deep.\n\n\nThe requirement for a primary key definition is that no duplicates should exist for the defined combination of fields.  If this rule is violated, an arbitrary match will be returned for queries when multiple matches exist.  \n\n\n\n\nPrimary Key Violations\n\n\nViolations of the \"no duplicate\" primary key rule can be detected using the \ngetDuplicateKeys()\n method on a \nHollowPrimaryKeyIndex\n, which returns a \nCollection\nObject[]\n.  If no duplicate keys exist, the returned Collection will be empty.  If they do, the returned values will indicate the keys for which duplicate records exist.\n\n\n\n\nIf a \nHollowPrimaryKeyIndex\n will be retained for a long duration, they should be kept updated as deltas are applied to the underlying \nHollowReadStateEngine\n.  This is accomplished with a single call after instantiation to the \nlistenForDeltaUpdates()\n method. \n\n\n\n\nDetaching Primary Key Indexes\n\n\nIf \nlistenForDeltaUpdates()\n is called on a primary key index, then it cannot be garbage collected.  If you intend to drop an index which is listening for updates, first call \ndetachFromDeltaUpdates()\n to prevent a memory leak.\n\n\n\n\nIndexes which are listening for delta updates are updated after a dataset is updated.  In the brief interim time between when a dataset is updated and the index is updated, the index will point to the \nghost records\n located at tombstoned ordinals.  This helps guarantee that all in-flight operations will observe correct data.\n\n\nHash Indexes\n\n\nIt is sometimes desirable to index records by fields other than primary keys.  The \nHollowHashIndex\n allows for indexing records by fields or combinations of fields for which values may match multiple records, and records may match multiple values.\n\n\nIn our \nMovie\n/\nActor\n example, we may want to index movies by their starring actors:\n\n\nHollowHashIndex idx = \n            new HollowHashIndex(readEngine, \nMovie\n, \n, \nactors.element.id\n);\n\n\n\n\nThe \nHollowHashIndex\n expects in its constructor arguments a query start type, a select field, and a set of match fields.  The constructor arguments above indicate that queries will start with the \nMovie\n type, select the root of the query (indicated by the empty string), and match the id of any \nActor\n record in the actors list.\n\n\nTo query this index:\n\n\nHollowHashIndexResult result = idx.findMatches(102);\n\nif(result != null) {\n    System.out.println(\nFound matches: \n + result.numResults());\n\n    HollowOrdinalIterator iter = result.iterator();\n    int matchedOrdinal = iter.next();\n    while(matchedOrdinal != HollowOrdinalIterator.NO_MORE_ORDINALS) {\n        MovieHollow movie = api.getMovieHollow(matchedOrdinal);\n        System.out.println(\nStarred in: \n + movie._getTitle()._getValue());\n        matchedOrdinal = iter.next();\n    }\n}\n\n\n\n\nAlternatively, if the data model included the nationality of Actors, and we needed to index Actors by nationality and the titles of movies in which they starred:\n\n\nHollowHashIndex idx = \n            new HollowHashIndex(readEngine, \nMovie\n, \nactors.element\n,\n                                            \ntitle.value\n,\n                                            \nactors.element.nationality.id\n);\n\n\n\n\nIn this case, the query start type is still \nMovie\n, but we\u2019re selecting related \nActors\n.  Matches are selected based on the \nMovie\n\u2019s title, and the actor\u2019s nationality.  Using this index, one can query for Brazilian actors who starred in movies titled \u201cNarcos\u201d:\n\n\nHollowHashIndexResult result = idx.findMatches(\nNarcos\n, \nBR\n);\n\nif(result != null) {\n    HollowOrdinalIterator iter = result.iterator();\n    int matchedOrdinal = iter.next();\n    while(matchedOrdinal != HollowOrdinalIterator.NO_MORE_ORDINALS) {\n        ActorHollow actor = api.getMovieHollow(matchedOrdinal);\n        System.out.println(\nMatched actor: \n + \n                                      actor._getActorName()._getValue());\n        matchedOrdinal = iter.next();\n    }\n}\n\n\n\n\nThe \nHollowHashIndex\n does not yet have a facility for listening for delta updates.  If an index is necessary across multiple states, currently the index must be recreated on each update.", 
            "title": "Tooling"
        }, 
        {
            "location": "/tooling/#insight-tools", 
            "text": "", 
            "title": "Insight Tools"
        }, 
        {
            "location": "/tooling/#history-tool", 
            "text": "Hollow provides the ability to retain, in memory, the changes in a dataset over many states, and to easily access historical data.  This is accomplished via the   HollowHistory  class:  public HollowHistory(HollowReadStateEngine initialHollowStateEngine, \n                     long initialVersion, \n                     int maxHistoricalStatesToKeep)   State Versioning  The  initialVersion  parameter above should be a unique value identifying the state.   The  HollowHistory  should be configured with the primary keys of records for which we are interested in tracking history.  For example, using our  Movie / Actor  example from the Getting Started guide, we may specify the following configuration:  HollowHistory history = new HollowHistory(readEngine, 1, 1000);\nHollowHistoryKeyIndex historyIdx = history.getKeyIndex();\n\nhistoryIdx.addTypeIndex( Movie ,  id );\nhistoryIdx.indexTypeField( Movie ,  id );\n\nhistoryIdx.addTypeIndex( Actor ,  actorId );  Notice there are two types of calls available to the  HollowHistoryKeyIndex :   The  addTypeIndex()  call specifies the primary key for a type which we want to be able to view historical changes over time.  Primary keys may be defined over multiple fields.  The final parameter in the  addTypeIndex()  call is a vararg.    The  indexTypeField()  call specifies an individual primary key field over which we want to be able to  search  for historical changes over time.    Primary Keys  The  HollowHistory  will, by default, automatically configure any primary keys which are defined in the  Object  schemas of your dataset.  However, the calls to  indexTypeField()  will not be automatically configured.   Once instantiated and configured, the  HollowHistory  should be notified each time the state engine is transitioned via the  deltaOccurred(long newVersion)  method.  The  HollowHistory  will track the entire dataset for each state which through which the state engine is transitioned.    This historical data is maintained by retaining and indexing all of the changes for the delta chain in memory.  Because only changes over time are retained, rather than complete states, a great length of history can often be held in memory.  Hollow includes a ready-made UI which can be applied to a  HollowHistory  for any dataset.  The included UI clearly displays the changes which occur between adjacent states as the state engine transitions through a delta chain.  This will allow users to quickly realize all of the benefits of indexed, historical data retention at their fingertips.    The  HollowHistoryUI  class in the  hollow-diff-ui  project is instantiated using a  HollowHistory  and a base URL path.  Incoming requests should be sent to the handle method:  public boolean handle(String target, \n                      HttpServletRequest req, \n                      HttpServletResponse resp) throws IOException  The  HollowHistoryUI  can be used in the context of an existing web container, or can be invoked via the included  HollowHistoryUIServer , which uses the Jetty HTTP Servlet Server:  HollowHistory history = /// set up the history;\n\nHollowHistoryUI ui = new HollowHistoryUI( , history);\nHollowHistoryUIServer server = new HollowHistoryUIServer(ui, 8080);\n\nserver.start();\nserver.join();  While the above code is running, you can point a browser to  http://localhost:8080  to explore the history.  Right out of the box, the history tool provides the ability to get a bird\u2019s eye view of all of the changes a dataset goes through over time, while simultaneously allowing for specific queries to see exactly how individual records change as the dataset transitions between states.  The history tool has proven to be enormously beneficial when investigating data issues in production scenarios.  When something looks incorrect, it\u2019s easy to pinpoint exactly what changed when, which can vastly expedite data corrections and eliminate hours of potential detective work.", 
            "title": "History tool"
        }, 
        {
            "location": "/tooling/#diff-tool", 
            "text": "Just as the Hollow history tool UI makes the differences between any two  adjacent  states in a delta chain readily accessible, the Hollow diff tool is used to investigate the differences between any two  arbitrary  data states, even those which may exist in different delta chains.   This is especially useful as a step in a regular release cadence, as the differences between data states produced, for example, in a test environment and production environment can be evaluated at a glance.  Sometimes, unintended consequences of code updates may be discovered this way, which prevents production issues before they happen.  Initiating a diff between two data states is accomplished by loading both states into separate  HollowReadStateEngines  in memory, and then instantiating a  HollowDiff  and configuring it with the primary keys of types to diff.  For our  Movie / Actor  example:  HollowReadStateEngine testData = /// load test data\nHollowReadStateEngine prodData = /// load test data\n\nHollowDiff diff = new HollowDiff(testData, prodData);\ndiff.addTypeDiff( Movie ,  id );\ndiff.addTypeDiff( Actor ,  actorId );\n\ndiff.calculateDiffs();  A diff is calculated by matching records of the same type based on defined primary keys.  The unmatched records in both states are tracked, and detailed differences between field values in matching pairs are also tracked.   Primary Keys  The  HollowDiff  will, by default, automatically configure any primary keys which are defined in the  Object  schemas of your dataset.    Hollow includes a ready-made UI which can be applied to a  HollowDiff .    The  HollowDiffUI  class can be used in the context of an existing web container, or can be invoked via the  HollowDiffUIServer , which uses the Jetty HTTP Servlet Server:  HollowDiff diff = /// build the diff\n\nHollowDiffUIServer server = new HollowDiffUIServer(8080);\nserver.start();\n\nserver.addDiff( diff , diff);\n\nserver.join();  While the above code is running, you can point a browser to  http://localhost:8080  to explore the diff.", 
            "title": "Diff Tool"
        }, 
        {
            "location": "/tooling/#heap-usage-analysis", 
            "text": "One of the most important considerations when dealing with in-memory datasets is the heap utilization of that dataset on consumer machines.  Hollow provides a number of methods to analyze this metric.  Given a loaded  HollowReadStateEngine , it is possible to iterate over each type and gather statistics about its approximate heap usage.  This is done in the following example:  HollowReadStateEngine stateEngine = /// a populated state engine\n\nlong totalApproximateHeapFootprint = 0;\n\nfor(HollowTypeReadState typeState : stateEngine.getTypeStates()) {\n    String typeName = typeState.getSchema().getName();\n    long heapCost = typeState.getApproximateHeapFootprintInBytes();\n    System.out.println(typeName +  :   + heapCost);\n    totalApproximateHeapFootprint += heapCost;\n}\n\nSystem.out.println( TOTAL:   + totalApproximateHeapFootprint);  As shown above, information can be gathered about the total heap footprint, and also about the heap footprint of individual types.  This information can be helpful in identifying optimization targets.  This technique can also be used to identify how the heap cost of individual types changes over time, which can provide early warning signs about optimizations which should be targeted proactively.", 
            "title": "Heap Usage Analysis"
        }, 
        {
            "location": "/tooling/#usage-tracking", 
            "text": "Hollow tracks usage, which can be investigated at runtime.  By default, this functionality is turned off, but it can be enabled by injecting a HollowSamplingDirector into a Hollow API in a running instance.  You can use the TimeSliceSamplingDirector implementation, which will by default record every access which happens during 1ms out of every second:  MovieAPI api = /// a custom-generated API\n\nTimeSliceSamplingDirector samplingDirector = new TimeSliceSamplingDirector();\nsamplingDirector.startSampling();\n\napi.setSamplingDirector(samplingDirector);  Once this is enabled, and some time has passed for samples to be gathered, the results can be collected for analysis:  for(SampleResult result : api.getAccessSampleResults()) {\n    if(result.getNumSamples()   0)\n        System.out.println(result.getIdentifier() +  :   + \n                                                  result.getNumSamples());\n}", 
            "title": "Usage Tracking"
        }, 
        {
            "location": "/tooling/#transitive-set-traverser", 
            "text": "The  TransitiveSetTraverser  can be used to find children and parent references for a selected set of records.  We start with an initial set of selected records by ordinal, represented with a  Map String, BitSet .  Entries in this map will indicate a type, plus the ordinals of the selected records:  Map String, BitSet  selection = new HashMap String, BitSet ();\n\n/// select the movies with IDs 1 and 6.\nBitSet selectedMovies = new BitSet();\nselectedMovies.set(movieIdx.getMatchingOrdinal(1));\nselectedMovies.set(movieIdx.getMatchingOrdinal(6));\n\nselection.put( Movie , movies);  We can add the references, and the  transitive references  of our selection.  After the following call returns, our selection will be augmented with these matches:  TransitiveSetTraverser.addTransitiveMatches(readEngine, selection);   Transitive References  If A references B, and B references C, then A transitively references C   Given a selection, we can also add any records which reference anything in the selection.  This is essentially the opposite operation as above; it can be said that  addTransitiveMatches  traverses down, while  addReferencingOutsideClosure  traverses up.  After the following call returns, our selection will be augmented with this selection:  TransitiveSetTraverser.removedReferencedOutsideClosure(readEngine, selection);", 
            "title": "Transitive Set Traverser"
        }, 
        {
            "location": "/tooling/#dataset-manipulation-tools", 
            "text": "", 
            "title": "Dataset Manipulation Tools"
        }, 
        {
            "location": "/tooling/#filtering", 
            "text": "Sometimes, a dataset will be of interest to multiple different types of consumers, but not all consumers may be interested in all aspects of a dataset.  In these cases, it\u2019s possible to omit certain types and fields from a client\u2019s view of the data.  This is typically done to tailor a consumer\u2019s heap footprint and startup time costs based on their data needs.  Using our  Movie / Actor  example above, if there was a consumer which was interested in  Movie  records, but not  Actor  records, that consumer might construct a consumer-side data filter configuration in the following way:  HollowFilterConfig config = new HollowFilterConfig(true);\nconfig.addField( Movie ,  actors );\nconfig.addType( ListOfActor );\nconfig.addType( Actor );  The boolean  true  parameter in the constructor above indicates that this is an exclusion filter.  We could accomplish the same goal using an inclusion filter:  HollowFilterConfig config = new HollowFilterConfig(false);\nconfig.addField( Movie ,  id );\nconfig.addField( Movie ,  title );\nconfig.addField( Movie ,  releaseYear );\nconfig.addType( String );  The difference between these two configurations is how the filter behaves as new types and fields are added to the data model.  The exclusion filter will not exclude them by default, whereas the inclusion filter will.  A filter configuration is applied to a state engine at read time:  HollowBlobReader reader = /// a blob reader\nInputStream stream = /// a stream of the snapshot\nHollowFilterConfig config = /// the filter configuration\n\nreader.readSnapshot(inputStream, filter);", 
            "title": "Filtering"
        }, 
        {
            "location": "/tooling/#combining", 
            "text": "The  HollowCombiner  is used to copy data from one or more copies of hollow datasets in  HollowReadStateEngine s into a single  HollowWriteStateEngine .  If each of the inputs contain the same data model, the following is sufficient to combine them:  HollowReadStateEngine input1 = /// an input\nHollowReadStateEngine input2 = /// another input\n\nHollowCombiner combiner = new HollowCombiner(input1, input2);\ncombiner.combine();\n\nHollowWriteStateEngine combined = combiner.getCombinedStateEngine();  By default, the combiner will copy all records from all types from the inputs to the output.  We can direct the combiner to exclude certain records from copying using a  HollowCombinerCopyDirector .  The interface for a  HollowCombinerCopyDirector  allows for making decisions about copying individual records during a combine operation by implementing the following method:  public boolean shouldCopy(HollowTypeReadState typeState, int ordinal);  If this method returns false, then the copier will not attempt to directly copy the matching record.  However, if the matching record is referenced via another record for which this method returns true, then it will still be copied regardless of the return value of this method.  The most broadly useful provided implementation of the  HollowCombinerCopyDirector  is the  HollowCombinerExcludePrimaryKeysCopyDirector , which can be used to specify record exclusions by primary key.  For example, if we wanted to create a copy of a state engine with the  Movie  records with ids 100 and 125 excluded:  HollowReadStateEngine input = /// an input\nHollowPrimaryKeyIndex idx = new HollowPrimaryKeyIndex(input,  Movie ,  id );\n\nHollowCombinerExcludePrimaryKeysCopyDirector director = \n                          new HollowCombinerExcludePrimaryKeysCopyDirector();\n\ndirector.excludeKey(idx, 100);\ndirector.excludeKey(idx, 125);\n\nHollowCombiner combiner = new HollowCombiner(director, input);\ncombiner.combine();\n\nHollowWriteStateEngine result = combiner.getCombineStateEngine();  It\u2019s possible that while combining two inputs, both may have a record of the same type with the same primary key.  This violation of the uniqueness constraint of a primary key can be avoided by informing the combiner of the primary keys in a data model prior to the combine operation:  HollowCombiner combiner = new HollowCombiner(input1, input2);\n\ncombiner.setPrimaryKeys(\n        new PrimaryKey( Movie ,  id ),\n        new PrimaryKey( Actor ,  actorId )\n);\n\ncombiner.combine();  If multiple records exist in the inputs matching a single value for any of the supplied primary keys, then only one such record will be copied to the output.  The specific record which is copied will be the record from the input was supplied earliest in the constructor of the  HollowCombiner .  Further, if any record references another record which was omitted because it would have been duplicate based on this rule, then that reference is remapped in the output state to the matching record which was chosen to be included.", 
            "title": "Combining"
        }, 
        {
            "location": "/tooling/#splitting", 
            "text": "A single dataset can be sharded into multiple datasets using a  HollowSplitter .  The  HollowSplitter  takes a  HollowSplitterCopyDirector , which indicates:   top level  types to split,  the number of shards to create, and   which shard to send individual records.    Top Level Types  Top level types are those which are not referenced by any other types.  In our  Movie / Actor  example,  Movie  is a top-level type, but  Actor  is not.   Two default implementations of  HollowSplitterCopyDirector  are available:    HollowSplitterOrdinalCopyDirector  HollowSplitterPrimaryKeyCopyDirector .     These directors will split top-level types among a specified number of shards either by ordinals or primary keys, respectively.  When splitting by ordinal, a record with a specific primary key may jump between shards when it is modified, while with the primary key director a specific primary key will consistently hash to the same shard.  Our  Movie / Actor  example may use the splitter to split a dataset into four shards with the following invocation:  HollowReadStateEngine stateEngine = /// a state engine\n\nHollowSplitterCopyDirector director = \n                            new HollowSplitterOrdinalCopyDirector(4,  Movie );\n\nHollowSplitter splitter = new HollowSplitter(director, stateEngine);\nsplitter.split();\n\n\nfor(int i=0; i 4; i++) {\n    HollowWriteStateEngine shard = splitter.getOutputShardStateEngine(i);\n}", 
            "title": "Splitting"
        }, 
        {
            "location": "/tooling/#state-manipulation-tools", 
            "text": "", 
            "title": "State Manipulation Tools"
        }, 
        {
            "location": "/tooling/#patching", 
            "text": "Using the  HollowWriteStateEngine \u2019s restore capability, it\u2019s possible to produce deltas forever, so that consumers never have to load a snapshot after initialization.  However, if environmental hiccups cause a producer to fail to publish a delta, or if a delta is lost, or if it\u2019s desired to publish a delta between non-adjacent states, then the  HollowStateDeltaPatcher  may be used to produce deltas between two arbitrary states within the same delta chain.  The  HollowStateDeltaPatcher  must produce  two  delta transitions to create a transition between arbitrary states.  This is because non-adjacent states may have different records occupying the same ordinals.  Since no ordinal may be removed and added in adjacent states, the state patcher must create an intermediate state in which modified records do not share any ordinals.  See the  HollowStateDeltaPatcher  javadocs for usage details.", 
            "title": "Patching"
        }, 
        {
            "location": "/tooling/#compacting", 
            "text": "It is possible to produce delta chains which extend over many thousands of states.  If during this delta chain an especially large delta happens for a specific type, it\u2019s possible that many ordinal holes will be present in that type.  If over time multiple types go through especially large deltas, this can have an impact on a dataset\u2019s heap footprint.  To reclaim heap space occupied by ordinal holes, the  HollowCompactor  may be used to move records off of the high end of the ordinal space into these holes.  This is accomplished by producing deltas which only include removals and additions of identical records allocated to more optimal ordinals.  See the  HollowCompactor  javadocs for usage details.", 
            "title": "Compacting"
        }, 
        {
            "location": "/tooling/#indexing-querying", 
            "text": "", 
            "title": "Indexing / Querying"
        }, 
        {
            "location": "/tooling/#primary-keys", 
            "text": "In our  Movie / Actor  example from the Getting Started guide, we saw that we can easily create a  HollowPrimaryKeyIndex  which will allow us to query for  Movie  records by id:  HollowPrimaryKeyIndex idx = \n                      new HollowPrimaryKeyIndex(readEngine,  Movie ,  id );\nidx.listenForDeltaUpdates();  In that example, the primary key was defined for  Movie  as its  id  field.  A primary key can also be defined over multiple and/or hierarchical fields.  Imagine that  Movie  additionally had a  country  field defined in its schema, and that across countries,  Movie   id s may be duplicated, but that there will never exist two  Movie  records with the same id and country:  public class Movie {\n    long id;\n    Country country;\n    String title;\n    int releaseYear;\n}\n\n\npublic class Country {\n    String id;\n    String name;\n}  A  HollowPrimaryKeyIndex  can be defined with a primary key consisting of both fields:  HollowPrimaryKeyIndex idx = \n            new HollowPrimaryKeyIndex(readEngine,  Movie ,  id ,  country.id.value );\nidx.listenForDeltaUpdates();  And to query for a  Movie  based on its id and country:  int movieOrdinal = idx.getMatchingOrdinal(2,  US );\nif(movieOrdinal != -1) {\n    MovieHollow movie = movieApi.getMovieHollow(movieOrdinal);\n    System.out.println( Found Movie:   + movie._getTitle()._getValue());\n}  Notice that  Movie \u2019s country field in the above example is actually a  REFERENCE  field.  The defined key includes the id of the movie, and the value of the id String of the referenced country.  We denote this traversal using dot notation in the primary key definition.  The field definitions can be multiple references deep.  The requirement for a primary key definition is that no duplicates should exist for the defined combination of fields.  If this rule is violated, an arbitrary match will be returned for queries when multiple matches exist.     Primary Key Violations  Violations of the \"no duplicate\" primary key rule can be detected using the  getDuplicateKeys()  method on a  HollowPrimaryKeyIndex , which returns a  Collection Object[] .  If no duplicate keys exist, the returned Collection will be empty.  If they do, the returned values will indicate the keys for which duplicate records exist.   If a  HollowPrimaryKeyIndex  will be retained for a long duration, they should be kept updated as deltas are applied to the underlying  HollowReadStateEngine .  This is accomplished with a single call after instantiation to the  listenForDeltaUpdates()  method.    Detaching Primary Key Indexes  If  listenForDeltaUpdates()  is called on a primary key index, then it cannot be garbage collected.  If you intend to drop an index which is listening for updates, first call  detachFromDeltaUpdates()  to prevent a memory leak.   Indexes which are listening for delta updates are updated after a dataset is updated.  In the brief interim time between when a dataset is updated and the index is updated, the index will point to the  ghost records  located at tombstoned ordinals.  This helps guarantee that all in-flight operations will observe correct data.", 
            "title": "Primary Keys"
        }, 
        {
            "location": "/tooling/#hash-indexes", 
            "text": "It is sometimes desirable to index records by fields other than primary keys.  The  HollowHashIndex  allows for indexing records by fields or combinations of fields for which values may match multiple records, and records may match multiple values.  In our  Movie / Actor  example, we may want to index movies by their starring actors:  HollowHashIndex idx = \n            new HollowHashIndex(readEngine,  Movie ,  ,  actors.element.id );  The  HollowHashIndex  expects in its constructor arguments a query start type, a select field, and a set of match fields.  The constructor arguments above indicate that queries will start with the  Movie  type, select the root of the query (indicated by the empty string), and match the id of any  Actor  record in the actors list.  To query this index:  HollowHashIndexResult result = idx.findMatches(102);\n\nif(result != null) {\n    System.out.println( Found matches:   + result.numResults());\n\n    HollowOrdinalIterator iter = result.iterator();\n    int matchedOrdinal = iter.next();\n    while(matchedOrdinal != HollowOrdinalIterator.NO_MORE_ORDINALS) {\n        MovieHollow movie = api.getMovieHollow(matchedOrdinal);\n        System.out.println( Starred in:   + movie._getTitle()._getValue());\n        matchedOrdinal = iter.next();\n    }\n}  Alternatively, if the data model included the nationality of Actors, and we needed to index Actors by nationality and the titles of movies in which they starred:  HollowHashIndex idx = \n            new HollowHashIndex(readEngine,  Movie ,  actors.element ,\n                                             title.value ,\n                                             actors.element.nationality.id );  In this case, the query start type is still  Movie , but we\u2019re selecting related  Actors .  Matches are selected based on the  Movie \u2019s title, and the actor\u2019s nationality.  Using this index, one can query for Brazilian actors who starred in movies titled \u201cNarcos\u201d:  HollowHashIndexResult result = idx.findMatches( Narcos ,  BR );\n\nif(result != null) {\n    HollowOrdinalIterator iter = result.iterator();\n    int matchedOrdinal = iter.next();\n    while(matchedOrdinal != HollowOrdinalIterator.NO_MORE_ORDINALS) {\n        ActorHollow actor = api.getMovieHollow(matchedOrdinal);\n        System.out.println( Matched actor:   + \n                                      actor._getActorName()._getValue());\n        matchedOrdinal = iter.next();\n    }\n}  The  HollowHashIndex  does not yet have a facility for listening for delta updates.  If an index is necessary across multiple states, currently the index must be recreated on each update.", 
            "title": "Hash Indexes"
        }, 
        {
            "location": "/advanced-topics/", 
            "text": "Schema Parser\n\n\nHollow schemas can be serialized as Java Strings.  Calling \ntoString()\n on a \nHollowSchema\n will produce a human-readable representation of the schema.  The following shows the String representations of all of the schemas from our \nMovie\n/\nActor\n example data model:\n\n\nMovie {\n    long id;\n    int releaseYear;\n    String title;\n    ListOfActor actors;\n}\n\nListOfActor List\nActor\n;\n\nActor {\n    long id;\n    String actorName;\n}\n\nString {\n    string id;\n}\n\n\n\n\nThese representations can be parsed using the \nHollowSchemaParser\n, and in turn can be used to initialize the state of a \nHollowWriteStateEngine\n:\n\n\nString allSchemas = /// a String containing all schemas\n\nList\nHollowSchema\n schemas = \n             HollowSchemaParser.parseCollectionOfSchemas(allSchemas);\n\nHollowWriteStateEngine initializedWriteEngine = \n             HollowWriteStateCreator.createWithSchemas(schemas);\n\n\n\n\n\n\nGuiding Data Ingestion with a Data Model\n\n\nFor a generic data ingestion mechanism, loading the schemas from a text representation comes in handy.  For example, the \nJSON to Hollow adapter\n requires a \nHollowWriteStateEngine\n which is preinitialized with a data model.  The data model can be configured in a text file, and loaded with the \nHollowSchemaParser\n.\n\n\n\n\nObject schema definitions take the following form:\n\n\nObjectTypeName {\n   FieldType1 fieldName1;\n   FieldType2 fieldName2;\n   ...\n   FieldTypeN fieldNameN;\n}\n\n\n\n\nObject schemas may define any of the following field types: \nint\n, \nlong\n, \nfloat\n, \ndouble\n, \nboolean\n, \nstring\n, \nbytes\n.  If a field has a type other than these, the field will be interpreted as a \nREFERENCE\n to another type of that name.\n\n\n\n\nLowercase Field Type Declarations\n\n\nNote that the declarations for each of the inline field types are all lowercase (including \nstring\n).  An uppercase letter in any of these types will be interpreted as a \nREFERENCE\n field to a separate type.\n\n\n\n\nList\n, \nSet\n, and \nMap\n types use the following notation:\n\n\n\n\nListTypeName List\nElementTypeName\n;\n\n\nSetTypeName Set\nElementTypeName\n;\n\n\nMapTypeName Map\nKeyTypeName, ValueTypeName\n\n\n\n\nElements, keys, and values in collection record types cannot be inlined.  Whitespace is unimportant when parsing schema definitions.\n\n\nLow Level Input API\n\n\nAlthough Hollow includes a few ready-made data ingestion utilities, other data ingestion utilities can be created.  Adding data into Hollow starts with a \nHollowWriteStateEngine\n.  We need to initialize a type state for each schema in our data model:\n\n\nHollowWriteStateEngine writeEngine = new HollowWriteStateEngine();\n\nHollowObjectSchema movieSchema = new HollowObjectSchema(\nMovie\n, 3);\nmovieSchema.addField(\nid\n, FieldType.LONG);\nmovieSchema.addField(\ntitle\n, FieldType.REFERENCE, \nString\n);\nmovieSchema.addField(\nreleaseYear\n, FieldType.INT);\n\nHollowObjectTypeWriteState movieState = new HollowObjectTypeWriteState(movieSchema);\n\nwriteEngine.addTypeState(typeState);\n\n\n\n\nOnce we\u2019ve initialized our type states, we can add data into our state engine using HollowWriteRecords:\n\n\nHollowObjectSchema stringSchema = /// the String schema\nHollowObjectSchema movieSchema = /// the Movie schema\n\nHollowObjectWriteRecord titleRec = new HollowObjectWriteRecord(stringSchema);\nHollowObjectWriteRecord movieRec = new HollowObjectWriteRecord(movieSchema);\n\ntitleRec.setString(\nvalue\n, \nThe Matrix\n);\n\nint titleOrdinal = writeEngine.addObject(\nString\n, titleRec);\n\nmovieRec.setLong(\nid\n, 1);\nmovieRec.setReference(\ntitle\n, titleOrdinal);\nmovieRec.setInt(\nreleaseYear\n, 1999);\n\nwriteEngine.addObject(\nMovie\n, movieRec);\n\n\n\n\nNote that referenced records must be added prior to referencing records in order to obtain the referenced ordinals.\n\n\n\n\nReusing HollowWriteRecords\n\n\nHollowWriteRecord\ns can be reused -- just be sure to call \nreset()\n before populating data from the next record.\n\n\n\n\nEach schema type has its own \nHollowTypeWriteState\n and \nHollowWriteRecord\n implementation:\n\n\nHollowObjectSchema objectSchema = /// an Object schema\nHollowListSchema listSchema = /// a List schema\nHollowSetSchema setSchema = /// a Set schema\nHollowMapSchema mapSchema = /// a Map schema\n\nHollowObjectTypeWriteState objectTypeState = new HollowObjectTypeWriteState(objectSchema);\nHollowListTypeWriteState listTypeState = new HollowListTypeWriteState(listSchema);\nHollowSetTypeWriteState setTypeState = new HollowSetTypeWriteState(setSchema);\nHollowMapTypeWritestate mapTypeState = new HollowMapTypeWriteState(mapSchema);\n\nHollowObjectWriteRecord objectRec = new HollowObjectWriteRecord(objectSchema);\nHollowListWriteRecord listRec = new HollowListWriteRecord();\nHollowSetWriteRecord setRec = new HollowSetWriteRecord();\nHollowMapWriteRecord mapRec = new HollowMapWriteRecord();\n\n\n\n\nUsing this API, it is possible to create a generic data ingestion mechanism from \nany\n type of input source. \n\n\nDetermining Populated Ordinals\n\n\nMany lower-level operations require knowledge of the currently populated ordinals (i.e. those which are not holes), or knowledge of ordinals which were populated in state prior to the last delta application.  We can determine this from any \ntype state\n in a \nHollowReadStateEngine\n.  Both of these are useful, for example, if consumers wish to inspect exactly what has changed.\n\n\nEach \nHollowTypeReadState\n underneath a \nHollowReadStateEngine\n contains the methods \ngetPopulatedOrdinals()\n and \ngetPreviousOrdinals()\n.  Each of these methods returns a \njava.util.BitSet\n.  The contents of returned \nBitSet\ns may be inspected, but should never be modified.\n\n\nCaching\n\n\nAlthough a lot of effort has gone into minimizing the cost to read Hollow data, there is still inevitably a performance difference between accessing a field in a POJO and accessing a field in Hollow.  In general, this will not be a performance bottleneck, but in some rare cases the performance of tight inner loops may suffer on consumers.  \n\n\nWhen there is a type with a low cardinality, we can instantiate and cache a POJO implementation for each ordinal, which can be used by consumers in tight inner loops.  This is accomplished by simply passing a \nSet\nString\n as the second constructor argument when instantiating a custom-generated Hollow API.  The elements in the Set should be the types to cache.\n\n\n\n\nAvoid Premature Optimization\n\n\nCaching should be used judiciously.  In all but the tightest of loops, caching will be unnecessary, and can even be detrimental to performance for types with a large cardinality.\n\n\n\n\nHollow Heap Effects\n\n\nDouble Snapshots\n\n\nAt times, a new state may be produced to which there is no available delta from the previous state.  When this \nbroken delta chain\n scenario occurs, consumers will by default attempt to load a snapshot to the latest state.  If a consumer loads a snapshot when it currently has a state loaded for the same dataset, we call this a \ndouble snapshot\n.  Double snapshots result in a doubling of the heap usage of your dataset.  This is because Hollow assumes that consumers may be actively using the current state, and it therefore must retain the current state to provide data while the next state is loaded.  Only after the next state is fully loaded can the old state be dropped.\n\n\nIf using the \nHollow Client\n, you can configure consumers to never attempt a double snapshot.  This is accomplished with a custom \nHollowClientMemoryConfig\n.  In this case, you should check for 'stuck' clients in a \nHollowUpdateListener\n, so that you may take the appropriate operational action.\n\n\nIf a dataset is large, double snapshots should be avoided for performance reasons.  Double snapshots can be entirely avoided by \nrestoring\n a \nHollowWriteStateEngine\n at producer initialization time, and, if necessary, using the \nHollowStateDeltaPatcher\n to operationally fill lost or missing deltas in the event an unforeseen issue occurs.  \n\n\nObject Longevity\n\n\nA Hollow object returned from a generated API contains a reference to the Hollow data store, and an ordinal.  For this reason, if a reference to a Hollow object is retained by the consumer for an extended period of time, and the underlying record changes unexpected results may begin to be returned from these references.  We call Hollow objects which were obtained from a no longer current state stale references.\n\n\nIt is best practice to \nnever\n cache Hollow objects.  However, if it somehow \ncannot\n be guaranteed that Hollow Objects will never be cached at the consumer, and guaranteed protection against accidentally cached objects is necessary, then \nobject longevity\n can be enabled.\n\n\nWith object longevity, Hollow objects will, after an update, be backed by a reserved copy of the data at the time the reference was created.  This guarantees that even if a reference is held for a long time, it will continue to return the same data when interrogated.\n\n\nWhen object longevity is defined, Two durations are defined: \n\n\n\n\na grace period, and \n\n\na usage detection period.  \n\n\n\n\nThe grace period is defined by its duration, in milliseconds, after a reference becomes stale.  During the grace period, usage of stale references is acceptable.  The usage detection period is defined by its duration, in milliseconds, after the grace period has expired.  During the usage detection period, usage of stale references is unexpected, but will not result in failed interrogations of Hollow objects.  After the usage detection period expires, data will be dropped if no usage was detected in the usage detection period.  If stale references are interrogated \nafter\n their backing data is dropped, then Exceptions will be thrown.\n\n\nThe \nHollowClientMemoryConfig\n, injected into the \nHollowClient\n constructor, contains a few methods which are used to configure object longevity behavior:\n\n\n\n\nboolean enableLongLivedObjectSupport()\n: Whether or not object longevity is enabled.\n\n\nlong gracePeriodMillis()\n: If object longevity is enabled, this returns the number of milliseconds before the usage of stale objects gets flagged.\n\n\nlong usageDetectionPeriodMillis()\n: If long-lived object support is enabled, this defines the number of milliseconds, after the grace period, during which data is still available in stale references, but usage will be flagged.\n\n\nboolean dropDataAutomatically()\n: Returns whether or not to drop data behind stale references after the grace period + usage detection period has elapsed, as long as no usage was detected during the usage detection period.  This can be used to avoid memory leaks when long lived object support is enabled and stale references are cached, but unused.\n\n\nboolean forceDropData()\n: Returns whether or not to drop data behind stale references after the grace period + usage detection period has elapsed, even if usage was detected during the usage detection period.  This can be used to avoid memory leaks when long lived object support is enabled and stale references are cached and used.\n\n\n\n\nYour implementation of the \nHollowClientMemoryConfig\n may be backed by a dynamic configuration and safely change on live running consumers.  For example, \nforceDropData()\n may be operationally useful for boxes that are exhibiting memory leaks due to non-critical cached Hollow objects.\n\n\n\n\nHow It Works\n\n\nWhen enabled, object longevity is achieved using the \nHollowHistory\n data structure, which results in a minimal heap overhead. \n\n\n\n\nMemory Pooling\n\n\nHollow pools and reuses memory to minimize GC effects while updating data.  This pool of memory is kept arrays on the heap.  Each array in the pool has a fixed length.  When a long array or a byte array is required in Hollow, it will stitch together pooled array segments as a \nSegmentedByteArray\n or \nSegmentedLongArray\n.  These classes encapsulate the details of treating segmented arrays as contiguous ranges of values.\n\n\nDelta-Based Producer Input\n\n\nThe Producer Cycle\n section in this documentation describes a Hollow producer which every so often reads the entire dataset from some source of truth, adds it to a \nHollowWriteStateEngine\n, then produces a delta based on the differences in the dataset since the last cycle.  It is possible, however, that a producer may \nreceive\n an incoming stream of events which directly indicate the changes to a dataset, obviating the need to scan through the entire source of truth and re-add the entire dataset on each cycle.\n\n\nIf desired, a \nHollowWriteStateEngine\n\u2019s state can be explicitly modified, rather than recreated, each cycle.  We start such a cycle by re-adding all of the records from the previous cycle to the state engine:\n\n\nHollowWriteStateEngine writeEngine = /// the state engine\n\nwriteEngine.prepareForNextCycle();\nwriteEngine.addAllObjectsFromPreviousCycle();\n\n\n\n\nWe\u2019ll also need an indexed \nHollowReadStateEngine\n, which is updated in lock-step with the \nHollowWriteStateEngine\n.  The index can be used to retrieve the ordinals of records to be replaced.  These ordinals can be removed from the \nHollowWriteStateEngine\n:\n\n\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\nHollowPrimaryKeyIndex idx = \n                       new HollowPrimaryKeyIndex(readEngine, \nMovie\n, \nid\n);\n\nHollowTypeWriteState movieTypeState = writeEngine.getTypeState(\nMovie\n);\n\nList\nMovieUpdateEvent\n eventBatch = /// a batch of events\n\nfor(MovieUpdateEvent event : eventBatch) {\n    int oldOrdinal = idx.getMatchingOrdinal(event.getMovie().getId());\n    movieTypeState.removeOrdinalFromThisCycle(oldOrdinal);\n    mapper.addObject(event.getMovie());\n}\n\n\n\n\n\n\nWatch out for Duplicates\n\n\nBe careful, this process assumes that no two events will have the same movie ID in the same cycle.  You'll want to dedup the \neventBatch\n.\n\n\n\n\nThis process may leave orphaned records around, since the call to \nremoveOrdinalFromThisCycle()\n doesn\u2019t remove any referenced records.  To solve this, the \nTransitiveSetTraverser\n can be used:\n\n\nList\nMovieUpdateEvent\n eventBatch = /// a batch of events\n\n/// find the Movie ordinals to remove\nBitSet removedMovieOrdinals = new BitSet();\nfor(MovieUpdateEvent event : eventBatch) {\n \u00a0\u00a0\u00a0int oldOrdinal = idx.getMatchingOrdinal(event.getMovie().getId());\n \u00a0\u00a0\u00a0removedMovieOrdinals.set(oldOrdinal);\n}\n\n/// initially the removal selection includes just Movies\nMap\nString, BitSet\n removeRecords = new HashMap\n();\nremoveRecords.put(\nMovie\n, removedMovieOrdinals);\n\n/// expand the selection to include any records referenced by selected Movies\nTransitiveSetTraverser.addTransitiveMatches(readEngine, removeRecords);\n/// but don't include records which are also referenced by unselected movies\nTransitiveSetTraverser.removeReferencedOutsideClosure(readEngine, removeRecords);\n\n/// remove everything in the selection\nfor(Map.Entry\nString, BitSet\n entry : removeRecords) {\n \u00a0\u00a0\u00a0HollowTypeWriteState typeState = writeEngine.getTypeState(entry.getKey());\n \u00a0\u00a0\u00a0int removeOrdinal = entry.getValue().nextSetBit(0);\n \u00a0\u00a0\u00a0while(removeOrdinal != -1) {\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0typeState.removeOrdinalFromThisCycle(removeOrdinal);\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0removeOrdinal = entry.getValue().nextSetBit(removeOrdinal + 1);\n \u00a0\u00a0\u00a0}\n}\n\n\n\n\nIn the above code, each of the \nMovie\n ordinals to be replaced are added to a \nBitSet\n.  Then, the \nTransitiveSetTraverser\n is used to expand the collection of selected records by adding any records referenced by the selected \nMovies\n.  Then, the \nTransitiveSetTraverser\n is again used to deselect any child records which are also referenced by other Movies which were not selected for removal.  Finally, the selection is actually removed from the \nHollowWriteStateEngine\n.\n\n\nIn-Memory Data Layout\n\n\nEach record in Hollow begins with a fixed-length number of bits.  At the lowest level, these bits are held in long arrays using the class \nFixedLengthElementArray\n.  This class allows for storage and retrieval of fixed-length data in a range of bits.  For example, if a \nFixedLengthElementArray\n was queried for the 6-bit value starting at bit 7 in the following example range of bits:\n\n\n\n\nThe value \n100100\n in binary, or \n36\n in base 10, would be returned.\n\n\nObject Layout\n\n\nAn \nOBJECT\n record is a fixed set of strongly typed fields.  Each field is represented by a fixed-length number of bits.  Each record is represented by a fixed length number of bits equal to the sum of the bits required to represent each fields.  For each type, all fields of all records are packed into a single \nFixedLengthElementArray\n.  No bookkeeping data structures are required to locate a record -- the start bit for each record can is located by simply multiplying the number of bits per record times the record\u2019s ordinal.\n\n\n\n\nThe number of bits used to represent a field which is one of the types (\nINT\n, \nLONG\n, \nREFERENCE\n) is exactly equal to the number of bits required to represent the maximum value contained in the field across all records.  The values for \nINT\n and \nLONG\n fields are represented using zig-zag encoding, so that smaller absolute values require fewer bits.  The values for \nREFERENCE\n fields are encoded as the referenced record\u2019s ordinal, which along with the referenced type (from the schema) is sufficient to identify and locate the referenced record.\n\n\n32 bits are used to represent a \nFLOAT\n, and 64 bits are used to represent a \nDOUBLE\n.\n\n\nSTRING\n and \nBYTES\n fields each get a separate byte array, into which the values for all records are packed.  The fixed-length value in these fields are offsets into the field\u2019s byte array where the record\u2019s value ends.  In order to determine the begin byte for the record with ordinal n, the offset encoded into the record with ordinal (n-1) is read.  The number of fixed length bits used to represent the offsets is exactly equal to the number of number of bits required to represent the maximum offset, plus one.\n\n\nEach field type may be assigned a null value.  For \nINT\n, \nLONG\n, and \nREFERENCE\n fields, null is encoded as a value with all ones.  For \nFLOAT\n and \nDOUBLE\n fields, null is encoded as special bit sequences.  For \nSTRING\n and \nBYTES\n fields, null is encoded by setting a designated null bit at the beginning of each field, followed by the end offset of the last populated value for that field.\n\n\nList Layout\n\n\nA \nLIST\n is an ordered collection of records of a specific type.  \nLIST\n types are represented with two FixedLengthElementArrays.  We can refer to these arrays as the \noffset array\n and the \nelement array\n.\n\n\nEach \nLIST\n type contains a single \nelement array\n into which the references to elements for all records are packed.  References are encoded as the ordinals of the element records, which is sufficient to identify and locate the record.  Each reference is represented using a fixed number of bits equal to the number of bits required to represent the maximum referenced ordinal across all records.  Each record is represented with a contiguous range of elements in the \nelement array\n.  \n\n\nThe \noffset array\n contains fixed-length offsets into the element array where the record\u2019s elements end.  In order to determine the begin element for the record with ordinal n, the end value for the element (n-1) is read.  \n\n\n\n\nElements in a \nLIST\n record may not be null.\n\n\nSet Layout\n\n\nA \nSET\n is an unordered collection of records of a specific type.  The records for \nSET\n elements are hashed into an open-addressed hash table.  \nSET\n types are represented with two \nFixedLengthElementArrays\n.  We can refer to these arrays as the \noffset array\n and the \nbucket array\n.\n\n\nEach \nSET\n element contains a single \nbucket array\n into which the references to elements for all records are packed.  Each record is represented with a contiguous range of buckets in the \nbucket array\n.  The range of buckets for each record will contain an open-addressed hash table, with a linear probing hash collision resolution strategy.  The number of buckets for each record will be a power of two, and will be large enough such that all elements for the record will fit into those buckets with a load factor no greater than 70%.  Each bucket is represented using a fixed number of bits equal to the number of bits required to represent the maximum referenced ordinal across all records.  A populated bucket is encoded as the ordinal of the referenced record in that bucket.  To represent empty buckets, a sentinel value equal to all ones in binary is reserved.\n\n\nThe \noffset array\n contains two fixed-length fields per record:  the size of the set, and the offset to the bucket where the record\u2019s data ends.  Each of these fields is encoded using a fixed number of bits equal to the number of bits required to represent the field\u2019s maximum value across all records.\n\n\n\n\nElements in a \nSET\n record may not be null.\n\n\nMap Layout\n\n\nA \nMAP\n is an unordered collection of key/value pairs, where each key is a specific type, and each value type is a specific type.  The records for MAP elements are hashed into an open-addressed hash table.  \nMAP\n types are represented with two \nFixedLengthElementArrays\n.  We can refer to these arrays as the \noffset array\n and the \nbucket array\n.\n\n\nEach \nMAP\n type contains a single bucket array into which the references to keys and values for all records are packed.  Each record is represented with a contiguous range of buckets in the \nbucket array\n.  The range of buckets for each record will contain an open-addressed hash table, with a linear probing hash collision resolution strategy.  The number of buckets for each record will be a power of two, and will be large enough such that all key/value pairs for the record will fit into those buckets with a load factor no greater than 70%.  Each bucket is represented using a fixed number of bits equal to the number of bits required to represent the maximum referenced key ordinal, plus the number of bits required to represent the maximum referenced value ordinal.  A populated bucket contains two fixed length fields: the first field contains the ordinal of the referenced key, and the second field contains the ordinal of the referenced value.  Empty buckets are represented with a key field containing a reserved sentinel value equal to all ones in binary.\n\n\nThe \noffset array\n contains two fixed-length fields per record:  the size of the map, and the offset to the bucket where the record\u2019s data ends.  Each of these fields is encoded using a fixed number of bits equal to the number of bits required to represent the field\u2019s maximum value across all records.\n\n\n\n\nA \nMAP\n cannot contain null keys or values.\n\n\nPrimary Key Index Layout\n\n\nA primary key index is a single \nFixedLengthElementArray\n, which represents an open-addressed hash table of pointers to records of the given type.  The hash of each record is derived based on the fields designated in the primary key.  Each bucket in the hash table is represented using a fixed number of bits equal to the number of bits required to represent the maximum ordinal of the indexed type.  A populated bucket is encoded as the ordinal of the referenced record in that bucket.  To represent empty buckets, a sentinel value equal to all ones in binary is reserved.\n\n\nWhen queried with a key, the index will hash the key, look in the corresponding bucket to find the ordinal of a record which also hashes to this key, then compare the referenced record\u2019s key to the query.  When a matching record is found, the ordinal at that bucket is returned as the match.  Collisions are resolved with linear probing.  If after linear probing an empty bucket is encountered, no such record exists in the dataset.\n\n\nHash Index Layout\n\n\nA hash index is uses two \nFixedLengthElementArrays\n.  These arrays can be referred to as the \nmatch array\n and the \nselect array\n.  The \nmatch array\n is an open-addressed hash table and contains buckets.  Like a primary key index, the match array does not re-encode the values of keys.  Instead, it retains pointers to existing records which may be used to retrieve the hashed keys.\n\n\nUnlike a primary key index, it may not be sufficient to retain a single pointer to the indexed type and extract the hashed key from that record -- each record in a hash index may match multiple keys.  Instead, we retain 1-n pointers.  Each pointer will indicate a record through which one or more of the key fields may be unambiguously retrieved.  If multiple key fields may be unambiguously traversed to via a single type, then only a single pointer for the field group will be retained per bucket.  \n\n\nConsider a scenario in which a hash index is used to index Movies by the nationalities and birth year of actors.  For the key \n[\u201cBritish\u201d, 1972]\n, the corresponding entry in the match array may contain a pointer to the \nActor\n record for Idris Elba.  Although the record points to a specific \nActor\n, the matching records for this entry will contain movies starring any British Actor born in 1972.\n\n\nEach pointer field in the match array bucket references a specific type, and is encoded as the ordinal to which the bucket refers.  Each is represented using a fixed number of bits equal to the maximum ordinal in the referenced type.\n\n\nIn addition to pointers which allow us to look up the matching key, each bucket in the \nmatch array\n includes the number of matching records, and an offset into the \nselect array\n.  The \nselect array\n contains lists of ordinals to matching records.\n\n\nWhen queried with a key, the index will hash the key, then look to the corresponding bucket in the \nmatch array\n.  The match pointers are used to compare the queried key with the matching key.  If a match is found, then the corresponding entries in the select array are returned as a \nHollowHashIndexResult\n.  Collisions are resolved with linear probing.  If after linear probing an empty bucket is encountered in the match array, no such record exists in the dataset.", 
            "title": "Advanced Topics"
        }, 
        {
            "location": "/advanced-topics/#schema-parser", 
            "text": "Hollow schemas can be serialized as Java Strings.  Calling  toString()  on a  HollowSchema  will produce a human-readable representation of the schema.  The following shows the String representations of all of the schemas from our  Movie / Actor  example data model:  Movie {\n    long id;\n    int releaseYear;\n    String title;\n    ListOfActor actors;\n}\n\nListOfActor List Actor ;\n\nActor {\n    long id;\n    String actorName;\n}\n\nString {\n    string id;\n}  These representations can be parsed using the  HollowSchemaParser , and in turn can be used to initialize the state of a  HollowWriteStateEngine :  String allSchemas = /// a String containing all schemas\n\nList HollowSchema  schemas = \n             HollowSchemaParser.parseCollectionOfSchemas(allSchemas);\n\nHollowWriteStateEngine initializedWriteEngine = \n             HollowWriteStateCreator.createWithSchemas(schemas);   Guiding Data Ingestion with a Data Model  For a generic data ingestion mechanism, loading the schemas from a text representation comes in handy.  For example, the  JSON to Hollow adapter  requires a  HollowWriteStateEngine  which is preinitialized with a data model.  The data model can be configured in a text file, and loaded with the  HollowSchemaParser .   Object schema definitions take the following form:  ObjectTypeName {\n   FieldType1 fieldName1;\n   FieldType2 fieldName2;\n   ...\n   FieldTypeN fieldNameN;\n}  Object schemas may define any of the following field types:  int ,  long ,  float ,  double ,  boolean ,  string ,  bytes .  If a field has a type other than these, the field will be interpreted as a  REFERENCE  to another type of that name.   Lowercase Field Type Declarations  Note that the declarations for each of the inline field types are all lowercase (including  string ).  An uppercase letter in any of these types will be interpreted as a  REFERENCE  field to a separate type.   List ,  Set , and  Map  types use the following notation:   ListTypeName List ElementTypeName ;  SetTypeName Set ElementTypeName ;  MapTypeName Map KeyTypeName, ValueTypeName   Elements, keys, and values in collection record types cannot be inlined.  Whitespace is unimportant when parsing schema definitions.", 
            "title": "Schema Parser"
        }, 
        {
            "location": "/advanced-topics/#low-level-input-api", 
            "text": "Although Hollow includes a few ready-made data ingestion utilities, other data ingestion utilities can be created.  Adding data into Hollow starts with a  HollowWriteStateEngine .  We need to initialize a type state for each schema in our data model:  HollowWriteStateEngine writeEngine = new HollowWriteStateEngine();\n\nHollowObjectSchema movieSchema = new HollowObjectSchema( Movie , 3);\nmovieSchema.addField( id , FieldType.LONG);\nmovieSchema.addField( title , FieldType.REFERENCE,  String );\nmovieSchema.addField( releaseYear , FieldType.INT);\n\nHollowObjectTypeWriteState movieState = new HollowObjectTypeWriteState(movieSchema);\n\nwriteEngine.addTypeState(typeState);  Once we\u2019ve initialized our type states, we can add data into our state engine using HollowWriteRecords:  HollowObjectSchema stringSchema = /// the String schema\nHollowObjectSchema movieSchema = /// the Movie schema\n\nHollowObjectWriteRecord titleRec = new HollowObjectWriteRecord(stringSchema);\nHollowObjectWriteRecord movieRec = new HollowObjectWriteRecord(movieSchema);\n\ntitleRec.setString( value ,  The Matrix );\n\nint titleOrdinal = writeEngine.addObject( String , titleRec);\n\nmovieRec.setLong( id , 1);\nmovieRec.setReference( title , titleOrdinal);\nmovieRec.setInt( releaseYear , 1999);\n\nwriteEngine.addObject( Movie , movieRec);  Note that referenced records must be added prior to referencing records in order to obtain the referenced ordinals.   Reusing HollowWriteRecords  HollowWriteRecord s can be reused -- just be sure to call  reset()  before populating data from the next record.   Each schema type has its own  HollowTypeWriteState  and  HollowWriteRecord  implementation:  HollowObjectSchema objectSchema = /// an Object schema\nHollowListSchema listSchema = /// a List schema\nHollowSetSchema setSchema = /// a Set schema\nHollowMapSchema mapSchema = /// a Map schema\n\nHollowObjectTypeWriteState objectTypeState = new HollowObjectTypeWriteState(objectSchema);\nHollowListTypeWriteState listTypeState = new HollowListTypeWriteState(listSchema);\nHollowSetTypeWriteState setTypeState = new HollowSetTypeWriteState(setSchema);\nHollowMapTypeWritestate mapTypeState = new HollowMapTypeWriteState(mapSchema);\n\nHollowObjectWriteRecord objectRec = new HollowObjectWriteRecord(objectSchema);\nHollowListWriteRecord listRec = new HollowListWriteRecord();\nHollowSetWriteRecord setRec = new HollowSetWriteRecord();\nHollowMapWriteRecord mapRec = new HollowMapWriteRecord();  Using this API, it is possible to create a generic data ingestion mechanism from  any  type of input source.", 
            "title": "Low Level Input API"
        }, 
        {
            "location": "/advanced-topics/#determining-populated-ordinals", 
            "text": "Many lower-level operations require knowledge of the currently populated ordinals (i.e. those which are not holes), or knowledge of ordinals which were populated in state prior to the last delta application.  We can determine this from any  type state  in a  HollowReadStateEngine .  Both of these are useful, for example, if consumers wish to inspect exactly what has changed.  Each  HollowTypeReadState  underneath a  HollowReadStateEngine  contains the methods  getPopulatedOrdinals()  and  getPreviousOrdinals() .  Each of these methods returns a  java.util.BitSet .  The contents of returned  BitSet s may be inspected, but should never be modified.", 
            "title": "Determining Populated Ordinals"
        }, 
        {
            "location": "/advanced-topics/#caching", 
            "text": "Although a lot of effort has gone into minimizing the cost to read Hollow data, there is still inevitably a performance difference between accessing a field in a POJO and accessing a field in Hollow.  In general, this will not be a performance bottleneck, but in some rare cases the performance of tight inner loops may suffer on consumers.    When there is a type with a low cardinality, we can instantiate and cache a POJO implementation for each ordinal, which can be used by consumers in tight inner loops.  This is accomplished by simply passing a  Set String  as the second constructor argument when instantiating a custom-generated Hollow API.  The elements in the Set should be the types to cache.   Avoid Premature Optimization  Caching should be used judiciously.  In all but the tightest of loops, caching will be unnecessary, and can even be detrimental to performance for types with a large cardinality.", 
            "title": "Caching"
        }, 
        {
            "location": "/advanced-topics/#hollow-heap-effects", 
            "text": "", 
            "title": "Hollow Heap Effects"
        }, 
        {
            "location": "/advanced-topics/#double-snapshots", 
            "text": "At times, a new state may be produced to which there is no available delta from the previous state.  When this  broken delta chain  scenario occurs, consumers will by default attempt to load a snapshot to the latest state.  If a consumer loads a snapshot when it currently has a state loaded for the same dataset, we call this a  double snapshot .  Double snapshots result in a doubling of the heap usage of your dataset.  This is because Hollow assumes that consumers may be actively using the current state, and it therefore must retain the current state to provide data while the next state is loaded.  Only after the next state is fully loaded can the old state be dropped.  If using the  Hollow Client , you can configure consumers to never attempt a double snapshot.  This is accomplished with a custom  HollowClientMemoryConfig .  In this case, you should check for 'stuck' clients in a  HollowUpdateListener , so that you may take the appropriate operational action.  If a dataset is large, double snapshots should be avoided for performance reasons.  Double snapshots can be entirely avoided by  restoring  a  HollowWriteStateEngine  at producer initialization time, and, if necessary, using the  HollowStateDeltaPatcher  to operationally fill lost or missing deltas in the event an unforeseen issue occurs.", 
            "title": "Double Snapshots"
        }, 
        {
            "location": "/advanced-topics/#object-longevity", 
            "text": "A Hollow object returned from a generated API contains a reference to the Hollow data store, and an ordinal.  For this reason, if a reference to a Hollow object is retained by the consumer for an extended period of time, and the underlying record changes unexpected results may begin to be returned from these references.  We call Hollow objects which were obtained from a no longer current state stale references.  It is best practice to  never  cache Hollow objects.  However, if it somehow  cannot  be guaranteed that Hollow Objects will never be cached at the consumer, and guaranteed protection against accidentally cached objects is necessary, then  object longevity  can be enabled.  With object longevity, Hollow objects will, after an update, be backed by a reserved copy of the data at the time the reference was created.  This guarantees that even if a reference is held for a long time, it will continue to return the same data when interrogated.  When object longevity is defined, Two durations are defined:    a grace period, and   a usage detection period.     The grace period is defined by its duration, in milliseconds, after a reference becomes stale.  During the grace period, usage of stale references is acceptable.  The usage detection period is defined by its duration, in milliseconds, after the grace period has expired.  During the usage detection period, usage of stale references is unexpected, but will not result in failed interrogations of Hollow objects.  After the usage detection period expires, data will be dropped if no usage was detected in the usage detection period.  If stale references are interrogated  after  their backing data is dropped, then Exceptions will be thrown.  The  HollowClientMemoryConfig , injected into the  HollowClient  constructor, contains a few methods which are used to configure object longevity behavior:   boolean enableLongLivedObjectSupport() : Whether or not object longevity is enabled.  long gracePeriodMillis() : If object longevity is enabled, this returns the number of milliseconds before the usage of stale objects gets flagged.  long usageDetectionPeriodMillis() : If long-lived object support is enabled, this defines the number of milliseconds, after the grace period, during which data is still available in stale references, but usage will be flagged.  boolean dropDataAutomatically() : Returns whether or not to drop data behind stale references after the grace period + usage detection period has elapsed, as long as no usage was detected during the usage detection period.  This can be used to avoid memory leaks when long lived object support is enabled and stale references are cached, but unused.  boolean forceDropData() : Returns whether or not to drop data behind stale references after the grace period + usage detection period has elapsed, even if usage was detected during the usage detection period.  This can be used to avoid memory leaks when long lived object support is enabled and stale references are cached and used.   Your implementation of the  HollowClientMemoryConfig  may be backed by a dynamic configuration and safely change on live running consumers.  For example,  forceDropData()  may be operationally useful for boxes that are exhibiting memory leaks due to non-critical cached Hollow objects.   How It Works  When enabled, object longevity is achieved using the  HollowHistory  data structure, which results in a minimal heap overhead.", 
            "title": "Object Longevity"
        }, 
        {
            "location": "/advanced-topics/#memory-pooling", 
            "text": "Hollow pools and reuses memory to minimize GC effects while updating data.  This pool of memory is kept arrays on the heap.  Each array in the pool has a fixed length.  When a long array or a byte array is required in Hollow, it will stitch together pooled array segments as a  SegmentedByteArray  or  SegmentedLongArray .  These classes encapsulate the details of treating segmented arrays as contiguous ranges of values.", 
            "title": "Memory Pooling"
        }, 
        {
            "location": "/advanced-topics/#delta-based-producer-input", 
            "text": "The Producer Cycle  section in this documentation describes a Hollow producer which every so often reads the entire dataset from some source of truth, adds it to a  HollowWriteStateEngine , then produces a delta based on the differences in the dataset since the last cycle.  It is possible, however, that a producer may  receive  an incoming stream of events which directly indicate the changes to a dataset, obviating the need to scan through the entire source of truth and re-add the entire dataset on each cycle.  If desired, a  HollowWriteStateEngine \u2019s state can be explicitly modified, rather than recreated, each cycle.  We start such a cycle by re-adding all of the records from the previous cycle to the state engine:  HollowWriteStateEngine writeEngine = /// the state engine\n\nwriteEngine.prepareForNextCycle();\nwriteEngine.addAllObjectsFromPreviousCycle();  We\u2019ll also need an indexed  HollowReadStateEngine , which is updated in lock-step with the  HollowWriteStateEngine .  The index can be used to retrieve the ordinals of records to be replaced.  These ordinals can be removed from the  HollowWriteStateEngine :  HollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\nHollowPrimaryKeyIndex idx = \n                       new HollowPrimaryKeyIndex(readEngine,  Movie ,  id );\n\nHollowTypeWriteState movieTypeState = writeEngine.getTypeState( Movie );\n\nList MovieUpdateEvent  eventBatch = /// a batch of events\n\nfor(MovieUpdateEvent event : eventBatch) {\n    int oldOrdinal = idx.getMatchingOrdinal(event.getMovie().getId());\n    movieTypeState.removeOrdinalFromThisCycle(oldOrdinal);\n    mapper.addObject(event.getMovie());\n}   Watch out for Duplicates  Be careful, this process assumes that no two events will have the same movie ID in the same cycle.  You'll want to dedup the  eventBatch .   This process may leave orphaned records around, since the call to  removeOrdinalFromThisCycle()  doesn\u2019t remove any referenced records.  To solve this, the  TransitiveSetTraverser  can be used:  List MovieUpdateEvent  eventBatch = /// a batch of events\n\n/// find the Movie ordinals to remove\nBitSet removedMovieOrdinals = new BitSet();\nfor(MovieUpdateEvent event : eventBatch) {\n \u00a0\u00a0\u00a0int oldOrdinal = idx.getMatchingOrdinal(event.getMovie().getId());\n \u00a0\u00a0\u00a0removedMovieOrdinals.set(oldOrdinal);\n}\n\n/// initially the removal selection includes just Movies\nMap String, BitSet  removeRecords = new HashMap ();\nremoveRecords.put( Movie , removedMovieOrdinals);\n\n/// expand the selection to include any records referenced by selected Movies\nTransitiveSetTraverser.addTransitiveMatches(readEngine, removeRecords);\n/// but don't include records which are also referenced by unselected movies\nTransitiveSetTraverser.removeReferencedOutsideClosure(readEngine, removeRecords);\n\n/// remove everything in the selection\nfor(Map.Entry String, BitSet  entry : removeRecords) {\n \u00a0\u00a0\u00a0HollowTypeWriteState typeState = writeEngine.getTypeState(entry.getKey());\n \u00a0\u00a0\u00a0int removeOrdinal = entry.getValue().nextSetBit(0);\n \u00a0\u00a0\u00a0while(removeOrdinal != -1) {\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0typeState.removeOrdinalFromThisCycle(removeOrdinal);\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0removeOrdinal = entry.getValue().nextSetBit(removeOrdinal + 1);\n \u00a0\u00a0\u00a0}\n}  In the above code, each of the  Movie  ordinals to be replaced are added to a  BitSet .  Then, the  TransitiveSetTraverser  is used to expand the collection of selected records by adding any records referenced by the selected  Movies .  Then, the  TransitiveSetTraverser  is again used to deselect any child records which are also referenced by other Movies which were not selected for removal.  Finally, the selection is actually removed from the  HollowWriteStateEngine .", 
            "title": "Delta-Based Producer Input"
        }, 
        {
            "location": "/advanced-topics/#in-memory-data-layout", 
            "text": "Each record in Hollow begins with a fixed-length number of bits.  At the lowest level, these bits are held in long arrays using the class  FixedLengthElementArray .  This class allows for storage and retrieval of fixed-length data in a range of bits.  For example, if a  FixedLengthElementArray  was queried for the 6-bit value starting at bit 7 in the following example range of bits:   The value  100100  in binary, or  36  in base 10, would be returned.", 
            "title": "In-Memory Data Layout"
        }, 
        {
            "location": "/advanced-topics/#object-layout", 
            "text": "An  OBJECT  record is a fixed set of strongly typed fields.  Each field is represented by a fixed-length number of bits.  Each record is represented by a fixed length number of bits equal to the sum of the bits required to represent each fields.  For each type, all fields of all records are packed into a single  FixedLengthElementArray .  No bookkeeping data structures are required to locate a record -- the start bit for each record can is located by simply multiplying the number of bits per record times the record\u2019s ordinal.   The number of bits used to represent a field which is one of the types ( INT ,  LONG ,  REFERENCE ) is exactly equal to the number of bits required to represent the maximum value contained in the field across all records.  The values for  INT  and  LONG  fields are represented using zig-zag encoding, so that smaller absolute values require fewer bits.  The values for  REFERENCE  fields are encoded as the referenced record\u2019s ordinal, which along with the referenced type (from the schema) is sufficient to identify and locate the referenced record.  32 bits are used to represent a  FLOAT , and 64 bits are used to represent a  DOUBLE .  STRING  and  BYTES  fields each get a separate byte array, into which the values for all records are packed.  The fixed-length value in these fields are offsets into the field\u2019s byte array where the record\u2019s value ends.  In order to determine the begin byte for the record with ordinal n, the offset encoded into the record with ordinal (n-1) is read.  The number of fixed length bits used to represent the offsets is exactly equal to the number of number of bits required to represent the maximum offset, plus one.  Each field type may be assigned a null value.  For  INT ,  LONG , and  REFERENCE  fields, null is encoded as a value with all ones.  For  FLOAT  and  DOUBLE  fields, null is encoded as special bit sequences.  For  STRING  and  BYTES  fields, null is encoded by setting a designated null bit at the beginning of each field, followed by the end offset of the last populated value for that field.", 
            "title": "Object Layout"
        }, 
        {
            "location": "/advanced-topics/#list-layout", 
            "text": "A  LIST  is an ordered collection of records of a specific type.   LIST  types are represented with two FixedLengthElementArrays.  We can refer to these arrays as the  offset array  and the  element array .  Each  LIST  type contains a single  element array  into which the references to elements for all records are packed.  References are encoded as the ordinals of the element records, which is sufficient to identify and locate the record.  Each reference is represented using a fixed number of bits equal to the number of bits required to represent the maximum referenced ordinal across all records.  Each record is represented with a contiguous range of elements in the  element array .    The  offset array  contains fixed-length offsets into the element array where the record\u2019s elements end.  In order to determine the begin element for the record with ordinal n, the end value for the element (n-1) is read.     Elements in a  LIST  record may not be null.", 
            "title": "List Layout"
        }, 
        {
            "location": "/advanced-topics/#set-layout", 
            "text": "A  SET  is an unordered collection of records of a specific type.  The records for  SET  elements are hashed into an open-addressed hash table.   SET  types are represented with two  FixedLengthElementArrays .  We can refer to these arrays as the  offset array  and the  bucket array .  Each  SET  element contains a single  bucket array  into which the references to elements for all records are packed.  Each record is represented with a contiguous range of buckets in the  bucket array .  The range of buckets for each record will contain an open-addressed hash table, with a linear probing hash collision resolution strategy.  The number of buckets for each record will be a power of two, and will be large enough such that all elements for the record will fit into those buckets with a load factor no greater than 70%.  Each bucket is represented using a fixed number of bits equal to the number of bits required to represent the maximum referenced ordinal across all records.  A populated bucket is encoded as the ordinal of the referenced record in that bucket.  To represent empty buckets, a sentinel value equal to all ones in binary is reserved.  The  offset array  contains two fixed-length fields per record:  the size of the set, and the offset to the bucket where the record\u2019s data ends.  Each of these fields is encoded using a fixed number of bits equal to the number of bits required to represent the field\u2019s maximum value across all records.   Elements in a  SET  record may not be null.", 
            "title": "Set Layout"
        }, 
        {
            "location": "/advanced-topics/#map-layout", 
            "text": "A  MAP  is an unordered collection of key/value pairs, where each key is a specific type, and each value type is a specific type.  The records for MAP elements are hashed into an open-addressed hash table.   MAP  types are represented with two  FixedLengthElementArrays .  We can refer to these arrays as the  offset array  and the  bucket array .  Each  MAP  type contains a single bucket array into which the references to keys and values for all records are packed.  Each record is represented with a contiguous range of buckets in the  bucket array .  The range of buckets for each record will contain an open-addressed hash table, with a linear probing hash collision resolution strategy.  The number of buckets for each record will be a power of two, and will be large enough such that all key/value pairs for the record will fit into those buckets with a load factor no greater than 70%.  Each bucket is represented using a fixed number of bits equal to the number of bits required to represent the maximum referenced key ordinal, plus the number of bits required to represent the maximum referenced value ordinal.  A populated bucket contains two fixed length fields: the first field contains the ordinal of the referenced key, and the second field contains the ordinal of the referenced value.  Empty buckets are represented with a key field containing a reserved sentinel value equal to all ones in binary.  The  offset array  contains two fixed-length fields per record:  the size of the map, and the offset to the bucket where the record\u2019s data ends.  Each of these fields is encoded using a fixed number of bits equal to the number of bits required to represent the field\u2019s maximum value across all records.   A  MAP  cannot contain null keys or values.", 
            "title": "Map Layout"
        }, 
        {
            "location": "/advanced-topics/#primary-key-index-layout", 
            "text": "A primary key index is a single  FixedLengthElementArray , which represents an open-addressed hash table of pointers to records of the given type.  The hash of each record is derived based on the fields designated in the primary key.  Each bucket in the hash table is represented using a fixed number of bits equal to the number of bits required to represent the maximum ordinal of the indexed type.  A populated bucket is encoded as the ordinal of the referenced record in that bucket.  To represent empty buckets, a sentinel value equal to all ones in binary is reserved.  When queried with a key, the index will hash the key, look in the corresponding bucket to find the ordinal of a record which also hashes to this key, then compare the referenced record\u2019s key to the query.  When a matching record is found, the ordinal at that bucket is returned as the match.  Collisions are resolved with linear probing.  If after linear probing an empty bucket is encountered, no such record exists in the dataset.", 
            "title": "Primary Key Index Layout"
        }, 
        {
            "location": "/advanced-topics/#hash-index-layout", 
            "text": "A hash index is uses two  FixedLengthElementArrays .  These arrays can be referred to as the  match array  and the  select array .  The  match array  is an open-addressed hash table and contains buckets.  Like a primary key index, the match array does not re-encode the values of keys.  Instead, it retains pointers to existing records which may be used to retrieve the hashed keys.  Unlike a primary key index, it may not be sufficient to retain a single pointer to the indexed type and extract the hashed key from that record -- each record in a hash index may match multiple keys.  Instead, we retain 1-n pointers.  Each pointer will indicate a record through which one or more of the key fields may be unambiguously retrieved.  If multiple key fields may be unambiguously traversed to via a single type, then only a single pointer for the field group will be retained per bucket.    Consider a scenario in which a hash index is used to index Movies by the nationalities and birth year of actors.  For the key  [\u201cBritish\u201d, 1972] , the corresponding entry in the match array may contain a pointer to the  Actor  record for Idris Elba.  Although the record points to a specific  Actor , the matching records for this entry will contain movies starring any British Actor born in 1972.  Each pointer field in the match array bucket references a specific type, and is encoded as the ordinal to which the bucket refers.  Each is represented using a fixed number of bits equal to the maximum ordinal in the referenced type.  In addition to pointers which allow us to look up the matching key, each bucket in the  match array  includes the number of matching records, and an offset into the  select array .  The  select array  contains lists of ordinals to matching records.  When queried with a key, the index will hash the key, then look to the corresponding bucket in the  match array .  The match pointers are used to compare the queried key with the matching key.  If a match is found, then the corresponding entries in the select array are returned as a  HollowHashIndexResult .  Collisions are resolved with linear probing.  If after linear probing an empty bucket is encountered in the match array, no such record exists in the dataset.", 
            "title": "Hash Index Layout"
        }, 
        {
            "location": "/glossary/", 
            "text": "adjacent state\n\n\n\n\nIf state \nA\n is connected via a single delta to state \nB\n, then \nA\n and \nB\n are adjacent to each other.\n\n\n\n\nannounce\n\n\n\n\nAfter the blobs for a state have been published to a blob store by a producer, the state must be \nannounced\n to consumers.  The announcement signals to consumers that they should transition to the announced state.\n\n\n\n\nblob\n\n\n\n\nA blob is a file used by consumers to update their dataset.  A blob will be either a snapshot, delta, or reverse delta\n\n\n\n\nblob store\n\n\n\n\nA blob store is a file store to which blobs can be published by a producer and retrieved by a consumer. \n\n\n\n\nbroken delta chain\n\n\n\n\nWhen a blob namespace contains a state which is not adjacent to any prior states, the delta chain is said to be broken.  In this scenario, consumers may need to load a double snapshot.\n\n\n\n\nconsumer\n\n\n\n\nOne of many machines on which a dataset is made accessible.  Consumers are updated in lock-step based on the actions of the producer.\n\n\n\n\ncycle\n\n\n\n\nA producer runs in an infinite loop.  Each exection of the loop is called a cycle.  Each cycle produces a single data state.\n\n\n\n\ndata model\n\n\n\n\nA data model defines the structure of a dataset.  It is specified with a set of schemas.\n\n\n\n\ndata state\n\n\n\n\nA dataset changes over time.  The timeline for a changing dataset can be broken down into discrete data states, each of which is a complete snapshot of the data at a particular point in time.\n\n\n\n\ndeduplication\n\n\n\n\nTwo records which have identical data in Hollow will be consolidated into a single record.  Any references to duplicate records will be mapped to the canonical one when a dataset is represented with Hollow.\n\n\n\n\ndelta\n\n\n\n\nA set of encoded instructions to transition from one data state to an adjacent state.  Deltas are encoded as a set of ordinals to remove and a set of ordinals to add, along with the accompanying data to add.  'Delta' may refer specifically to a transition between an earlier state and a later state, contrasted with 'reverse delta', which specifically refers to a transition between a later state and an earlier state.\n\n\n\n\ndelta chain\n\n\n\n\nA series of states which are all connected via contiguous deltas.\n\n\n\n\ndiff\n\n\n\n\nA comprehensive accounting for the differences between two data states.\n\n\n\n\ndouble snapshot\n\n\n\n\nWhen a consumer already has an initialized state and an announcement signals to move to a new state for which a path of deltas is not available, the consumer may transition to that state via a snapshot.  In this scenario two full copies of the dataset must be loaded in memory.\n\n\n\n\nfield\n\n\n\n\nA single value encoded inside of a Hollow record.\n\n\n\n\nhash key\n\n\n\n\nA user-defined specification of one or more fields used to hash elements into a set or entries into a map.\n\n\n\n\ningestion\n\n\n\n\nGathering data from a source of truth and importing it into Hollow.\n\n\n\n\ninline\n\n\n\n\nA field for which the value is encoded directly into a record, as opposed to referenced via another record.\n\n\n\n\nnamespace (blobs)\n\n\n\n\nAn addressable, logical separation of both published artifacts in a blob store and announcement location.  Used to allow multiple publishers to communicate on separate channels to specific groups of consumers.\n\n\n\n\nnamespace (references)\n\n\n\n\nThe deliberate creation of a type to hold a specific referenced field's data in order to reduce the cardinality of the referenced records.\n\n\n\n\nobject longevity\n\n\n\n\nA technique used to ensure that stale references to Hollow Objects always return the same data they did initially upon creation.  Configured via the \nHollowObjectMemoryConfig\n.\n\n\n\n\nordinal\n\n\n\n\nAn integer value uniquely identifying a record within a type.  Because records are represented with a fixed-length number of bits, the only necessary information to locate a record in memory is the record's type and ordinal.  Ordinals are automatically assigned by Hollow, and are recycled as records are removed and added.  Consequently, they lie in the range of 0-n, where n is generally not much larger than the total number of records for the type.\n\n\n\n\npatch (states)\n\n\n\n\nCreating a series of two deltas between states in a delta chain.\n\n\n\n\npinning\n\n\n\n\nOverriding the state version announcement from the producer, to force clients to go back to or stay at an older state.\n\n\n\n\nprimary key\n\n\n\n\nA user-defined specification of one or more fields used to uniquely identify a record within a type.\n\n\n\n\nproducer\n\n\n\n\nA single machine that retrieves all data from a source of truth and produces a delta chain.\n\n\n\n\npublish\n\n\n\n\nWriting blobs to a blob store.\n\n\n\n\nread state engine\n\n\n\n\nA \nHollowReadStateEngine\n, the root handle to a Hollow dataset as a consumer.\n\n\n\n\nrecord\n\n\n\n\nA strongly-typed collection of fields or references, the structure of which is specified by a schema.\n\n\n\n\nreference\n\n\n\n\nA field type which indicates a pointer to another field.  Can also refer to the technique of pulling out a specific field into a record type of its own to deliberately allow Hollow to deduplicate the values.\n\n\n\n\nrestore\n\n\n\n\nInitializing a \nHollowWriteStateEngine\n with data from a previously produced state so that a delta may be created during a producer's first cycle.\n\n\n\n\nreverse delta\n\n\n\n\nA delta from a later state to an earlier state.  Generally used during pinning scenarios.\n\n\n\n\nschema\n\n\n\n\nMetadata about a Hollow type which defines the structure of the records.\n\n\n\n\nsnapshot\n\n\n\n\nA blob type which contains a serialization of all of the records in a type.  Consumed during initialization, and possibly in a broken delta chain scenario.\n\n\n\n\nstate\n\n\n\n\nSee \ndata state\n.\n\n\n\n\nstate version\n\n\n\n\nA unique identifier for a state.  Should by monotonically increasing as time passes.\n\n\n\n\nstate engine\n\n\n\n\nBoth the producer and consumers handle datasets with a state engine.  A state engine can be transitioned between data states.  A producer uses a \nwrite state engine\n and a consumer uses a \nread state engine\n\n\n\n\ntype\n\n\n\n\nA collection of records all conforming to a specific schema.\n\n\n\n\nwrite state engine\n\n\n\n\nA \nHollowWriteStateEngine\n, the root handle to a Hollow dataset as a consumer.", 
            "title": "Glossary"
        }, 
        {
            "location": "/community/", 
            "text": "Getting Support\n\n\nFor bug reports and feature requests, please file a \nGitHub issue\n.  \n\n\nIf you have a question that isn't covered in this documentation, please reach out for help either on Stack Overflow or \nGitter\n\n\nStack Overflow\n\n\nThe Platform Data Technologies team at Netflix will monitor posts tagged with \nhollow\n.\n\n\nGitter\n\n\nThe Platform Data Technologies team at Netflix is often available for chat via \nGitter\n.  We hope that you'll stick around and pay it forward by answering other users' questions when they arise.\n\n\nContributing to Hollow\n\n\nWe'll gladly review and accept pull requests for Hollow.  If you want to have a design discussion for your changes, please reach out to us on Gitter.\n\n\nBackwards Compatibility\n\n\nNew features in Hollow should always be added in a way that is backwards compatible, except in \nextremely\n rare cases when a major version is released.\n\n\nIf you would like to make a contribution which breaks backwards compatibility, please contact us so we can evaluate alternate ways to achieve the desired result, and/or whether to schedule the change for an upcoming major version release.\n\n\nDependencies\n\n\nThe core project \nhollow\n should have zero \ncompile\n dependencies, and should only depend on one library (jUnit) as a \ntest\n dependency.  We believe this provides long-term stability for users, reduces licensing concerns, and eliminates the possibility that \nother\n project dependencies will be compiled against incompatible versions of dependent libraries.\n\n\nIf you would like to make a contribution which requires a third-party dependency, please contact us before proceeding so we can discuss the appropriate location for the addition.", 
            "title": "Community"
        }, 
        {
            "location": "/community/#getting-support", 
            "text": "For bug reports and feature requests, please file a  GitHub issue .    If you have a question that isn't covered in this documentation, please reach out for help either on Stack Overflow or  Gitter", 
            "title": "Getting Support"
        }, 
        {
            "location": "/community/#stack-overflow", 
            "text": "The Platform Data Technologies team at Netflix will monitor posts tagged with  hollow .", 
            "title": "Stack Overflow"
        }, 
        {
            "location": "/community/#gitter", 
            "text": "The Platform Data Technologies team at Netflix is often available for chat via  Gitter .  We hope that you'll stick around and pay it forward by answering other users' questions when they arise.", 
            "title": "Gitter"
        }, 
        {
            "location": "/community/#contributing-to-hollow", 
            "text": "We'll gladly review and accept pull requests for Hollow.  If you want to have a design discussion for your changes, please reach out to us on Gitter.", 
            "title": "Contributing to Hollow"
        }, 
        {
            "location": "/community/#backwards-compatibility", 
            "text": "New features in Hollow should always be added in a way that is backwards compatible, except in  extremely  rare cases when a major version is released.  If you would like to make a contribution which breaks backwards compatibility, please contact us so we can evaluate alternate ways to achieve the desired result, and/or whether to schedule the change for an upcoming major version release.", 
            "title": "Backwards Compatibility"
        }, 
        {
            "location": "/community/#dependencies", 
            "text": "The core project  hollow  should have zero  compile  dependencies, and should only depend on one library (jUnit) as a  test  dependency.  We believe this provides long-term stability for users, reduces licensing concerns, and eliminates the possibility that  other  project dependencies will be compiled against incompatible versions of dependent libraries.  If you would like to make a contribution which requires a third-party dependency, please contact us before proceeding so we can discuss the appropriate location for the addition.", 
            "title": "Dependencies"
        }, 
        {
            "location": "/acknowledgements/", 
            "text": "Hollow is originally created by Drew Koszewnik with the advice and support of fellow members of the Data Platform Technologies team at Netflix:\n\n\n\n\nDavid Su\n\n\nDeva Jayaraman\n\n\nJatin Shah\n\n\nKinesh Satiya\n\n\nLavanya Kanchanapalli\n\n\nRamin Forood\n\n\nRohit Kaul\n\n\nTim Taylor\n\n\n\n\nHollow is maintained by the Data Platform Technologies team at Netflix.\n\n\nHollow makes use of an \nimplementation\n of the MurmurHash3 algorithm authored by Yonik Seeley.\n\n\nHollow makes use of Thomas Wang's well known 32-bit mix function, which is based on an original suggestion by Robert Jenkins.\n\n\nThe \nhollow-diff-ui\n object diff view was inspired in part by Chas Emerick's \njsdifflib\n, and borrows some of the .css from that project.\n\n\nThis documentation was created with \nMkDocs\n, using a modified theme from \nreadthedocs\n.", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/license/", 
            "text": "Copyright 2016 Netflix, Inc.\n\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.", 
            "title": "License"
        }
    ]
}