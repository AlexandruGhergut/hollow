{
    "docs": [
        {
            "location": "/", 
            "text": "The Problem\n\n\nSoftware engineers often encounter problems which require the dissemination of small or moderately sized data sets which don\u2019t fit the label \u201cbig data\u201d.  To solve these problems, we often send the data to an RDBMS or nosql data store and query it at runtime, or serialize the data as json or xml, distribute it, and keep a local copy on each consumer.\n\n\nScaling each of these solutions presents different challenges.  Sending the data to an RDBMS, nosql data store, or even a memcached cluster may allow your dataset to grow indefinitely large, but there are limitations on the latency and frequency with which you can interact with that dataset.  Serializing and keeping a local copy (if in RAM) can allow many orders of magnitude lower latency and higher frequency access, but this approach has many scaling challenges:\n\n\n\n\nThe dataset size is limited by available RAM.\n\n\nThe full dataset may need to be re-downloaded each time it is updated.\n\n\nUpdating the dataset may require significant CPU resources or impact GC behavior.\n\n\n\n\nNetflix, serving many billions of personalized requests each day, has a few use cases for which the latency of a remote datastore would be highly undesirable given the frequency with which those datasets are accessed.\n\n\nThe Solution\n\n\nNetflix Hollow is a java library and toolset for disseminating in-memory datasets from a single producer to many consumers for high performance read-only access. Hollow aggressively addresses the scaling challenges of in-memory datasets, and is built with servers busily serving requests at or near maximum capacity in mind.\n\n\nDue to its performance characteristics, Hollow shifts the scale in terms of appropriate dataset sizes for an in-memory solution.  Datasets for which such liberation may never previously have been considered can be candidates for Hollow.  \n\n\n\n\nSmall to Medium Datasets\n\n\nHollow may be entirely appropriate for datasets which, if represented with json or XML, might require in excess of 100GB.  A good rule of thumb in 2017: KB, MB, and often GB, but not TB or PB.\n\n\n\n\nHollow simultaneously targets three goals:\n\n\n\n\nMaximum development agility\n\n\nHighly optimized performance and resource management\n\n\nExtreme stability and reliability\n\n\n\n\nMaximum Agility\n\n\nHollow provides the capability to automatically generate a custom API based on a specific data model, so that consumers can intuitively interact with the data, with the benefit of IDE code completion.\n\n\nHollow provides insight tools, which will help users understand their dataset, and how it changes over time, more deeply than ever before:\n\n\n\n\nComprehensive change history\n\n\nDiffing entire datasets between arbitrary states\n\n\nHeap usage analysis\n\n\nUsage tracking\n\n\n\n\nThe toolset available for working with Hollow datasets allows for a surprising variety of operations to be performed with ease, including:\n\n\n\n\nIndexing / Querying for individual records in a dataset\n\n\nSplitting / Combining entire datasets in many different ways\n\n\nFiltering individual record types at the consumer to reduce heap footprint\n\n\n\n\nOptimized Performance\n\n\nHollow is hyper-optimized with a few performance metrics at top-of-mind:\n\n\n\n\nHeap footprint\n\n\nComputational cost of access\n\n\nGC impact of updates\n\n\nComputational cost of updates\n\n\nNetwork cost of updates\n\n\n\n\nOver time, Hollow automatically calculates the changes in a dataset on the producer.  Instead of retransmitting the entire snapshot of the data for each update, only the changes are disseminated to consumers to keep them up to date.\n\n\nOn consumers, Hollow keeps a compact encoding of the dataset in RAM.  This representation is optimized for both minimizing heap footprint and minimizing access CPU cost.  To retain and keep the dataset updated, Hollow pools and reuses heap memory to avoid GC tenuring.\n\n\nExtreme Stability\n\n\nHollow has been battle-hardened over more than two years of continuous use at Netflix.  Hollow is used to represent crucial datasets, essential to the fulfillment of the Netflix experience, on servers answering live customer requests.  So although Hollow goes to extraordinary lengths to squeeze every last bit of performance out of servers\u2019 hardware, enormous attention to detail has gone into solidifying this foundational piece of our infrastructure.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#the-problem", 
            "text": "Software engineers often encounter problems which require the dissemination of small or moderately sized data sets which don\u2019t fit the label \u201cbig data\u201d.  To solve these problems, we often send the data to an RDBMS or nosql data store and query it at runtime, or serialize the data as json or xml, distribute it, and keep a local copy on each consumer.  Scaling each of these solutions presents different challenges.  Sending the data to an RDBMS, nosql data store, or even a memcached cluster may allow your dataset to grow indefinitely large, but there are limitations on the latency and frequency with which you can interact with that dataset.  Serializing and keeping a local copy (if in RAM) can allow many orders of magnitude lower latency and higher frequency access, but this approach has many scaling challenges:   The dataset size is limited by available RAM.  The full dataset may need to be re-downloaded each time it is updated.  Updating the dataset may require significant CPU resources or impact GC behavior.   Netflix, serving many billions of personalized requests each day, has a few use cases for which the latency of a remote datastore would be highly undesirable given the frequency with which those datasets are accessed.", 
            "title": "The Problem"
        }, 
        {
            "location": "/#the-solution", 
            "text": "Netflix Hollow is a java library and toolset for disseminating in-memory datasets from a single producer to many consumers for high performance read-only access. Hollow aggressively addresses the scaling challenges of in-memory datasets, and is built with servers busily serving requests at or near maximum capacity in mind.  Due to its performance characteristics, Hollow shifts the scale in terms of appropriate dataset sizes for an in-memory solution.  Datasets for which such liberation may never previously have been considered can be candidates for Hollow.     Small to Medium Datasets  Hollow may be entirely appropriate for datasets which, if represented with json or XML, might require in excess of 100GB.  A good rule of thumb in 2017: KB, MB, and often GB, but not TB or PB.   Hollow simultaneously targets three goals:   Maximum development agility  Highly optimized performance and resource management  Extreme stability and reliability", 
            "title": "The Solution"
        }, 
        {
            "location": "/#maximum-agility", 
            "text": "Hollow provides the capability to automatically generate a custom API based on a specific data model, so that consumers can intuitively interact with the data, with the benefit of IDE code completion.  Hollow provides insight tools, which will help users understand their dataset, and how it changes over time, more deeply than ever before:   Comprehensive change history  Diffing entire datasets between arbitrary states  Heap usage analysis  Usage tracking   The toolset available for working with Hollow datasets allows for a surprising variety of operations to be performed with ease, including:   Indexing / Querying for individual records in a dataset  Splitting / Combining entire datasets in many different ways  Filtering individual record types at the consumer to reduce heap footprint", 
            "title": "Maximum Agility"
        }, 
        {
            "location": "/#optimized-performance", 
            "text": "Hollow is hyper-optimized with a few performance metrics at top-of-mind:   Heap footprint  Computational cost of access  GC impact of updates  Computational cost of updates  Network cost of updates   Over time, Hollow automatically calculates the changes in a dataset on the producer.  Instead of retransmitting the entire snapshot of the data for each update, only the changes are disseminated to consumers to keep them up to date.  On consumers, Hollow keeps a compact encoding of the dataset in RAM.  This representation is optimized for both minimizing heap footprint and minimizing access CPU cost.  To retain and keep the dataset updated, Hollow pools and reuses heap memory to avoid GC tenuring.", 
            "title": "Optimized Performance"
        }, 
        {
            "location": "/#extreme-stability", 
            "text": "Hollow has been battle-hardened over more than two years of continuous use at Netflix.  Hollow is used to represent crucial datasets, essential to the fulfillment of the Netflix experience, on servers answering live customer requests.  So although Hollow goes to extraordinary lengths to squeeze every last bit of performance out of servers\u2019 hardware, enormous attention to detail has gone into solidifying this foundational piece of our infrastructure.", 
            "title": "Extreme Stability"
        }, 
        {
            "location": "/quick-start/", 
            "text": "Hollow has an available \nreference implementation\n, which is designed to get you up and running with a demo in minutes.  Then, we'll walk through swapping in a fully functional, production scalable, AWS-based infrastructure implementation in about an hour.\n\n\nThe reference implementation is a great starting point to integrate Hollow for your use case; it contains a simple mocked-up data model, which you can easily modify to suit your needs.\n\n\n\n\nLearn by Doing\n\n\nThis \nQuick Start Guide\n is placed at the beginning of the documentation as a starting point for those who prefer to \"learn by doing\".  If you'd prefer to gain a greater understanding of how everything fits together prior to jumping in, skip ahead to the \nGetting Started\n guide, but do come back later.  \n\n\n\n\nRunning the Demo\n\n\nClone the Reference Implementation\n\n\nStart by cloning the \nnetflix-hollow-reference-implementation\n repo on GitHub:\n\n\ngit clone https://github.com/Netflix/hollow-reference-implementation.git\n\n\n\n\nImport into your IDE\n\n\nImport the project into your IDE.  This project ships with both a \nbuild.gradle\n file and a \npom.xml\n file, so you can either use a gradle plugin or a standard maven plugin to import the dependencies.\n\n\n\n\nDependencies in Hollow\n\n\nThe core Hollow jar does not require or include \nany\n third party dependencies.  The dependencies in the \nreference implementation\n are required for infrastructure demonstration purposes.\n\n\n\n\nStart a Producer\n\n\nThe class \nhow.hollow.producer.Producer\n contains a main method.  Run it.  This will be our data \nproducer\n, and will write data to a directory \npublish-dir\n underneath the temp directory.\n\n\nYou should see output like this:\n\n\nI AM THE PRODUCER.  I WILL PUBLISH TO /tmp/publish-dir\nATTEMPTING TO RESTORE PRIOR STATE...\nRESTORE NOT AVAILABLE\n...\n\n\n\n\nAnd you should have a folder \n/tmp/publish-dir\n with contents like this:\n\n\n    17 announced.version\n   370 delta-20161110185218001-20161110185228002\n   604 delta-20161110185228002-20161110185238003\n   385 reversedelta-20161110185228002-20161110185218001\n   551 reversedelta-20161110185238003-20161110185228002\n597567 snapshot-20161110185218001\n597688 snapshot-20161110185228002\n597813 snapshot-20161110185238003\n\n\n\n\nStart a Consumer\n\n\nThe class \nhow.hollow.consumer.Consumer\n also contains a main method.  Run it.  This will be our data \nconsumer\n, and will read data from the directory \npublish-dir\n underneath the temp directory.  \n\n\nYou should see output like this:\n\n\nI AM THE CONSUMER.  I WILL READ FROM /tmp/publish-dir\nSNAPSHOT COMPLETED IN 45ms\nTYPES: [Actor, Movie, SetOfActor, String]\nDELTA COMPLETED IN 20ms\nTYPES: [Movie, SetOfActor, String]\nSNAPSHOT COMPLETED IN 1ms\nTYPES: [Actor, Movie]\nDELTA COMPLETED IN 12ms\nTYPES: [Actor, Movie, SetOfActor, String]\nDELTA COMPLETED IN 0ms\nTYPES: [Movie]\n\n\n\n\n\nInspecting the Data\n\n\nThat's it!  You now have a producer, which is reading data from a data source and publishing Hollow data to \n/tmp/publish-dir\n, and a consumer which is reading that data and keeping its internal state up to date.  Let's take a look at what the data actually looks like, and how it's changing over time.  The consumer started a \nhistory\n server, so open up a browser and visit \nhttp://localhost:7777\n.\n\n\n\n\nFake Data\n\n\nIn this demo case, there is no data source.  Instead, the producer is manufacturing some randomized fake data on the first cycle, and making some minor random modifications to that data for each cycle.  Inspect the package \nhow.hollow.producer.datamodel\n to see the mock data model and how it is randomized.\n\n\n\n\nPlugging in Infrastructure\n\n\nThe demo we have so far is fully functional, but writing to and reading from a local disk directory is probably not realistic for a production implementation.  In the following few sections, we'll actually set up a simple AWS-based infrastructure which could easily scale to any production deployment.\n\n\n\n\nUsing AWS\n\n\nThis demonstration uses AWS because it is accessible to anyone, easy to set up, and extremely reliable.  In order to proceed, you'll need \nan AWS account\n, which is free to sign up for -- you only pay for the services you use.\n\n\nEven if you're not able to use the prescribed AWS-based infrastructure, running through the following steps will be useful to gain an understanding of how to swap in different infrastructure implementations for use with Hollow.\n\n\n\n\nCreate a User\n\n\nOnce you've logged into your AWS account, select \nIdentity \n Access Management\n from the AWS console:\n\n\n\n\nSelect \nUsers\n, then \nAdd User\n.  Enter a name in box (e.g. \nHollowReference\n).\n\n\n\n\nSelect the checkbox \nProgrammatic access\n.  Click \nNext:Permissions\n in the bottom right corner of the screen.\n\n\nOn the next page, Click \nAttach existing policies directly\n.  \n\n\n\n\nWe'll need \nS3\n and \nDynamoDB\n access for this user.  For now, let's give this user full access to S3.  Select the checkbox next to the policy named \nAmazonS3FullAccess\n:\n\n\n\n\nUsing the same interface, after the S3 policy checkbox is selected, on the same page search for and select the \nAmazonDynamoDBFullAccess\n policy.\n\n\nClick \nNext: Review\n.  You should see the following:\n\n\n\n\nClick \nCreate User\n.  Here you'll see an \nAccess Key ID\n and a \nSecret Access Key\n.  Copy both of these strings and set them off to the side, we'll need them soon.\n\n\nCreate an S3 Bucket\n\n\nBack on the AWS console landing page, select S3:\n\n\n\n\nClick \nCreate Bucket\n, then create a bucket.  Select a unique name for your bucket.  Your bucket can be in any region, here we're using the \nUS Standard\n region:\n\n\n\n\nClick \nNext\n.\n\n\nLeave the defaults on the next couple of screens for now.  Click \nNext\n twice more, then \nCreate Bucket\n.\n\n\nPlug the Producer into S3\n\n\nNow that we've set up our user and our S3 bucket, we can plug the producer into S3.  Open up the \nProducer\n class in the Hollow reference implementation project.  Modify the \nmain\n method: swap the \nPublisher\n and \nAnnouncer\n implementations as follows, replacing \nzzz-hollow-reference-implementation\n with the bucket name you chose in the prior step:\n\n\nAWSCredentials credentials = \n                new BasicAWSCredentials(\nAccess Key ID\n, \nSecret Access Key\n);\n\nPublisher publisher = \n                new S3Publisher(credentials, \nzzz-hollow-reference-implementation\n, \ndemo\n);\nAnnouncer announcer = \n                new S3Announcer(credentials, \nzzz-hollow-reference-implementation\n, \ndemo\n);\n\nHollowProducer producer = HollowProducer.withPublisher(publisher)\n                                        .withAnnouncer(announcer)\n                                        .build();\n\n\n\n\nStart the producer.  After a few cycles, inspect the \ndemo\n folder in your S3 bucket to see what the producer is writing.\n\n\n\n\nWatch your AWS Usage\n\n\nThe AWS meter is running.  Leaving this version of the producer running overnight could run up an AWS bill.  Shut the demo down once you're done!\n\n\n\n\n\n\nClean Up After Yourself\n\n\nIf you do ultimately choose to use S3 as your \nblob storage\n infrastructure, be aware that this implementation does not automatically clean up data for you, which can result in increasing AWS bills.  You should have a cleanup strategy, which can be as simple as adding a \nLifecycle Rule\n to your bucket which will, for example, delete old data after 30 days.\n\n\n\n\nPlug the Consumer into S3\n\n\nNow that our producer is writing data into S3, we need our consumers to read that data.  Open up the \nConsumer\n class in the Hollow reference implementation project.  Modify the \nmain\n method: swap the \nBlobRetriever\n and \nAnnouncementWatcher\n implementations as follows, replacing \nzzz-hollow-reference-implementation\n with the bucket name you chose in the prior step:\n\n\nAWSCredentials credentials = \n                new BasicAWSCredentials(\nAccess Key ID\n, \nSecret Access Key\n);\n\nBlobRetriever blobRetriever = \n        new S3BlobRetriever(credentials, \nzzz-hollow-reference-implementation\n, \ndemo\n);\nAnnouncementWatcher announcementWatcher = \n        new S3AnnouncementWatcher(credentials, \nzzz-hollow-reference-implementation\n, \ndemo\n);\n\nConsumer consumer = HollowConsumer.withBlobRetriever(blobRetriever)\n                                  .withAnnouncementWatcher(announcementWatcher)\n                    ...\n                                  .build();\n\n\n\n\n\nStart the consumer.  You'll see the same output from the demo step, except now the Hollow input data is coming from S3.  \n\n\nAt this point, we have a fully distributed infrastructure.  You can start the producer on one machine, and start a consumer on many other machines, anywhere in the world.  Each of the consumers will update in lock-step each time the producer publishes changes.\n\n\n\n\nAddress already in use Exception\n\n\nNote that the \nConsumer\n attempts to start a \nhistory\n server on port 7777.  Because of this, only one \nConsumer\n can be running on a single machine at one time.  If you get a \njava.net.BindException\n, shut down the other \nConsumer\n and try again.\n\n\n\n\n\n\nPublishing Closer to the Consumer\n\n\nOur implementation currently publishes to a single S3 bucket.  This is OK, but S3 buckets reside in specific AWS regions, and it is often beneficial to publish data closer to your consumers.  For example, if you have some consumers in the AWS region \nUS Standard\n, and some in the region \nSydney\n, then it makes sense to simultaneously publish to one bucket in each region, and have consumers read from the bucket closest to them.\n\n\n\n\nA Better Announcement Infrastructure\n\n\nOur distributed infrastructure thus far leverages S3 for both a \nblob store\n mechanism, and an \nannouncement\n mechanism.  Although S3 is perfectly suitable for blob storage, it's less well suited for announcement.  Instead, we can leverage DynamoDB for the announcement infrastructure, and achieve both improved scalability and better economics.\n\n\nFirst, we need to create a DynamoDB table.  Back on the AWS console landing page, select DynamoDB:\n\n\n\n\nSelect \nCreate table\n and enter a Table name (e.g. \nHollowAnnouncement\n) and use \nnamespace\n as the \nPartition key\n:\n\n\n\n\nSelect \nCreate\n.\n\n\nPlug the Producer into DynamoDB\n\n\nNow that we've set up our DynamoDB table, let's swap it into our producer as our announcement mechanism.  Go back to the \nProducer\n class and modify the \nmain\n method, swapping the \nAnnouncer\n implementation as follows:\n\n\nAnnouncer announcer = new DynamoDBAnnouncer(credentials, \nHollowAnnouncement\n, \ndemo\n);\n\n\n\n\nStart the producer.  After at least one cycle, you'll be able to scan the table and see a record indicating the currently announced version for our \ndemo\n namespace:\n\n\n\n\nPlug the Consumer into DynamoDB\n\n\nNow that our producer is announcing to DynamoDB, of course our consumer needs to look there for update directions.  Go back to the \nConsumer\n class and modify the \nmain\n method, swapping the \nAnnouncementWatcher\n implementation as follows:\n\n\nAnnouncementWatcher announcementWatcher = \n            new DynamoDBAnnouncementWatcher(credentials, \nHollowAnnouncement\n, \ndemo\n);\n\n\n\n\nRun the consumer.  You'll see the same output from prior steps, except now the Hollow input data is coming from S3, and the state version announcement is read from DynamoDB.\n\n\n\n\nPinning The State\n\n\nThe provided \nDynamoDBAnnouncementWatcher\n can be \npinned\n.  If the announcement item in DynamoDB contains a field \npin_version\n, then the consumer will go to the version indicated in that field, instead of the version the producer announces.  See \nPinning Consumers\n for more details about pinning data.\n\n\n\n\nPlug In Your Data Model\n\n\nCongratulations!  You now have a living, breathing, fully production scalable implementation of Hollow in your hands.  All that's left to do is substitute your data model for the example provided.  The data model can be defined, as in this example, with a set of POJO classes.  Take this starting point and make it your own -- remove the classes underneath \nhow.hollow.producer.datamodel\n and replace them with your data model.\n\n\n\n\nModeling Data with POJOs\n\n\nThe \nHollowObjectMapper\n section of this document provides details about how to convert your POJO data model into Hollow.\n\n\n\n\nContinue on to the \nGetting Started\n guide to learn more about how the fundamental pieces of Hollow fit together, how to create a consumer API custom-tailored to your data model, and how to index your data for easy, efficient retrieval by consumers.", 
            "title": "Quick Start"
        }, 
        {
            "location": "/quick-start/#running-the-demo", 
            "text": "", 
            "title": "Running the Demo"
        }, 
        {
            "location": "/quick-start/#clone-the-reference-implementation", 
            "text": "Start by cloning the  netflix-hollow-reference-implementation  repo on GitHub:  git clone https://github.com/Netflix/hollow-reference-implementation.git", 
            "title": "Clone the Reference Implementation"
        }, 
        {
            "location": "/quick-start/#import-into-your-ide", 
            "text": "Import the project into your IDE.  This project ships with both a  build.gradle  file and a  pom.xml  file, so you can either use a gradle plugin or a standard maven plugin to import the dependencies.   Dependencies in Hollow  The core Hollow jar does not require or include  any  third party dependencies.  The dependencies in the  reference implementation  are required for infrastructure demonstration purposes.", 
            "title": "Import into your IDE"
        }, 
        {
            "location": "/quick-start/#start-a-producer", 
            "text": "The class  how.hollow.producer.Producer  contains a main method.  Run it.  This will be our data  producer , and will write data to a directory  publish-dir  underneath the temp directory.  You should see output like this:  I AM THE PRODUCER.  I WILL PUBLISH TO /tmp/publish-dir\nATTEMPTING TO RESTORE PRIOR STATE...\nRESTORE NOT AVAILABLE\n...  And you should have a folder  /tmp/publish-dir  with contents like this:      17 announced.version\n   370 delta-20161110185218001-20161110185228002\n   604 delta-20161110185228002-20161110185238003\n   385 reversedelta-20161110185228002-20161110185218001\n   551 reversedelta-20161110185238003-20161110185228002\n597567 snapshot-20161110185218001\n597688 snapshot-20161110185228002\n597813 snapshot-20161110185238003", 
            "title": "Start a Producer"
        }, 
        {
            "location": "/quick-start/#start-a-consumer", 
            "text": "The class  how.hollow.consumer.Consumer  also contains a main method.  Run it.  This will be our data  consumer , and will read data from the directory  publish-dir  underneath the temp directory.    You should see output like this:  I AM THE CONSUMER.  I WILL READ FROM /tmp/publish-dir\nSNAPSHOT COMPLETED IN 45ms\nTYPES: [Actor, Movie, SetOfActor, String]\nDELTA COMPLETED IN 20ms\nTYPES: [Movie, SetOfActor, String]\nSNAPSHOT COMPLETED IN 1ms\nTYPES: [Actor, Movie]\nDELTA COMPLETED IN 12ms\nTYPES: [Actor, Movie, SetOfActor, String]\nDELTA COMPLETED IN 0ms\nTYPES: [Movie]", 
            "title": "Start a Consumer"
        }, 
        {
            "location": "/quick-start/#inspecting-the-data", 
            "text": "That's it!  You now have a producer, which is reading data from a data source and publishing Hollow data to  /tmp/publish-dir , and a consumer which is reading that data and keeping its internal state up to date.  Let's take a look at what the data actually looks like, and how it's changing over time.  The consumer started a  history  server, so open up a browser and visit  http://localhost:7777 .   Fake Data  In this demo case, there is no data source.  Instead, the producer is manufacturing some randomized fake data on the first cycle, and making some minor random modifications to that data for each cycle.  Inspect the package  how.hollow.producer.datamodel  to see the mock data model and how it is randomized.", 
            "title": "Inspecting the Data"
        }, 
        {
            "location": "/quick-start/#plugging-in-infrastructure", 
            "text": "The demo we have so far is fully functional, but writing to and reading from a local disk directory is probably not realistic for a production implementation.  In the following few sections, we'll actually set up a simple AWS-based infrastructure which could easily scale to any production deployment.   Using AWS  This demonstration uses AWS because it is accessible to anyone, easy to set up, and extremely reliable.  In order to proceed, you'll need  an AWS account , which is free to sign up for -- you only pay for the services you use.  Even if you're not able to use the prescribed AWS-based infrastructure, running through the following steps will be useful to gain an understanding of how to swap in different infrastructure implementations for use with Hollow.", 
            "title": "Plugging in Infrastructure"
        }, 
        {
            "location": "/quick-start/#create-a-user", 
            "text": "Once you've logged into your AWS account, select  Identity   Access Management  from the AWS console:   Select  Users , then  Add User .  Enter a name in box (e.g.  HollowReference ).   Select the checkbox  Programmatic access .  Click  Next:Permissions  in the bottom right corner of the screen.  On the next page, Click  Attach existing policies directly .     We'll need  S3  and  DynamoDB  access for this user.  For now, let's give this user full access to S3.  Select the checkbox next to the policy named  AmazonS3FullAccess :   Using the same interface, after the S3 policy checkbox is selected, on the same page search for and select the  AmazonDynamoDBFullAccess  policy.  Click  Next: Review .  You should see the following:   Click  Create User .  Here you'll see an  Access Key ID  and a  Secret Access Key .  Copy both of these strings and set them off to the side, we'll need them soon.", 
            "title": "Create a User"
        }, 
        {
            "location": "/quick-start/#create-an-s3-bucket", 
            "text": "Back on the AWS console landing page, select S3:   Click  Create Bucket , then create a bucket.  Select a unique name for your bucket.  Your bucket can be in any region, here we're using the  US Standard  region:   Click  Next .  Leave the defaults on the next couple of screens for now.  Click  Next  twice more, then  Create Bucket .", 
            "title": "Create an S3 Bucket"
        }, 
        {
            "location": "/quick-start/#plug-the-producer-into-s3", 
            "text": "Now that we've set up our user and our S3 bucket, we can plug the producer into S3.  Open up the  Producer  class in the Hollow reference implementation project.  Modify the  main  method: swap the  Publisher  and  Announcer  implementations as follows, replacing  zzz-hollow-reference-implementation  with the bucket name you chose in the prior step:  AWSCredentials credentials = \n                new BasicAWSCredentials( Access Key ID ,  Secret Access Key );\n\nPublisher publisher = \n                new S3Publisher(credentials,  zzz-hollow-reference-implementation ,  demo );\nAnnouncer announcer = \n                new S3Announcer(credentials,  zzz-hollow-reference-implementation ,  demo );\n\nHollowProducer producer = HollowProducer.withPublisher(publisher)\n                                        .withAnnouncer(announcer)\n                                        .build();  Start the producer.  After a few cycles, inspect the  demo  folder in your S3 bucket to see what the producer is writing.   Watch your AWS Usage  The AWS meter is running.  Leaving this version of the producer running overnight could run up an AWS bill.  Shut the demo down once you're done!    Clean Up After Yourself  If you do ultimately choose to use S3 as your  blob storage  infrastructure, be aware that this implementation does not automatically clean up data for you, which can result in increasing AWS bills.  You should have a cleanup strategy, which can be as simple as adding a  Lifecycle Rule  to your bucket which will, for example, delete old data after 30 days.", 
            "title": "Plug the Producer into S3"
        }, 
        {
            "location": "/quick-start/#plug-the-consumer-into-s3", 
            "text": "Now that our producer is writing data into S3, we need our consumers to read that data.  Open up the  Consumer  class in the Hollow reference implementation project.  Modify the  main  method: swap the  BlobRetriever  and  AnnouncementWatcher  implementations as follows, replacing  zzz-hollow-reference-implementation  with the bucket name you chose in the prior step:  AWSCredentials credentials = \n                new BasicAWSCredentials( Access Key ID ,  Secret Access Key );\n\nBlobRetriever blobRetriever = \n        new S3BlobRetriever(credentials,  zzz-hollow-reference-implementation ,  demo );\nAnnouncementWatcher announcementWatcher = \n        new S3AnnouncementWatcher(credentials,  zzz-hollow-reference-implementation ,  demo );\n\nConsumer consumer = HollowConsumer.withBlobRetriever(blobRetriever)\n                                  .withAnnouncementWatcher(announcementWatcher)\n                    ...\n                                  .build();  Start the consumer.  You'll see the same output from the demo step, except now the Hollow input data is coming from S3.    At this point, we have a fully distributed infrastructure.  You can start the producer on one machine, and start a consumer on many other machines, anywhere in the world.  Each of the consumers will update in lock-step each time the producer publishes changes.   Address already in use Exception  Note that the  Consumer  attempts to start a  history  server on port 7777.  Because of this, only one  Consumer  can be running on a single machine at one time.  If you get a  java.net.BindException , shut down the other  Consumer  and try again.    Publishing Closer to the Consumer  Our implementation currently publishes to a single S3 bucket.  This is OK, but S3 buckets reside in specific AWS regions, and it is often beneficial to publish data closer to your consumers.  For example, if you have some consumers in the AWS region  US Standard , and some in the region  Sydney , then it makes sense to simultaneously publish to one bucket in each region, and have consumers read from the bucket closest to them.", 
            "title": "Plug the Consumer into S3"
        }, 
        {
            "location": "/quick-start/#a-better-announcement-infrastructure", 
            "text": "Our distributed infrastructure thus far leverages S3 for both a  blob store  mechanism, and an  announcement  mechanism.  Although S3 is perfectly suitable for blob storage, it's less well suited for announcement.  Instead, we can leverage DynamoDB for the announcement infrastructure, and achieve both improved scalability and better economics.  First, we need to create a DynamoDB table.  Back on the AWS console landing page, select DynamoDB:   Select  Create table  and enter a Table name (e.g.  HollowAnnouncement ) and use  namespace  as the  Partition key :   Select  Create .", 
            "title": "A Better Announcement Infrastructure"
        }, 
        {
            "location": "/quick-start/#plug-the-producer-into-dynamodb", 
            "text": "Now that we've set up our DynamoDB table, let's swap it into our producer as our announcement mechanism.  Go back to the  Producer  class and modify the  main  method, swapping the  Announcer  implementation as follows:  Announcer announcer = new DynamoDBAnnouncer(credentials,  HollowAnnouncement ,  demo );  Start the producer.  After at least one cycle, you'll be able to scan the table and see a record indicating the currently announced version for our  demo  namespace:", 
            "title": "Plug the Producer into DynamoDB"
        }, 
        {
            "location": "/quick-start/#plug-the-consumer-into-dynamodb", 
            "text": "Now that our producer is announcing to DynamoDB, of course our consumer needs to look there for update directions.  Go back to the  Consumer  class and modify the  main  method, swapping the  AnnouncementWatcher  implementation as follows:  AnnouncementWatcher announcementWatcher = \n            new DynamoDBAnnouncementWatcher(credentials,  HollowAnnouncement ,  demo );  Run the consumer.  You'll see the same output from prior steps, except now the Hollow input data is coming from S3, and the state version announcement is read from DynamoDB.   Pinning The State  The provided  DynamoDBAnnouncementWatcher  can be  pinned .  If the announcement item in DynamoDB contains a field  pin_version , then the consumer will go to the version indicated in that field, instead of the version the producer announces.  See  Pinning Consumers  for more details about pinning data.", 
            "title": "Plug the Consumer into DynamoDB"
        }, 
        {
            "location": "/quick-start/#plug-in-your-data-model", 
            "text": "Congratulations!  You now have a living, breathing, fully production scalable implementation of Hollow in your hands.  All that's left to do is substitute your data model for the example provided.  The data model can be defined, as in this example, with a set of POJO classes.  Take this starting point and make it your own -- remove the classes underneath  how.hollow.producer.datamodel  and replace them with your data model.   Modeling Data with POJOs  The  HollowObjectMapper  section of this document provides details about how to convert your POJO data model into Hollow.   Continue on to the  Getting Started  guide to learn more about how the fundamental pieces of Hollow fit together, how to create a consumer API custom-tailored to your data model, and how to index your data for easy, efficient retrieval by consumers.", 
            "title": "Plug In Your Data Model"
        }, 
        {
            "location": "/getting-started/", 
            "text": "In the \nQuick Start\n guide, we got a reference implementation of Hollow up and running, with a mock data model that can be easily modified to suit any use case.  After reading this section, you'll have an understanding of the basic usage patterns for Hollow, and how each of the core pieces fit together.\n\n\nCore Concepts\n\n\nHollow manages datasets which are built by a single \nproducer\n, and disseminated to one or many \nconsumers\n for read-only access.  A dataset changes over time.  The timeline for a changing dataset can be broken down into discrete \ndata states\n, each of which is a complete snapshot of the data at a particular point in time.\n\n\nProducing a Data Snapshot\n\n\nLet's assume we have a POJO class \nMovie\n:\n\n\npublic class Movie {\n    long id;\n    String title;\n    int releaseYear;\n\n    public Movie(long id, String title, int releaseYear) {\n        this.id = id;\n        this.title = title;\n        this.releaseYear = releaseYear;\n    }\n}\n\n\n\n\nAnd that many \nMovie\ns exist which comprise a dataset that needs to be disseminated:\n\n\nList\nMovie\n movies = Arrays.asList(\n        new Movie(1, \nThe Matrix\n, 1999),\n        new Movie(2, \nBeasts of No Nation\n, 2015),\n        new Movie(3, \nPulp Fiction\n, 1994)\n);\n\n\n\n\nWe'll need a data \nproducer\n to create a data state which will be transmitted to consumers:\n\n\nFile localPublishDir = new File(\n/path/to/local/disk/publish/dir\n);\n\nHollowFilesystemPublisher publisher = new HollowFilesystemPublisher(localPublishDir);\nHollowFilesystemAnnouncer announcer = new HollowFilesystemAnnouncer(localPublishDir);\n\nHollowProducer producer = HollowProducer.withPublisher(publisher)\n                                        .withAnnouncer(announcer)\n                                        .build();\n\nproducer.runCycle(new Populator() {\n    public void populate(HollowProducer.WriteState state) {\n        for(Movie movie : movies)\n            state.add(movie);\n    }\n});\n\n\n\n\n\nOr, if you prefer, using Java 8:\n\n\nproducer.runCycle(state -\n {\n    for(Movie movie : movies)\n        state.add(movie);\n});\n\n\n\n\nThis producer runs a single \ncycle\n and produces a data state.  Once this runs, you should have a \nsnapshot\n blob file on your local disk.  \n\n\n\n\nPublishing Blobs\n\n\nNote that the example code above is writing data to local disk.  This is a great way to start testing.  In a production scenario, data can be written to a remote file store such as Amazon S3 for retrieval by consumers.  See the \nreference implementation\n and the \nquick start guide\n for a scalable example using AWS.\n\n\n\n\nConsumer API Generation\n\n\nOnce the data has been populated into a producer, that producer's \nstate engine\n is aware of the data model, and can be used to automatically produce a client API.  We can also initialize the data model from a brand new \nstate engine\n using our POJOs:\n\n\nHollowWriteStateEngine writeEngine = new HollowWriteStateEngine();\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\nmapper.initializeTypeState(Movie.class);\n\nHollowAPIGenerator generator = \n       new HollowAPIGenerator.Builder().withAPIClassname(\nMovieAPI\n)\n                                       .withPackageName(\nhow.hollow.example\n)\n                                       .withDataModel(writeEngine)\n                                       .build();\n\ngenerator.generateFiles(\n/path/to/java/api/files\n);\n\n\n\n\nAfter this code executes, an set of Java files will be written to the location \n/path/to/java/api/files\n.  These java files will be a generated API based on the data model defined by the schemas in our state engine, and will provide convenient methods to access that data.\n\n\n\n\nInitializing multiple types\n\n\nIf we have multiple top-level types, we should call \ninitializeTypeState()\n multiple times, once for each class.\n\n\n\n\nConsuming a Data Snapshot\n\n\nA data consumer can load a snapshot created by the producer into memory:\n\n\nFile localPublishDir = new File(\n/path/to/local/disk/publish/dir\n);\n\nHollowFilesystemBlobRetriever blobRetriever = \n                                new HollowFilesystemBlobRetriever(localPublishDir);\n\nHollowFilesystemAnnouncementWatcher announcementWatcher = \n                                new HollowFilesystemAnnouncementWatcher(localPublishDir);\n\nHollowConsumer consumer = HollowConsumer.withBlobRetriever(blobRetriever)\n                                        .withAnnouncementWatcher(announcementWatcher)\n                                        .withGeneratedAPIClass(MovieAPI.class)\n                                        .build();\n\nconsumer.triggerRefresh();\n\n\n\n\nThe \nHollowConsumer\n will retrieve data using the provided \nBlobRetrievier\n, and will load the latest \ndata state\n currently announced by the \nAnnouncementWatcher\n.\n\n\nOnce this dataset is loaded into memory, we can access the data for any records using our generated API.  Below, we're iterating over all records:\n\n\nMovieAPI movieApi = (MovieAPI)consumer.getAPI();\n\nfor(MovieHollow movie : movieApi.getAllMovieHollow()) {\n    System.out.println(movie.getId() + \n, \n + \n                       movie.getTitle().getValue() + \n, \n + \n                       movie.getReleaseYear());\n}\n\n\n\n\nThe output of the above code will be:\n\n\n1, The Matrix, 1999\n2, Beasts of No Nation, 2015\n3, Pulp Fiction, 1994\n\n\n\n\n\n\nIntegrating with Infrastructure\n\n\nIn order to integrate with your infrastructure, you only need to provide Hollow with four implementations of simple interfaces: \n\n\n\n\nThe \nHollowProducer\n needs a \nPublisher\n and \nAnnouncer\n\n\nThe \nHollowConsumer\n needs a \nBlobRetriever\n and \nAnnouncementWatcher\n\n\n\n\nYour \nBlobRetriever\n and \nAnnouncementWatcher\n implementations should be mirror your \nPublisher\n and \nAnnouncer\n interfaces.   Here, we're publishing and retrieving from local disk.  In production, we'll be publishing to and retrieving from a remote file store.  We'll discuss in more detail how to integrate with your specific infrastructure in \nInfrastructure Integration\n.\n\n\n\n\nProducing a Delta\n\n\nSome time has passed and the dataset has evolved.  It now contains these records:\n\n\nList\nMovie\n movies = Arrays.asList(\n        new Movie(1, \nThe Matrix\n, 1999),\n        new Movie(2, \nBeasts of No Nation\n, 2015),\n        new Movie(4, \nGoodfellas\n, 1990),\n        new Movie(5, \nInception\n, 2010)\n);\n\n\n\n\nThe producer, needs to communicate this updated dataset to consumers.  We're going to create a brand new state, and the entirety of the data for the new state must be added to the state engine in a new \ncycle\n.   When the cycle runs, a new data state will be \npublished\n, and the new data state's (automatically generated) version identifier will be \nannounced\n.\n\n\nUsing the same \nHollowProducer\n in memory, we can use the following code:\n\n\nproducer.runCycle(state -\n {\n    for(Movie movie : movies)\n        state.add(movie);\n});\n\n\n\n\nLet's take a closer look at what the above code does.  The same \nHollowProducer\n which was used to produce the \nsnapshot\n blob is used -- it already knows everything about the prior state and can be transitioned to the next state.  When creating a new state, \nall of the movies currently in our dataset are re-added again.\n  It's not necessary to figure out which records were added, removed, or modified -- that's Hollow's job.\n\n\nEach time we call \nrunCycle\n we will be producing a \ndata state\n.  For each state after the first, the \nHollowProducer\n will publish three artifacts: a \nsnapshot\n, a \ndelta\n, and a \nreverse delta\n.  Encoded into the \ndelta\n is a set of instructions to update a consumer\u2019s data store from the previous state to the current state.  Inversely, encoded into each \nreverse delta\n is a set of instructions to update a consumer in reverse -- from the current state to the previous state.  Consumers may use the \nreverse delta\n later if we need to \npin\n.\n\n\nWhen consumers initialize, they will use the most recent \nsnapshot\n to initialize their data store.  After initialization, consumers will keep up to date using \ndeltas\n.\n\n\n\n\nProducer Cycles\n\n\nWe call what the producer does to create a data state a \ncycle\n.  During each \ncycle\n, you\u2019ll want to add \nevery record\n from your source of truth.  Hollow will handle the details of publishing a delta for all of your established consumer instances, and a snapshot to initialize any consumer instances which start up before your next cycle.\n\n\n\n\nConsuming a Delta\n\n\nNo manual intervention is necessary to consume the delta you produced.  The \nHollowConsumer\n will automatically stay up-to-date.  \n\n\n\n\nAnnouncements keep consumers updated\n\n\nWhen the producer runs a cycle, it \nannounces\n the latest version.  The \nAnnouncementWatcher\n implementation provided to the \nHollowConsumer\n will listen for changes to the announced version -- and when updates occur notify the \nHollowConsumer\n by calling \ntriggerAsyncRefresh()\n.  See the source of the \nHollowFilesystemAnnouncementWatcher\n, or the \ntwo\n separate \nexamples\n in the reference implementation.\n\n\n\n\nAfter this delta has been applied, the consumer is at the new state.  If the generated API is used to iterate over the movies again as shown in the prior consumer example, the new output will be:\n\n\n1, The Matrix, 1999\n2, Beasts of No Nation, 2015\n4, Goodfellas, 1990\n5, Inception, 2010\n\n\n\n\n\n\nThread Safety\n\n\nIt is safe to use Hollow to retrieve data while a delta transition is in progress.\n\n\n\n\n\n\nAdjacent States\n\n\nWe refer to states which are directly connected via single delta transitions as \nadjacent\n states, and a continuous set of adjacent states as a \ndelta chain\n\n\n\n\nIndexing Data for Retrieval\n\n\nIn prior examples the generated Hollow API was used by the data consumer to iterate over all \nMovie\n records in the dataset.  Most often, however, it isn\u2019t desirable to iterate over the entire dataset \u2014 instead, specific records will be accessed based on some known key.  Let\u2019s assume that the \nMovie\n\u2019s id is a known key.\n\n\nAfter a \nHollowConsumer\n has been initialized, any type can be indexed.  For example, we can index \nMovie\n records by \nid\n:\n\n\nHollowConsumer consumer = ...;\n\nconsumer.triggerRefresh();\n\nMoviePrimaryKeyIndex idx = new MoviePrimaryKeyIndex(consumer, \nid\n);\n\n\n\n\nThis index can be held in memory and then used in conjunction with the generated Hollow API to retrieve Movie records by id:\n\n\nMovie movie = idx.findMatch(2);\nif(movie != null)\n    System.out.println(\nFound Movie: \n + movie.getTitle().getValue());\n\n\n\n\nWhich outputs:\n\n\nFound Movie: Beasts of No Nation\n\n\n\n\nIn our generated API, each type in our data model has a generated index class.  We can index by any field, or multiple fields.\n\n\n\n\nReuse Indexes\n\n\nRetrieval from an index is extremely cheap, and indexing is (relatively) expensive.  You should create your indexes when the \nHollowConsumer\n is initialized and share them thereafter.  Indexes will automatically stay up-to-date with the \nHollowConsumer\n.\n\n\n\n\n\n\nThread Safety\n\n\nRetrievals from Hollow indexes are thread-safe.  They are safe to use across multiple threads, and it is safe to query while a transition is in progress.\n\n\n\n\nWe've just begun to scratch the surface of what indexes can do.  See \nIndexing/Querying\n for an in-depth exploration of this topic.\n\n\nHierarchical Data Models\n\n\nOur data models can be much richer than in the prior example.  Assume an updated \nMovie\n class:\n\n\npublic class Movie {\n    long id;\n    String title;\n    int releaseYear;\n    List\nActor\n actors;\n\n    public Movie(long id, String title, int year, List\nActor\n actors) {\n        this.id = id;\n        this.title = title;\n        this.releaseYear = year;\n        this.actors = actors;\n    }\n}\n\n\n\n\nWhich references \nActor\n records:\n\n\npublic class Actor {\n    long actorId;\n    String actorName;\n\n    public Actor(long actorId, String actorName) {\n        this.actorId = actorId;\n        this.actorName = actorName;\n    }\n}\n\n\n\n\nSome records are added to a \nHollowProducer\n:\n\n\nList\nMovie\n movies = Arrays.asList(\n        new Movie(1, \nThe Matrix\n, 1999, Arrays.asList(\n                new Actor(101, \nKeanu Reeves\n),\n                new Actor(102, \nLaurence Fishburne\n),\n                new Actor(103, \nCarrie-Ann Moss\n),\n                new Actor(104, \nHugo Weaving\n)\n        )),\n        new Movie(6, \nEvent Horizon\n, 1997, Arrays.asList(\n                new Actor(102, \nLaurence Fishburne\n),\n                new Actor(105, \nSam Neill\n)\n        ))\n);\n\nproducer.runCycle(state -\n {\n    for(Movie movie : movies)\n        state.addObject(movie);\n});\n\n\n\n\nWhen we add these movies to the dataset, Hollow will traverse everything referenced by the provided records and add them to the state as well.  Consequently, both a type \nMovie\n and a type \nActor\n will exist in the data model after the above code runs.  \n\n\n\n\nDeduplication\n\n\nLaurence Fishburne starred in both of these films.  Rather than creating two \nActor\n records for Mr. Fishburne, a single record will be created and assigned to both of our \nMovie\n records.  This \ndeduplication\n happens automatically by virtue of having the exact same data contained in both Actor inputs.\n\n\n\n\nConsumers of this dataset may want to also create an index for \nActor\n records.  For example:\n\n\nActorPrimaryKeyIndex actorIdx = new ActorPrimaryKeyIndex(consumer, \nactorId\n);\n\nActor actor = actorIdx.getMatchingOrdinal(102);\nif(actor != null)\n    System.out.println(\nFound Actor: \n + actor.getActorName().getValue());\n\n\n\n\nOutputs:\n\n\nFound Actor: Laurence Fishburne\n\n\n\n\nRestoring at Startup\n\n\nFrom time to time, we need to redeploy our producer.  When we first create a \nHollowProducer\n and run a cycle it will not be able to produce a delta, because it does not know anything about the prior \ndata state\n.  If no action is taken, a new state with only a snapshot will be produced and announced, and clients will load that data state with an operation called a \ndouble snapshot\n, which has potentially undesirable performance characteristics.  \n\n\nWe can remedy this situation by \nrestoring\n our newly created producer with the last announced data state.  For example:\n\n\nPublisher publisher = ...\nAnnouncer announcer = ...\nBlobRetriever blobRetriever = ...\nAnnouncementWatcher announcementWatcher = ...\n\nHollowProducer producer = HollowProducer.withPublisher(publisher)\n                                        .withAnnouncer(announcer)\n                                        .build();\n\nproducer.initializeDataModel(Movie.class);\n\nlong latestAnnouncedVersion = announcementWatcher.getLatestVersion();\nproducer.restore(latestAnnouncedVersion, blobRetriever);\n\nproducer.runCycle(new Populator() {\n   ...\n});\n\n\n\n\n\nIn the above code, we first \ninitialize\n the data model by providing the set of classes we will add during the cycle.  After that, we \nrestore\n by providing our \nBlobRetriever\n implementation, along with the version which should be restored.  The \nHollowProducer\n will will use the \nBlobRetriever\n to load the desired state, then use it to \nrestore\n itself.  In this way, a delta can be produced at startup, and consumers will not have to load a snapshot to get up-to-date.\n\n\n\n\nInitializing the data model\n\n\nBefore \nrestoring\n, we must always \ninitialize\n our data model.  When a data model changes between deployments, Hollow will automatically merge records of types which have changed.  In order to do this correctly, Hollow needs to know about the current data model before the restore operation begins.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting-started/#core-concepts", 
            "text": "Hollow manages datasets which are built by a single  producer , and disseminated to one or many  consumers  for read-only access.  A dataset changes over time.  The timeline for a changing dataset can be broken down into discrete  data states , each of which is a complete snapshot of the data at a particular point in time.", 
            "title": "Core Concepts"
        }, 
        {
            "location": "/getting-started/#producing-a-data-snapshot", 
            "text": "Let's assume we have a POJO class  Movie :  public class Movie {\n    long id;\n    String title;\n    int releaseYear;\n\n    public Movie(long id, String title, int releaseYear) {\n        this.id = id;\n        this.title = title;\n        this.releaseYear = releaseYear;\n    }\n}  And that many  Movie s exist which comprise a dataset that needs to be disseminated:  List Movie  movies = Arrays.asList(\n        new Movie(1,  The Matrix , 1999),\n        new Movie(2,  Beasts of No Nation , 2015),\n        new Movie(3,  Pulp Fiction , 1994)\n);  We'll need a data  producer  to create a data state which will be transmitted to consumers:  File localPublishDir = new File( /path/to/local/disk/publish/dir );\n\nHollowFilesystemPublisher publisher = new HollowFilesystemPublisher(localPublishDir);\nHollowFilesystemAnnouncer announcer = new HollowFilesystemAnnouncer(localPublishDir);\n\nHollowProducer producer = HollowProducer.withPublisher(publisher)\n                                        .withAnnouncer(announcer)\n                                        .build();\n\nproducer.runCycle(new Populator() {\n    public void populate(HollowProducer.WriteState state) {\n        for(Movie movie : movies)\n            state.add(movie);\n    }\n});  Or, if you prefer, using Java 8:  producer.runCycle(state -  {\n    for(Movie movie : movies)\n        state.add(movie);\n});  This producer runs a single  cycle  and produces a data state.  Once this runs, you should have a  snapshot  blob file on your local disk.     Publishing Blobs  Note that the example code above is writing data to local disk.  This is a great way to start testing.  In a production scenario, data can be written to a remote file store such as Amazon S3 for retrieval by consumers.  See the  reference implementation  and the  quick start guide  for a scalable example using AWS.", 
            "title": "Producing a Data Snapshot"
        }, 
        {
            "location": "/getting-started/#consumer-api-generation", 
            "text": "Once the data has been populated into a producer, that producer's  state engine  is aware of the data model, and can be used to automatically produce a client API.  We can also initialize the data model from a brand new  state engine  using our POJOs:  HollowWriteStateEngine writeEngine = new HollowWriteStateEngine();\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\nmapper.initializeTypeState(Movie.class);\n\nHollowAPIGenerator generator = \n       new HollowAPIGenerator.Builder().withAPIClassname( MovieAPI )\n                                       .withPackageName( how.hollow.example )\n                                       .withDataModel(writeEngine)\n                                       .build();\n\ngenerator.generateFiles( /path/to/java/api/files );  After this code executes, an set of Java files will be written to the location  /path/to/java/api/files .  These java files will be a generated API based on the data model defined by the schemas in our state engine, and will provide convenient methods to access that data.   Initializing multiple types  If we have multiple top-level types, we should call  initializeTypeState()  multiple times, once for each class.", 
            "title": "Consumer API Generation"
        }, 
        {
            "location": "/getting-started/#consuming-a-data-snapshot", 
            "text": "A data consumer can load a snapshot created by the producer into memory:  File localPublishDir = new File( /path/to/local/disk/publish/dir );\n\nHollowFilesystemBlobRetriever blobRetriever = \n                                new HollowFilesystemBlobRetriever(localPublishDir);\n\nHollowFilesystemAnnouncementWatcher announcementWatcher = \n                                new HollowFilesystemAnnouncementWatcher(localPublishDir);\n\nHollowConsumer consumer = HollowConsumer.withBlobRetriever(blobRetriever)\n                                        .withAnnouncementWatcher(announcementWatcher)\n                                        .withGeneratedAPIClass(MovieAPI.class)\n                                        .build();\n\nconsumer.triggerRefresh();  The  HollowConsumer  will retrieve data using the provided  BlobRetrievier , and will load the latest  data state  currently announced by the  AnnouncementWatcher .  Once this dataset is loaded into memory, we can access the data for any records using our generated API.  Below, we're iterating over all records:  MovieAPI movieApi = (MovieAPI)consumer.getAPI();\n\nfor(MovieHollow movie : movieApi.getAllMovieHollow()) {\n    System.out.println(movie.getId() +  ,   + \n                       movie.getTitle().getValue() +  ,   + \n                       movie.getReleaseYear());\n}  The output of the above code will be:  1, The Matrix, 1999\n2, Beasts of No Nation, 2015\n3, Pulp Fiction, 1994   Integrating with Infrastructure  In order to integrate with your infrastructure, you only need to provide Hollow with four implementations of simple interfaces:    The  HollowProducer  needs a  Publisher  and  Announcer  The  HollowConsumer  needs a  BlobRetriever  and  AnnouncementWatcher   Your  BlobRetriever  and  AnnouncementWatcher  implementations should be mirror your  Publisher  and  Announcer  interfaces.   Here, we're publishing and retrieving from local disk.  In production, we'll be publishing to and retrieving from a remote file store.  We'll discuss in more detail how to integrate with your specific infrastructure in  Infrastructure Integration .", 
            "title": "Consuming a Data Snapshot"
        }, 
        {
            "location": "/getting-started/#producing-a-delta", 
            "text": "Some time has passed and the dataset has evolved.  It now contains these records:  List Movie  movies = Arrays.asList(\n        new Movie(1,  The Matrix , 1999),\n        new Movie(2,  Beasts of No Nation , 2015),\n        new Movie(4,  Goodfellas , 1990),\n        new Movie(5,  Inception , 2010)\n);  The producer, needs to communicate this updated dataset to consumers.  We're going to create a brand new state, and the entirety of the data for the new state must be added to the state engine in a new  cycle .   When the cycle runs, a new data state will be  published , and the new data state's (automatically generated) version identifier will be  announced .  Using the same  HollowProducer  in memory, we can use the following code:  producer.runCycle(state -  {\n    for(Movie movie : movies)\n        state.add(movie);\n});  Let's take a closer look at what the above code does.  The same  HollowProducer  which was used to produce the  snapshot  blob is used -- it already knows everything about the prior state and can be transitioned to the next state.  When creating a new state,  all of the movies currently in our dataset are re-added again.   It's not necessary to figure out which records were added, removed, or modified -- that's Hollow's job.  Each time we call  runCycle  we will be producing a  data state .  For each state after the first, the  HollowProducer  will publish three artifacts: a  snapshot , a  delta , and a  reverse delta .  Encoded into the  delta  is a set of instructions to update a consumer\u2019s data store from the previous state to the current state.  Inversely, encoded into each  reverse delta  is a set of instructions to update a consumer in reverse -- from the current state to the previous state.  Consumers may use the  reverse delta  later if we need to  pin .  When consumers initialize, they will use the most recent  snapshot  to initialize their data store.  After initialization, consumers will keep up to date using  deltas .   Producer Cycles  We call what the producer does to create a data state a  cycle .  During each  cycle , you\u2019ll want to add  every record  from your source of truth.  Hollow will handle the details of publishing a delta for all of your established consumer instances, and a snapshot to initialize any consumer instances which start up before your next cycle.", 
            "title": "Producing a Delta"
        }, 
        {
            "location": "/getting-started/#consuming-a-delta", 
            "text": "No manual intervention is necessary to consume the delta you produced.  The  HollowConsumer  will automatically stay up-to-date.     Announcements keep consumers updated  When the producer runs a cycle, it  announces  the latest version.  The  AnnouncementWatcher  implementation provided to the  HollowConsumer  will listen for changes to the announced version -- and when updates occur notify the  HollowConsumer  by calling  triggerAsyncRefresh() .  See the source of the  HollowFilesystemAnnouncementWatcher , or the  two  separate  examples  in the reference implementation.   After this delta has been applied, the consumer is at the new state.  If the generated API is used to iterate over the movies again as shown in the prior consumer example, the new output will be:  1, The Matrix, 1999\n2, Beasts of No Nation, 2015\n4, Goodfellas, 1990\n5, Inception, 2010   Thread Safety  It is safe to use Hollow to retrieve data while a delta transition is in progress.    Adjacent States  We refer to states which are directly connected via single delta transitions as  adjacent  states, and a continuous set of adjacent states as a  delta chain", 
            "title": "Consuming a Delta"
        }, 
        {
            "location": "/getting-started/#indexing-data-for-retrieval", 
            "text": "In prior examples the generated Hollow API was used by the data consumer to iterate over all  Movie  records in the dataset.  Most often, however, it isn\u2019t desirable to iterate over the entire dataset \u2014 instead, specific records will be accessed based on some known key.  Let\u2019s assume that the  Movie \u2019s id is a known key.  After a  HollowConsumer  has been initialized, any type can be indexed.  For example, we can index  Movie  records by  id :  HollowConsumer consumer = ...;\n\nconsumer.triggerRefresh();\n\nMoviePrimaryKeyIndex idx = new MoviePrimaryKeyIndex(consumer,  id );  This index can be held in memory and then used in conjunction with the generated Hollow API to retrieve Movie records by id:  Movie movie = idx.findMatch(2);\nif(movie != null)\n    System.out.println( Found Movie:   + movie.getTitle().getValue());  Which outputs:  Found Movie: Beasts of No Nation  In our generated API, each type in our data model has a generated index class.  We can index by any field, or multiple fields.   Reuse Indexes  Retrieval from an index is extremely cheap, and indexing is (relatively) expensive.  You should create your indexes when the  HollowConsumer  is initialized and share them thereafter.  Indexes will automatically stay up-to-date with the  HollowConsumer .    Thread Safety  Retrievals from Hollow indexes are thread-safe.  They are safe to use across multiple threads, and it is safe to query while a transition is in progress.   We've just begun to scratch the surface of what indexes can do.  See  Indexing/Querying  for an in-depth exploration of this topic.", 
            "title": "Indexing Data for Retrieval"
        }, 
        {
            "location": "/getting-started/#hierarchical-data-models", 
            "text": "Our data models can be much richer than in the prior example.  Assume an updated  Movie  class:  public class Movie {\n    long id;\n    String title;\n    int releaseYear;\n    List Actor  actors;\n\n    public Movie(long id, String title, int year, List Actor  actors) {\n        this.id = id;\n        this.title = title;\n        this.releaseYear = year;\n        this.actors = actors;\n    }\n}  Which references  Actor  records:  public class Actor {\n    long actorId;\n    String actorName;\n\n    public Actor(long actorId, String actorName) {\n        this.actorId = actorId;\n        this.actorName = actorName;\n    }\n}  Some records are added to a  HollowProducer :  List Movie  movies = Arrays.asList(\n        new Movie(1,  The Matrix , 1999, Arrays.asList(\n                new Actor(101,  Keanu Reeves ),\n                new Actor(102,  Laurence Fishburne ),\n                new Actor(103,  Carrie-Ann Moss ),\n                new Actor(104,  Hugo Weaving )\n        )),\n        new Movie(6,  Event Horizon , 1997, Arrays.asList(\n                new Actor(102,  Laurence Fishburne ),\n                new Actor(105,  Sam Neill )\n        ))\n);\n\nproducer.runCycle(state -  {\n    for(Movie movie : movies)\n        state.addObject(movie);\n});  When we add these movies to the dataset, Hollow will traverse everything referenced by the provided records and add them to the state as well.  Consequently, both a type  Movie  and a type  Actor  will exist in the data model after the above code runs.     Deduplication  Laurence Fishburne starred in both of these films.  Rather than creating two  Actor  records for Mr. Fishburne, a single record will be created and assigned to both of our  Movie  records.  This  deduplication  happens automatically by virtue of having the exact same data contained in both Actor inputs.   Consumers of this dataset may want to also create an index for  Actor  records.  For example:  ActorPrimaryKeyIndex actorIdx = new ActorPrimaryKeyIndex(consumer,  actorId );\n\nActor actor = actorIdx.getMatchingOrdinal(102);\nif(actor != null)\n    System.out.println( Found Actor:   + actor.getActorName().getValue());  Outputs:  Found Actor: Laurence Fishburne", 
            "title": "Hierarchical Data Models"
        }, 
        {
            "location": "/getting-started/#restoring-at-startup", 
            "text": "From time to time, we need to redeploy our producer.  When we first create a  HollowProducer  and run a cycle it will not be able to produce a delta, because it does not know anything about the prior  data state .  If no action is taken, a new state with only a snapshot will be produced and announced, and clients will load that data state with an operation called a  double snapshot , which has potentially undesirable performance characteristics.    We can remedy this situation by  restoring  our newly created producer with the last announced data state.  For example:  Publisher publisher = ...\nAnnouncer announcer = ...\nBlobRetriever blobRetriever = ...\nAnnouncementWatcher announcementWatcher = ...\n\nHollowProducer producer = HollowProducer.withPublisher(publisher)\n                                        .withAnnouncer(announcer)\n                                        .build();\n\nproducer.initializeDataModel(Movie.class);\n\nlong latestAnnouncedVersion = announcementWatcher.getLatestVersion();\nproducer.restore(latestAnnouncedVersion, blobRetriever);\n\nproducer.runCycle(new Populator() {\n   ...\n});  In the above code, we first  initialize  the data model by providing the set of classes we will add during the cycle.  After that, we  restore  by providing our  BlobRetriever  implementation, along with the version which should be restored.  The  HollowProducer  will will use the  BlobRetriever  to load the desired state, then use it to  restore  itself.  In this way, a delta can be produced at startup, and consumers will not have to load a snapshot to get up-to-date.   Initializing the data model  Before  restoring , we must always  initialize  our data model.  When a data model changes between deployments, Hollow will automatically merge records of types which have changed.  In order to do this correctly, Hollow needs to know about the current data model before the restore operation begins.", 
            "title": "Restoring at Startup"
        }, 
        {
            "location": "/indexing-querying/", 
            "text": "A Data Model\n\n\nFor the purposes of these examples, let's imagine we have a data model defined by the following Objects:\n\n\n@HollowPrimaryKey(fields=\nid\n)\npublic class Movie {\n    int id;\n    String title;\n\n    @HollowHashKey(fields=\nactor.actorId\n)\n    Set\nActorRole\n cast;\n}\n\npublic class ActorRole {\n    Actor actor;\n    int movieId;\n    String characterName;\n}\n\n@HollowPrimaryKey(fields=\nactorId\n)\npublic class Actor {\n    int actorId;\n    String name;\n}\n\n\n\n\nPrimary Key Indexes\n\n\nWhen we \ngenerate a client API\n, each type in our data model gets a custom index class called \ntypename\nPrimaryKeyIndex\n.  We can use these classes to look up records based on \nprimary key\n values.\n\n\nDefault Primary Keys\n\n\nOnce we have loaded a dataset into a \nHollowConsumer\n, we can use the \nMovie\n index to retrieve data by the default primary key \nid\n:\n\n\n\nHollowConsumer consumer = ...;\n\nMoviePrimaryKeyIndex idx = new MoviePrimaryKeyIndex(consumer);\n\nint knownMovieId = ...;\n\nMovie movie = idx.findMatch(knownMovieId);\n\n\n\n\n\nJust as the \nHollowConsumer\n will automatically stay up-to-date as your dataset updates, a primary key index will also stay up-to-date with the \nHollowConsumer\n with which it is backed.\n\n\n\n\nShare Indexes\n\n\nQueries to indexes are thread-safe.  We should create each of the indexes we need only once, and share them everywhere they are needed.\n\n\n\n\nConsumer-specified Primary Keys\n\n\nIn the prior example, our primary key index was using the default primary key defined in the data model.  A primary key index is not restricted to just default primary keys.  For example, we could \nalso\n index movies by their title:\n\n\nMoviePrimaryKeyIndex idx = new MoviePrimaryKeyIndex(consumer, \ntitle\n);\n\nString knownMovieTitle = ...;\n\nMovie movie = idx.findMatch(knownMovieTitle);\n\n\n\n\n\n\n\nPrimary Keys\n\n\nA primary key index should be used when there is a one-to-one mapping between records and key values.  A primary key can only return one record per key, and if multiple records exist for a given key, then an arbitrary match will be returned.\n\n\n\n\nCompound Primary Keys\n\n\nA primary key index may also be specified over multiple fields.  For example, we can define a primary key index for the \nActorRole\n type above:\n\n\nActorRolePrimaryKeyIndex idx = new ActorRolePrimaryKeyIndex(consumer, \nactor.id\n, \nmovieId\n);\n\nint knownActorId = ...;\nint knownMovieId = ...;\n\nActorRole actorRoleInMovie = idx.findMatch(knownActorId, knownMovieId);\n\n\n\n\nIn the above example, we are looking for the actor role which matches \nboth\n the actor ID and the movie ID.  Note that the actor id was specified with dot-notation as \nactor.id\n.  This is a \nfield path\n, and indicates that the actual value we're indexing belongs to a \nreferenced\n record.  Note that for a primary key index, we can only traverse through referenced \nObject\n records, not \nList\n, \nSet\n, or \nMap\n records.  We'll cover more about field paths \na bit further down\n.\n\n\nHash Indexes\n\n\nIf we want to find records based on keys for which there is not a one-to-one mapping between records and key values, we want a \nhash index\n.  With our generated client API, we have a single class \nAPI classname\nHashIndex\n.  We can use instances of this class to specify hash indexes.  A hash index must specify each of a \nquery type\n, a \nselect field\n, and one or more \nmatch fields\n.  If we want to \nselect\n the same type we are using to search, we should specify our \nselect field\n as and empty String \n\"\"\n.\n\n\nFor example, if we want to match \nMovie\n records which had characters with some name, we can use the following:\n\n\nMovieAPIHashIndex idx = new MovieAPIHashIndex(\nMovie\n, \n, \ncast.element.characterName.value\n);\n\nString knownCharacterName = ...;\n\nfor(Movie movie : idx.findMovieMatches(knownCharacterName)) {\n    System.out.println(\nThe movie \n + movie.getTitle().getValue() + \n                       \n has a character named \n + knownCharacterName);\n}\n\n\n\n\nAbove, we are \nselecting\n the same type from which our query is derived.  However, if we wanted to find \nActor\n records which starred in \nMovie\n records that have a specific title, we need to formulate our query at the \nMovie\n level, but we are \nselecting\n a different node:\n\n\nMovieAPIHashIndex idx = new MovieAPIHashIndex(\nMovie\n, \ncast.element.actor\n, \ntitle.value\n);\n\nString knownMovieTitle = ...;\n\nfor(Actor actor : idx.findActorMatches(knownMovieTitle)) {\n    System.out.println(\nThe actor \n + actor.getName().getValue() +\n                       \n starred in \n + knownMovieTitle);\n}\n\n\n\n\nWe can also match at multiple places in a type hierarchy.  For example, if we want to find the \nActorRole\n by actor id and movie title, we can use the following:\n\n\nMovieAPIHashIndex idx = new MovieAPIHashIndex(\nMovie\n, \ncast.element\n, \n                                              \ncast.element.actor.actorId\n, \ntitle.value\n);\n\nString knownMovieTitle = ...;\nint knownActorId = ...;\n\nfor(ActorRole role : idx.findActorRoleMatches(knownActorId, knownMovieTitle)) {\n    System.out.println(\nThe actor \n + role.getActor().getName().getValue() +\n                       \n starred in \n + knownMovieTitle + \n                       \n as \n + role.getCharacterName().getValue());\n}\n\n\n\n\n\nField Paths\n\n\nA field path indicates how to traverse through a type hierarchy. It contains multiple parts delimited by \n.\n, and we need one part per type through which we're traversing. Each part corresponding to an \nOBJECT\n type should be equal to the name of a field in that type. \n\n\nPrimary key field paths may only span through \nOBJECT\n types.  These field paths will be automatically expanded if they end in a \nREFERENCE\n field which points to a type that has only a single field, or a type which has a primary key with only a single field defined.  If auto-expansion is not desired, the field path should terminate with a \n!\n character.  For example, in our data model example above, the following field paths for the type \nMovie\n are equivalent: \ntitle\n, \ntitle.value\n.  If we actually want the field path to terminate at the \nREFERENCE\n field \ntitle\n, we can specify the field path as \ntitle!\n.\n\n\nHash key field paths may span through any type.  Each part corresponding to a \nLIST\n or \nSET\n type should be specified as \nelement\n. Similarly, each part corresponding to a \nMAP\n type should be specified as either \nkey\n or \nvalue\n.  Hash key field paths are never auto-expanded.\n\n\nHash Keys\n\n\nNotice that in the POJOs of our data model defined at the beginning of this topic, we annotated the \nSet\nActorRole\n in the \nMovie\n type with \n@HollowHashKey(fields=\"actor.actorId\")\n.  This means that for each of these sets, the data will be hashed by the actor ID in the contained record.  In our generated API, we can easily find any record by actor ID using the \nfindElement()\n method.  For example:\n\n\nMovie movie = ...;\nint knownActorId = ...;\n\nActorRole role = movie.getCast().findElement(knownActorId);\n\n\n\n\nIn this way, each of our set records can be indexed by any field, or combination of fields, for O(1) retrieval of contained records.  The rules for defining a hash key are similar to the rules for defining a primary key:\n\n\n\n\nCompound hash keys may be defined by specifying multiple fields.\n\n\nField paths may only span through \nOBJECT\n types.\n\n\nField paths will be auto-expanded if they terminate in a \nREFERENCE\n field.\n\n\nShould be used when there is a one-to-one mapping between records and keys \nper set\n.  If duplicates exist, an arbitrary valid match will be returned.\n\n\n\n\nIf defined on a \nset\n type, hash key field paths should be defined starting from the element type.\n\n\nHash keys may also be defined on \nmap\n record types.  When defined on a \nmap\n record, the field paths should be defined starting from the key type.  The methods \nfindKey()\n, \nfindValue()\n, and \nfindElement()\n are available on \nmap\n types in the generated API for consumers to look up records by hash key values.  \n\n\nIf using the \nHollowObjectMapper\n, unspecified hash keys will be automatically selected if an element or key type contain a single non-reference field.  Addionally, if a \nSet\n or \nMap\n references \nObject\n elements with a defined \nprimary key\n, then the \nhash key\n will default to the \nprimary key\n of the element type.  Alternatively, \nhash keys\n can be explicitly defined using the \n@HollowHashKey\n annotation in POJOs for \nSet\n schemas by specifying one or more fields from the element type, or for \nMap\n schemas by specifying one or more fields from the key type.  See our \ndata model example\n at the beginning of this section for an example.\n\n\nField Match Scan Queries\n\n\nEach of the examples above pre-index your dataset to achieve O(1) lookup times.  These are very efficient, but require pre-knowledge of what you're searching for. Given that all of hollow datasets exist in memory, for some use cases it is reasonable to scan through the entire dataset looking for matches. \n\n\nThe \nHollowFieldMatchQuery\n can be used to accommodate these use cases.  The Hollow Explorer UI, for example, uses this mechanism to provide a powerful \"search-for-anything\" capability with reasonable response times for low-volume query traffic.\n\n\nDiving Deeper\n\n\nLower-level interfaces are available to index data in the absence of a generated API.  See \nDiving Deeper: Indexing Data for Retrieval\n for a detailed look.", 
            "title": "Indexing/Querying"
        }, 
        {
            "location": "/indexing-querying/#a-data-model", 
            "text": "For the purposes of these examples, let's imagine we have a data model defined by the following Objects:  @HollowPrimaryKey(fields= id )\npublic class Movie {\n    int id;\n    String title;\n\n    @HollowHashKey(fields= actor.actorId )\n    Set ActorRole  cast;\n}\n\npublic class ActorRole {\n    Actor actor;\n    int movieId;\n    String characterName;\n}\n\n@HollowPrimaryKey(fields= actorId )\npublic class Actor {\n    int actorId;\n    String name;\n}", 
            "title": "A Data Model"
        }, 
        {
            "location": "/indexing-querying/#primary-key-indexes", 
            "text": "When we  generate a client API , each type in our data model gets a custom index class called  typename PrimaryKeyIndex .  We can use these classes to look up records based on  primary key  values.", 
            "title": "Primary Key Indexes"
        }, 
        {
            "location": "/indexing-querying/#default-primary-keys", 
            "text": "Once we have loaded a dataset into a  HollowConsumer , we can use the  Movie  index to retrieve data by the default primary key  id :  \nHollowConsumer consumer = ...;\n\nMoviePrimaryKeyIndex idx = new MoviePrimaryKeyIndex(consumer);\n\nint knownMovieId = ...;\n\nMovie movie = idx.findMatch(knownMovieId);  Just as the  HollowConsumer  will automatically stay up-to-date as your dataset updates, a primary key index will also stay up-to-date with the  HollowConsumer  with which it is backed.   Share Indexes  Queries to indexes are thread-safe.  We should create each of the indexes we need only once, and share them everywhere they are needed.", 
            "title": "Default Primary Keys"
        }, 
        {
            "location": "/indexing-querying/#consumer-specified-primary-keys", 
            "text": "In the prior example, our primary key index was using the default primary key defined in the data model.  A primary key index is not restricted to just default primary keys.  For example, we could  also  index movies by their title:  MoviePrimaryKeyIndex idx = new MoviePrimaryKeyIndex(consumer,  title );\n\nString knownMovieTitle = ...;\n\nMovie movie = idx.findMatch(knownMovieTitle);   Primary Keys  A primary key index should be used when there is a one-to-one mapping between records and key values.  A primary key can only return one record per key, and if multiple records exist for a given key, then an arbitrary match will be returned.", 
            "title": "Consumer-specified Primary Keys"
        }, 
        {
            "location": "/indexing-querying/#compound-primary-keys", 
            "text": "A primary key index may also be specified over multiple fields.  For example, we can define a primary key index for the  ActorRole  type above:  ActorRolePrimaryKeyIndex idx = new ActorRolePrimaryKeyIndex(consumer,  actor.id ,  movieId );\n\nint knownActorId = ...;\nint knownMovieId = ...;\n\nActorRole actorRoleInMovie = idx.findMatch(knownActorId, knownMovieId);  In the above example, we are looking for the actor role which matches  both  the actor ID and the movie ID.  Note that the actor id was specified with dot-notation as  actor.id .  This is a  field path , and indicates that the actual value we're indexing belongs to a  referenced  record.  Note that for a primary key index, we can only traverse through referenced  Object  records, not  List ,  Set , or  Map  records.  We'll cover more about field paths  a bit further down .", 
            "title": "Compound Primary Keys"
        }, 
        {
            "location": "/indexing-querying/#hash-indexes", 
            "text": "If we want to find records based on keys for which there is not a one-to-one mapping between records and key values, we want a  hash index .  With our generated client API, we have a single class  API classname HashIndex .  We can use instances of this class to specify hash indexes.  A hash index must specify each of a  query type , a  select field , and one or more  match fields .  If we want to  select  the same type we are using to search, we should specify our  select field  as and empty String  \"\" .  For example, if we want to match  Movie  records which had characters with some name, we can use the following:  MovieAPIHashIndex idx = new MovieAPIHashIndex( Movie ,  ,  cast.element.characterName.value );\n\nString knownCharacterName = ...;\n\nfor(Movie movie : idx.findMovieMatches(knownCharacterName)) {\n    System.out.println( The movie   + movie.getTitle().getValue() + \n                         has a character named   + knownCharacterName);\n}  Above, we are  selecting  the same type from which our query is derived.  However, if we wanted to find  Actor  records which starred in  Movie  records that have a specific title, we need to formulate our query at the  Movie  level, but we are  selecting  a different node:  MovieAPIHashIndex idx = new MovieAPIHashIndex( Movie ,  cast.element.actor ,  title.value );\n\nString knownMovieTitle = ...;\n\nfor(Actor actor : idx.findActorMatches(knownMovieTitle)) {\n    System.out.println( The actor   + actor.getName().getValue() +\n                         starred in   + knownMovieTitle);\n}  We can also match at multiple places in a type hierarchy.  For example, if we want to find the  ActorRole  by actor id and movie title, we can use the following:  MovieAPIHashIndex idx = new MovieAPIHashIndex( Movie ,  cast.element , \n                                               cast.element.actor.actorId ,  title.value );\n\nString knownMovieTitle = ...;\nint knownActorId = ...;\n\nfor(ActorRole role : idx.findActorRoleMatches(knownActorId, knownMovieTitle)) {\n    System.out.println( The actor   + role.getActor().getName().getValue() +\n                         starred in   + knownMovieTitle + \n                         as   + role.getCharacterName().getValue());\n}", 
            "title": "Hash Indexes"
        }, 
        {
            "location": "/indexing-querying/#field-paths", 
            "text": "A field path indicates how to traverse through a type hierarchy. It contains multiple parts delimited by  . , and we need one part per type through which we're traversing. Each part corresponding to an  OBJECT  type should be equal to the name of a field in that type.   Primary key field paths may only span through  OBJECT  types.  These field paths will be automatically expanded if they end in a  REFERENCE  field which points to a type that has only a single field, or a type which has a primary key with only a single field defined.  If auto-expansion is not desired, the field path should terminate with a  !  character.  For example, in our data model example above, the following field paths for the type  Movie  are equivalent:  title ,  title.value .  If we actually want the field path to terminate at the  REFERENCE  field  title , we can specify the field path as  title! .  Hash key field paths may span through any type.  Each part corresponding to a  LIST  or  SET  type should be specified as  element . Similarly, each part corresponding to a  MAP  type should be specified as either  key  or  value .  Hash key field paths are never auto-expanded.", 
            "title": "Field Paths"
        }, 
        {
            "location": "/indexing-querying/#hash-keys", 
            "text": "Notice that in the POJOs of our data model defined at the beginning of this topic, we annotated the  Set ActorRole  in the  Movie  type with  @HollowHashKey(fields=\"actor.actorId\") .  This means that for each of these sets, the data will be hashed by the actor ID in the contained record.  In our generated API, we can easily find any record by actor ID using the  findElement()  method.  For example:  Movie movie = ...;\nint knownActorId = ...;\n\nActorRole role = movie.getCast().findElement(knownActorId);  In this way, each of our set records can be indexed by any field, or combination of fields, for O(1) retrieval of contained records.  The rules for defining a hash key are similar to the rules for defining a primary key:   Compound hash keys may be defined by specifying multiple fields.  Field paths may only span through  OBJECT  types.  Field paths will be auto-expanded if they terminate in a  REFERENCE  field.  Should be used when there is a one-to-one mapping between records and keys  per set .  If duplicates exist, an arbitrary valid match will be returned.   If defined on a  set  type, hash key field paths should be defined starting from the element type.  Hash keys may also be defined on  map  record types.  When defined on a  map  record, the field paths should be defined starting from the key type.  The methods  findKey() ,  findValue() , and  findElement()  are available on  map  types in the generated API for consumers to look up records by hash key values.    If using the  HollowObjectMapper , unspecified hash keys will be automatically selected if an element or key type contain a single non-reference field.  Addionally, if a  Set  or  Map  references  Object  elements with a defined  primary key , then the  hash key  will default to the  primary key  of the element type.  Alternatively,  hash keys  can be explicitly defined using the  @HollowHashKey  annotation in POJOs for  Set  schemas by specifying one or more fields from the element type, or for  Map  schemas by specifying one or more fields from the key type.  See our  data model example  at the beginning of this section for an example.", 
            "title": "Hash Keys"
        }, 
        {
            "location": "/indexing-querying/#field-match-scan-queries", 
            "text": "Each of the examples above pre-index your dataset to achieve O(1) lookup times.  These are very efficient, but require pre-knowledge of what you're searching for. Given that all of hollow datasets exist in memory, for some use cases it is reasonable to scan through the entire dataset looking for matches.   The  HollowFieldMatchQuery  can be used to accommodate these use cases.  The Hollow Explorer UI, for example, uses this mechanism to provide a powerful \"search-for-anything\" capability with reasonable response times for low-volume query traffic.", 
            "title": "Field Match Scan Queries"
        }, 
        {
            "location": "/indexing-querying/#diving-deeper", 
            "text": "Lower-level interfaces are available to index data in the absence of a generated API.  See  Diving Deeper: Indexing Data for Retrieval  for a detailed look.", 
            "title": "Diving Deeper"
        }, 
        {
            "location": "/producer-consumer/", 
            "text": "Infrastructure Integration\n\n\nIn order to wield Hollow effectively for your organization, you need only implement four interfaces to integrate with your infrastructure:\n\n\n\n\nA \nPublisher\n for the \nHollowProducer\n\n\nAn \nAnnouncer\n for the \nHollowProducer\n\n\nA \nBlobRetriever\n for the \nHollowConsumer\n\n\nAn \nAnnouncementWatcher\n for the \nHollowConsumer\n\n\n\n\nOnce you've implemented these four interfaces, Hollow can be used in many different contexts in your organization.  You'll never again have to write code to ship json or csv data from one machine to another -- and better yet, you'll gain visibility and insights into previously opaque and hard-to-debug datasets.\n\n\n\n\nThink local first\n\n\nThe following sections describe how to plug Hollow into your infrastructure.  Now is the time to think about how you will debug your data later.  Consider making your implementations of these interfaces easily allow for (securely!) retrieving data from any environment, including production, right down to your local development box.\n\n\nIf you take this step, you'll be giving yourself immense power to glean insight into your data and debug production issues.  Imagine it's 10am, and you suspect some issue surrounding some particular data was present at 4am this morning.  You can open up Eclipse or IntelliJ and write a main method which -- with a few lines of code -- pulls down the data \nexactly\n as it existed on your production instances at that time.  You can write code against it to explore specific scenarios or feed it into the \nexplorer\n to confirm or deny your suspicion in seconds.\n\n\n\n\nStoring the Blobs\n\n\nBlobs are published to a file store which is accessible by consumers.  From this blob store, consumers must be able to query for and retrieve blobs in the following ways:\n\n\n\n\nSnapshots\n: Must be queryable based on the state identifier.  If a blob store is queried for a snapshot with an identifier which does not exist, the snapshot with the greatest identifier prior to the queried identifier should be retrieved.\n\n\nDeltas\n: Must be queryable based on the state identifier to which a delta should be applied (e.g. the delta's \nfrom\n state identifier).\n\n\nReverse Deltas\n: Must be queryable based on the state identifier to which a reverse delta should be applied (e.g. the reverse delta's \nfrom\n state identifier).\n\n\n\n\nThe \nPublisher\n and \nBlobRetriever\n are opposite sides of your blob store (writer and reader, respectively).\n\n\nYour \nPublisher\n implementation must only define a single method:\n\n\n    public void publish(HollowProducer.Blob blob);\n\n\n\n\nThe \nBlob\n passed to your \nPublisher\n should be published somewhere for retrieval by consumers.  The blob's data is retrieved for publish by calling either \nnewInputStream()\n or \ngetFile()\n.  The blob will be one of either a \nsnapshot\n, \ndelta\n, or \nreverse delta\n -- the type can be determined by calling \ngetType()\n.  The blob should be indexed for later retrieval as indicated above -- \nsnapshots\n by the result of \ngetToVersion()\n, and \ndeltas\n/\nreversedeltas\n by the result of \ngetFromVersion()\n.  Note that you will need to be able to distinguish between a \nsnapshot\n, \ndelta\n, and \nreversedelta\n with the same version number.  \n\n\n\n\nChoosing a blob store\n\n\nYou can publish blobs anywhere -- S3, an FTP server, an NFS, etc -- so long as that selected blob store can scale to the necessary volume of concurrent consumer requests.  \n\n\nNote that if your announcement mechanism is instantaneous all consumers will attempt to retrieve the blob files simultaneously.\n\n\n\n\n\n\nBlobs must be overwriteable\n\n\nYour \nPublisher\n implementation must allow blobs to be overwritten.  If an attempt is made to publish a blob with to be indexed by a state identifier for which a corresponding artifact already exists, it must \noverwrite\n the existing artifact previously published.  This happens routinely -- for example if a data state fails after publishing for any reason (e.g. validation fails), then the producer will automatically roll back the state and a delta will be re-published with the same \nfrom\n version.\n\n\n\n\nThe \nBlobRetriever\n is the other side of the blob store equation.  Your implementation must define three methods:\n\n\n    public HollowConsumer.Blob retrieveSnapshotBlob(long desiredVersion);\n\n    public HollowConsumer.Blob retrieveDeltaBlob(long currentVersion);\n\n    public HollowConsumer.Blob retrieveReverseDeltaBlob(long currentVersion);\n\n\n\n\nThe \nBlob\n you return will be a custom implementation for your blob store which extends \nHollowConsumer.Blob\n and implements the \ngetInputStream()\n method.  \n\n\nYour \nretrieveSnapshotBlob(long desiredVersion)\n implementation should return the blob which exactly matches the specified \ndesiredVersion\n if it exists.  If no such version exists then the greatest available version which is \nless than\n the specified \ndesiredVersion\n should be returned.  If no such match exists, return null.\n\n\nYour \nretrieveDeltaBlob(long currentVersion)\n and \nretrieveReverseDeltaBlob(long currentVersion)\n implementations should each return the blob which exactly matches the specified \ncurrentVersion\n.  If no such match exists, return null.\n\n\n\n\nScanning for snapshots\n\n\nIf an exact match for the requested snapshot doesn't exist, you'll need to scan the available versions for the closest match prior to the requested.  For this reason, if you have a large number of consumers, it makes sense to \nindex\n your available snapshot versions so this operation is fast.\n\n\n\n\nAnnouncing the State\n\n\nOnce the necessary transitions to bring clients up to date have been written to the blob store, the availability of the state must be \nannounced\n to clients.  This simply means that a centralized location must be maintained and updated by the producer which indicates the version of the currently available state.  \n\n\nWhen this announced state is updated, usually it is desirable to have consumers realize this update as quickly as possible.  This can be accomplished either via a push notification to all consumers, or via frequent polling by consumers.\n\n\nThe \nAnnouncer\n and \nAnnouncementWatcher\n are opposite sides of your announcement mechanism (writer and reader, respectively).\n\n\nYour \nAnnouncer\n implementation must only implement a single method:\n\n\n    public void announce(long stateVersion);\n\n\n\n\nThe \nstateVersion\n passed to your \nAnnouncer\n should be immediately communicated to your consumers.  You can use either a 'push' mechanism or a frequent 'polling' mechanism to minimize the time between when a producer announces a version, and all consumers receive that announcement.\n\n\nYour \nAnnouncementWatcher\n implementation must implement two methods:\n\n\n    public long getLatestVersion();\n\n    public void subscribeToUpdates(HollowConsumer consumer);\n\n\n\n\nWhen your \nAnnouncementWatcher\n is initialized, you should immediately set up your selected announcement mechanism -- either subscribe to your push notifications or set up a thread to poll for updates.  \n\n\nImplementations should maintain a list of subscribed \nHollowConsumer\ns, and each time \nsubcribeToUpdates(HollowConsumer consumer)\n is called, you should add the provided \nHollowConsumer\n to your list.  When the announced version changes, call \ntriggerAsyncRefresh()\n on each subscribed consumer.\n\n\nWhether or not any \nHollowConsumer\ns are subscribed, implementations should return the latest announced version each time \ngetLatestVersion()\n is called.\n\n\n\n\nHollowConsumer subscribes itself\n\n\nA \nHollowConsumer\n will automatically call \nsubscribeToUpdates()\n with itself for an \nAnnouncementWatcher\n with which it is initialized.\n\n\n\n\nPinning Consumers\n\n\nMistakes happen.  What's important is that we can recover from them quickly.  If you accidentally publish bad data, you should be able to revert those changes quickly.  If you give your \nAnnouncementWatcher\n implementation an alternate location to read the announcement from, which \noverrides\n the announcement from the consumer, then you can use this to quickly force clients to go back to any arbitrary state in the past.  We call setting a state version in this alternate location \npinning\n the consumers.\n\n\nImplementing a pinning mechanism is extremely useful and highly recommended.  You can operationally reverse data issues immediately upon discovery, so that symptoms go away while you diagnose exactly what went wrong.  This can save an enormous amount of stress and money.\n\n\n\n\nUnpinning\n\n\nIf you've pinned consumers due to a data issue, it's probably not desirable to simply 'unpin' them after the root cause is addressed.  Instead, restart the producer and instruct it to \nrestore\n from the pinned state.  It should then produce a delta which skips over all of the bad states.  Only unpin after the delta from the pinned version to a bad version is overwritten with a delta from the pinned version to the good version.\n\n\n\n\nBlob Namespaces\n\n\nDifferent use cases within your organization may want to reuse the same infrastructure integration.  You may want your \nBlobRetriever\n and \nAnnouncementWatcher\n to allow for multiple blob \nnamespaces\n, one for each use case.\n\n\nThe Producer/Consumer APIs\n\n\nIn \nGetting Started\n we encountered basic usage of the \nHollowProducer\n and \nHollowConsumer\n APIs.  This basic usage implies some default behavior which, if desired, may be customized to better suit your purposes.  A more in-depth exploration of the available customizable features of these APIs follows.\n\n\nThe HollowProducer\n\n\nGenerally, a producer runs a repeating \ncycle\n.  At the end of each cycle, the producer has created a \ndata state\n, published the artifacts necessary for consumers to bring their in-memory data stores to that \ndata state\n, and announced the availability of the \nstate\n.\n\n\nThe \nHollowProducer\n encapsulates the details of publishing, announcing, validating, and (if necessary) rollback of data states.  In order to accomplish this, a few infrastructure hooks should be injected:\n\n\nHollowProducer\n   .withPublisher(publisher)         /// required: a BlobPublisher\n   .withAnnouncer(announcer)         /// optional: an Announcer\n   .withValidators(validators)       /// optional: one or more Validator\n   .withListeners(listeners)         /// optional: one or more HollowProducerListeners\n   .withBlobStagingDir(dir)          /// optional: a java.io.File\n   .withBlobCompressor(compressor)   /// optional: a BlobCompressor\n   .withBlobStager(stager)           /// optional: a BlobStager\n   .withSnapshotPublishExecutor(e)   /// optional: a java.util.concurrent.Executor\n   .withNumStatesBetweenSnapshots(n) /// optional: an int\n   .withTargetMaxTypeShardSize(size) /// optional: a long\n\n\n\n\nLet's examine each of the injected configurations into the \nHollowProducer\n:\n\n\n\n\nBlobPublisher\n: Implementations of this class define how to publish blob data to the blob store.\n\n\nAnnouncer\n: Implementations of this class define the announcement mechanism, which is used to track the version of the currently announced state.\n\n\nValidator\n: Implementations of this class allow for semantic validation of the data contained in a state prior to announcement.  If an Exception is thrown during validation, the state will not be announced, and the producer will be automatically rolled back to the prior state.\n\n\nHollowProducerListener\n: Listeners are notified about the progress and status of producer cycles throughout the various cycle stages.\n\n\nBlob staging directory\n: Before blobs are published, they must be written and inspected/validated.  A directory may be specified as a File to which these \"staged\" blobs will be written prior to publish.  Staged blobs will be cleaned up automatically after publish.\n\n\nBlobCompressor\n: Implementations of this class intercept blob input/output streams to allow for compression in the blob store.\n\n\nBlobStager\n: Implementations will define how to stage blobs, if the default behavior of staging blobs on local disk is not desirable.  If a custom \nBlobStager\n is provided, then neither a blob staging directory or \nBlobCompressor\n should be provided.\n\n\nSnapshot publish\n \nExecutor\n: When consumers start up, if the latest announced version does not have a snapshot, they can load an earlier snapshot and follow deltas to get up-to-date.  A state can therefore be available and announced prior to the availability of the snapshot.  If an Executor is supplied here, then it will be used to publish snapshots.  This can be useful if snapshot publishing takes a long time -- subsequent cycles may proceed while snapshot uploads are still in progress.\n\n\nNumber of cycles between snapshots\n: Because snapshots are not necessary for a data state to be available and announced, they need not be published every cycle.  If this parameter is specified, then a snapshot will be produced only every \n(n+1)th\n cycle.\n\n\nVersionMinter\n: Allows for a custom version identifier minting strategy.\n\n\nTarget max type shard size\n: Specify a \ntarget max type shard size\n.  Defaults to 16MB.\n\n\n\n\nEach time a new \ndata state\n should be produced, users should call \n.runCycle(Populator)\n.  See \nGetting Started\n for more basic usage details.\n\n\nRestoring At Startup\n\n\nIdeally the same \nHollowProducer\n would be held in memory forever, and \nrunCycle()\n would be called every so often to produce a never-ending intact \ndelta chain\n.  However, this isn\u2019t always possible; the producer will need to be restarted from time to time due to deployment or other operational circumstances.\n\n\nIn order to produce a delta between states produced by one \nHollowProducer\n and another, the producer can \nrestore\n the prior state upon restart, which will allow a delta and reverse delta to be produced.  See \nRestoring at Startup\n for usage.\n\n\nOnce we have \nrestored\n the prior state, we can produce a delta from our producer's first cycle.  The delta will be applicable to any consumers which are on the state from which we restored.  \n\n\n\n\nInitializing Before Restore\n\n\nBefore \nrestoring\n, we must always \ninitialize\n our data model.  A \nHollowProducer\n's data model may be initialized:\n\n\n\n\nvia the \nHollowObjectMapper\n by calling \ninitTypeState()\n with all top-level classes\n\n\nvia a set of schemas \nloaded from a text file\n using the \nHollowSchemaParser\n and \nHollowWriteStateCreator\n\n\n\n\n\n\n\n\nTruncating a Delta Chain\n\n\nIf a problem occurs and you need to \npin back\n consumers, you \nmay\n want to restart your producer and explicitly restore from the pinned state.  Once the producer's first cycle completes, it will publish a delta from the pinned state to the newly produced state, \noverwriting\n the previous delta from the pinned state.  In this way, when you unpin, consumers will automatically follow the \nnew\n delta, and the old forward-path from the pinned state will be \ntruncated\n.\n\n\nIf any consumers somehow did happen to remain on a \ntruncated\n state, the reverse delta out of the truncated chain is still intact -- they could be manually pinned back to the restored state, then unpinned to get back up-to-date.\n\n\n\n\nRolling Back\n\n\nWhile producing a new state, if the \nHollowProducer\n encounters an error during data state population or validation fails, the current \ndata state\n will be aborted and the underlying \nstate engine\n will be rolled back to the previous data state.  Any delta produced on the next cycle will be from the last \nsuccessful\n data state.\n\n\nValidating Data\n\n\nIt likely makes sense to perform some basic \nvalidation\n on your produced data states before announcing them to clients.  If you provide one or more \nValidator\ns to the \nHollowProducer\n, these will be automatically executed prior to announcement.  Validation rules will be specific to the semantics of the dataset, and may include some heuristics-based metrics based on expectations about the dataset.  If your \nValidator\n throws an Exception, the \nHollowProducer\n will automatically roll back the state engine and the \nnext successful\n cycle will produce a delta from the prior successful state.  \n\n\nCompacting Data\n\n\nIt is possible to produce delta chains which extend over many thousands of states.  If during this delta chain an especially large delta happens for a specific type, it\u2019s possible that many ordinal holes will be present in that type.  If over time multiple types go through especially large deltas, this can have an impact on a dataset\u2019s heap footprint.\n\n\nTo reclaim heap space occupied by ordinal holes, a special \ncompaction cycle\n can be run on the \nHollowProducer\n.  During compaction, no record data will change, but identical records will be relocated off of the high end of the ordinal space into the ordinal holes.  This is accomplished by producing a new data state with no changes except for the more optimal ordinal assignments.\n\n\nTo run a \ncompaction cycle\n, call \nrunCompactionCycle(config)\n on the \nHollowProducer\n.  If this method returns a valid version identifier, then a compaction cycle occurred and produced a new data state.  If it returns \nLong.MIN_VALUE\n, then the compaction criteria specified in the \nCompactionConfig\n was not met and no action was taken.  See the \nHollowCompactor\n javadoc for more details.\n\n\nThe HollowConsumer\n\n\nData consumers keep their local copy of a dataset current by ensuring that their state engine is always at the latest \nannounced\n data state. Consumers can arrive at a particular data state in a couple of different ways:\n\n\n\n\nAt initialization time, they will load a snapshot, which is an entire copy of the dataset to be forklifted into memory.\n\n\nAfter initialization time, they will keep their local copy of the dataset current by applying delta transitions, which are the differences between adjacent data states.\n\n\n\n\nThe \nHollowConsumer\n encapsulates the details of initializing and keeping a dataset up to date.  In order to accomplish this task, a few infrastructure hooks should be injected:\n\n\nHollowConsumer\n   .withBlobRetriever(blobRetriever)              /// required: a BlobRetriever\n   .withLocalBlobStore(localDiskDir)              /// optional: a local disk location\n   .withAnnouncementWatcher(announcementWatcher)  /// optional: a AnnouncementWatcher\n   .withRefreshListener(refreshListener)          /// optional: a RefreshListener\n   .withGeneratedAPIClass(MyGeneratedAPI.class)   /// optional: a generated client API class\n   .withFilterConfig(filterConfig)                /// optional: a HollowFilterConfig\n   .withDoubleSnapshotConfig(doubleSnapshotCfg)   /// optional: a DoubleSnapshotConfig\n   .withObjectLongevityConfig(objectLongevityCfg) /// optional: an ObjectLongevityConfig\n   .withObjectLongevityDetector(detector)         /// optional: an ObjectLongevityDetector\n   .withRefreshExecutor(refreshExecutor)          /// optional: an Executor\n   .build();\n\n\n\n\nLet's examine each the injected hooks to the \nHollowConsumer\n:\n\n\n\n\nBlobRetriever\n: The interface to the blob store.  This is the only hook for which a custom implementation is required.  Each of the other hooks have default implementations which may be used.  The \nBlobRetriever\n may be omitted only if a previously-populated local blob store is specified.\n\n\nLocal blob store\n: A \nFile\n which indicates where to record downloaded blobs and find previously downloaded blobs.  If specified along with a \nBlobRetriever\n, the \nHollowConsumer\n will prefer to use previously downloaded blobs where applicable, and otherwise write newly downloaded blobs to the specified directory.  If specified \nwithout\n a \nBlobRetriever\n, only previously downloaded blobs will be available. \n\n\nAnnouncementWatcher\n: Provides an interface to the state announcement mechanism.  Often, announcement polling logic is encapsulated inside implementations.\n\n\nRefreshListener\n: Provides hooks so that actions may be taken during and after updates (e.g. indexing).\n\n\nGenerated API Class\n: Specifies a \ncustom-generated Hollow API\n to use.\n\n\nHollowFilterConfig\n: \n\n\nDoubleSnapshotConfig\n: Defines advanced settings related to \ndouble snapshots\n.\n\n\nObjectLongevityConfig\n: Defines advanced settings related to \nobject longevity\n.\n\n\nObjectLongevityDetector\n: Implementations are notified when stale hollow object existence and usage is detected.\n\n\nRefreshExecutor\n: An \nExecutor\n to use when asynchronous updates are called via \ntriggerAsyncRefresh()\n.\n\n\n\n\nEach time the identifier of the currently announced state changes, \ntriggerRefresh()\n should be called on the \nHollowConsumer\n.  This will bring the data up to date.\n\n\nIn general, the only requirement for getting Hollow consumers to work with your specific infrastructure is to implement a \nBlobRetriever\n and \nAnnouncementWatcher\n, and use them with a \nHollowConsumer\n.\n\n\n\n\nTriggering Refresh\n\n\nWhen implementing a \nAnnouncementWatcher\n, you will need to implement the method \nsubscribeToUpdates(HollowConsumer consumer)\n.  When you\ncreate a \nHollowConsumer\n with an \nAnnouncementWatcher\n, it will automatically call back to this method with itself as the argument.  \n\n\nYou should track all \nHollowConsumer\ns received by calls to this method.  When your announcement mechanism provides an updated value, \nyou should notify each \nHollowConsumer\n via the \ntriggerAsyncRefresh()\n method.\n\n\nIn this way, your \nHollowConsumer\n injected with this \nHollowAnnouncementWatcher\n implementation will be automatically kept up-to-date.\n\n\n\n\nDataset Consistency\n\n\nIf you have a long-running process which requires a consistent view of the dataset in a single state, you can prevent the \nHollowConsumer\n from updating while your process runs:\n\n\nHollowConsumer consumer = ...\n\nconsumer.getRefreshLock().lock();\ntry {\n    /// run your process\n} finally {\n    consumer.getRefreshLock().unlock();\n}\n\n\n\n\nThe \ngetRefreshLock()\n call returns the read lock in a \nReadWriteLock\n.  Refreshes use the write lock.", 
            "title": "Producers and Consumers"
        }, 
        {
            "location": "/producer-consumer/#infrastructure-integration", 
            "text": "In order to wield Hollow effectively for your organization, you need only implement four interfaces to integrate with your infrastructure:   A  Publisher  for the  HollowProducer  An  Announcer  for the  HollowProducer  A  BlobRetriever  for the  HollowConsumer  An  AnnouncementWatcher  for the  HollowConsumer   Once you've implemented these four interfaces, Hollow can be used in many different contexts in your organization.  You'll never again have to write code to ship json or csv data from one machine to another -- and better yet, you'll gain visibility and insights into previously opaque and hard-to-debug datasets.   Think local first  The following sections describe how to plug Hollow into your infrastructure.  Now is the time to think about how you will debug your data later.  Consider making your implementations of these interfaces easily allow for (securely!) retrieving data from any environment, including production, right down to your local development box.  If you take this step, you'll be giving yourself immense power to glean insight into your data and debug production issues.  Imagine it's 10am, and you suspect some issue surrounding some particular data was present at 4am this morning.  You can open up Eclipse or IntelliJ and write a main method which -- with a few lines of code -- pulls down the data  exactly  as it existed on your production instances at that time.  You can write code against it to explore specific scenarios or feed it into the  explorer  to confirm or deny your suspicion in seconds.", 
            "title": "Infrastructure Integration"
        }, 
        {
            "location": "/producer-consumer/#storing-the-blobs", 
            "text": "Blobs are published to a file store which is accessible by consumers.  From this blob store, consumers must be able to query for and retrieve blobs in the following ways:   Snapshots : Must be queryable based on the state identifier.  If a blob store is queried for a snapshot with an identifier which does not exist, the snapshot with the greatest identifier prior to the queried identifier should be retrieved.  Deltas : Must be queryable based on the state identifier to which a delta should be applied (e.g. the delta's  from  state identifier).  Reverse Deltas : Must be queryable based on the state identifier to which a reverse delta should be applied (e.g. the reverse delta's  from  state identifier).   The  Publisher  and  BlobRetriever  are opposite sides of your blob store (writer and reader, respectively).  Your  Publisher  implementation must only define a single method:      public void publish(HollowProducer.Blob blob);  The  Blob  passed to your  Publisher  should be published somewhere for retrieval by consumers.  The blob's data is retrieved for publish by calling either  newInputStream()  or  getFile() .  The blob will be one of either a  snapshot ,  delta , or  reverse delta  -- the type can be determined by calling  getType() .  The blob should be indexed for later retrieval as indicated above --  snapshots  by the result of  getToVersion() , and  deltas / reversedeltas  by the result of  getFromVersion() .  Note that you will need to be able to distinguish between a  snapshot ,  delta , and  reversedelta  with the same version number.     Choosing a blob store  You can publish blobs anywhere -- S3, an FTP server, an NFS, etc -- so long as that selected blob store can scale to the necessary volume of concurrent consumer requests.    Note that if your announcement mechanism is instantaneous all consumers will attempt to retrieve the blob files simultaneously.    Blobs must be overwriteable  Your  Publisher  implementation must allow blobs to be overwritten.  If an attempt is made to publish a blob with to be indexed by a state identifier for which a corresponding artifact already exists, it must  overwrite  the existing artifact previously published.  This happens routinely -- for example if a data state fails after publishing for any reason (e.g. validation fails), then the producer will automatically roll back the state and a delta will be re-published with the same  from  version.   The  BlobRetriever  is the other side of the blob store equation.  Your implementation must define three methods:      public HollowConsumer.Blob retrieveSnapshotBlob(long desiredVersion);\n\n    public HollowConsumer.Blob retrieveDeltaBlob(long currentVersion);\n\n    public HollowConsumer.Blob retrieveReverseDeltaBlob(long currentVersion);  The  Blob  you return will be a custom implementation for your blob store which extends  HollowConsumer.Blob  and implements the  getInputStream()  method.    Your  retrieveSnapshotBlob(long desiredVersion)  implementation should return the blob which exactly matches the specified  desiredVersion  if it exists.  If no such version exists then the greatest available version which is  less than  the specified  desiredVersion  should be returned.  If no such match exists, return null.  Your  retrieveDeltaBlob(long currentVersion)  and  retrieveReverseDeltaBlob(long currentVersion)  implementations should each return the blob which exactly matches the specified  currentVersion .  If no such match exists, return null.   Scanning for snapshots  If an exact match for the requested snapshot doesn't exist, you'll need to scan the available versions for the closest match prior to the requested.  For this reason, if you have a large number of consumers, it makes sense to  index  your available snapshot versions so this operation is fast.", 
            "title": "Storing the Blobs"
        }, 
        {
            "location": "/producer-consumer/#announcing-the-state", 
            "text": "Once the necessary transitions to bring clients up to date have been written to the blob store, the availability of the state must be  announced  to clients.  This simply means that a centralized location must be maintained and updated by the producer which indicates the version of the currently available state.    When this announced state is updated, usually it is desirable to have consumers realize this update as quickly as possible.  This can be accomplished either via a push notification to all consumers, or via frequent polling by consumers.  The  Announcer  and  AnnouncementWatcher  are opposite sides of your announcement mechanism (writer and reader, respectively).  Your  Announcer  implementation must only implement a single method:      public void announce(long stateVersion);  The  stateVersion  passed to your  Announcer  should be immediately communicated to your consumers.  You can use either a 'push' mechanism or a frequent 'polling' mechanism to minimize the time between when a producer announces a version, and all consumers receive that announcement.  Your  AnnouncementWatcher  implementation must implement two methods:      public long getLatestVersion();\n\n    public void subscribeToUpdates(HollowConsumer consumer);  When your  AnnouncementWatcher  is initialized, you should immediately set up your selected announcement mechanism -- either subscribe to your push notifications or set up a thread to poll for updates.    Implementations should maintain a list of subscribed  HollowConsumer s, and each time  subcribeToUpdates(HollowConsumer consumer)  is called, you should add the provided  HollowConsumer  to your list.  When the announced version changes, call  triggerAsyncRefresh()  on each subscribed consumer.  Whether or not any  HollowConsumer s are subscribed, implementations should return the latest announced version each time  getLatestVersion()  is called.   HollowConsumer subscribes itself  A  HollowConsumer  will automatically call  subscribeToUpdates()  with itself for an  AnnouncementWatcher  with which it is initialized.", 
            "title": "Announcing the State"
        }, 
        {
            "location": "/producer-consumer/#pinning-consumers", 
            "text": "Mistakes happen.  What's important is that we can recover from them quickly.  If you accidentally publish bad data, you should be able to revert those changes quickly.  If you give your  AnnouncementWatcher  implementation an alternate location to read the announcement from, which  overrides  the announcement from the consumer, then you can use this to quickly force clients to go back to any arbitrary state in the past.  We call setting a state version in this alternate location  pinning  the consumers.  Implementing a pinning mechanism is extremely useful and highly recommended.  You can operationally reverse data issues immediately upon discovery, so that symptoms go away while you diagnose exactly what went wrong.  This can save an enormous amount of stress and money.   Unpinning  If you've pinned consumers due to a data issue, it's probably not desirable to simply 'unpin' them after the root cause is addressed.  Instead, restart the producer and instruct it to  restore  from the pinned state.  It should then produce a delta which skips over all of the bad states.  Only unpin after the delta from the pinned version to a bad version is overwritten with a delta from the pinned version to the good version.", 
            "title": "Pinning Consumers"
        }, 
        {
            "location": "/producer-consumer/#blob-namespaces", 
            "text": "Different use cases within your organization may want to reuse the same infrastructure integration.  You may want your  BlobRetriever  and  AnnouncementWatcher  to allow for multiple blob  namespaces , one for each use case.", 
            "title": "Blob Namespaces"
        }, 
        {
            "location": "/producer-consumer/#the-producerconsumer-apis", 
            "text": "In  Getting Started  we encountered basic usage of the  HollowProducer  and  HollowConsumer  APIs.  This basic usage implies some default behavior which, if desired, may be customized to better suit your purposes.  A more in-depth exploration of the available customizable features of these APIs follows.", 
            "title": "The Producer/Consumer APIs"
        }, 
        {
            "location": "/producer-consumer/#the-hollowproducer", 
            "text": "Generally, a producer runs a repeating  cycle .  At the end of each cycle, the producer has created a  data state , published the artifacts necessary for consumers to bring their in-memory data stores to that  data state , and announced the availability of the  state .  The  HollowProducer  encapsulates the details of publishing, announcing, validating, and (if necessary) rollback of data states.  In order to accomplish this, a few infrastructure hooks should be injected:  HollowProducer\n   .withPublisher(publisher)         /// required: a BlobPublisher\n   .withAnnouncer(announcer)         /// optional: an Announcer\n   .withValidators(validators)       /// optional: one or more Validator\n   .withListeners(listeners)         /// optional: one or more HollowProducerListeners\n   .withBlobStagingDir(dir)          /// optional: a java.io.File\n   .withBlobCompressor(compressor)   /// optional: a BlobCompressor\n   .withBlobStager(stager)           /// optional: a BlobStager\n   .withSnapshotPublishExecutor(e)   /// optional: a java.util.concurrent.Executor\n   .withNumStatesBetweenSnapshots(n) /// optional: an int\n   .withTargetMaxTypeShardSize(size) /// optional: a long  Let's examine each of the injected configurations into the  HollowProducer :   BlobPublisher : Implementations of this class define how to publish blob data to the blob store.  Announcer : Implementations of this class define the announcement mechanism, which is used to track the version of the currently announced state.  Validator : Implementations of this class allow for semantic validation of the data contained in a state prior to announcement.  If an Exception is thrown during validation, the state will not be announced, and the producer will be automatically rolled back to the prior state.  HollowProducerListener : Listeners are notified about the progress and status of producer cycles throughout the various cycle stages.  Blob staging directory : Before blobs are published, they must be written and inspected/validated.  A directory may be specified as a File to which these \"staged\" blobs will be written prior to publish.  Staged blobs will be cleaned up automatically after publish.  BlobCompressor : Implementations of this class intercept blob input/output streams to allow for compression in the blob store.  BlobStager : Implementations will define how to stage blobs, if the default behavior of staging blobs on local disk is not desirable.  If a custom  BlobStager  is provided, then neither a blob staging directory or  BlobCompressor  should be provided.  Snapshot publish   Executor : When consumers start up, if the latest announced version does not have a snapshot, they can load an earlier snapshot and follow deltas to get up-to-date.  A state can therefore be available and announced prior to the availability of the snapshot.  If an Executor is supplied here, then it will be used to publish snapshots.  This can be useful if snapshot publishing takes a long time -- subsequent cycles may proceed while snapshot uploads are still in progress.  Number of cycles between snapshots : Because snapshots are not necessary for a data state to be available and announced, they need not be published every cycle.  If this parameter is specified, then a snapshot will be produced only every  (n+1)th  cycle.  VersionMinter : Allows for a custom version identifier minting strategy.  Target max type shard size : Specify a  target max type shard size .  Defaults to 16MB.   Each time a new  data state  should be produced, users should call  .runCycle(Populator) .  See  Getting Started  for more basic usage details.", 
            "title": "The HollowProducer"
        }, 
        {
            "location": "/producer-consumer/#restoring-at-startup", 
            "text": "Ideally the same  HollowProducer  would be held in memory forever, and  runCycle()  would be called every so often to produce a never-ending intact  delta chain .  However, this isn\u2019t always possible; the producer will need to be restarted from time to time due to deployment or other operational circumstances.  In order to produce a delta between states produced by one  HollowProducer  and another, the producer can  restore  the prior state upon restart, which will allow a delta and reverse delta to be produced.  See  Restoring at Startup  for usage.  Once we have  restored  the prior state, we can produce a delta from our producer's first cycle.  The delta will be applicable to any consumers which are on the state from which we restored.     Initializing Before Restore  Before  restoring , we must always  initialize  our data model.  A  HollowProducer 's data model may be initialized:   via the  HollowObjectMapper  by calling  initTypeState()  with all top-level classes  via a set of schemas  loaded from a text file  using the  HollowSchemaParser  and  HollowWriteStateCreator     Truncating a Delta Chain  If a problem occurs and you need to  pin back  consumers, you  may  want to restart your producer and explicitly restore from the pinned state.  Once the producer's first cycle completes, it will publish a delta from the pinned state to the newly produced state,  overwriting  the previous delta from the pinned state.  In this way, when you unpin, consumers will automatically follow the  new  delta, and the old forward-path from the pinned state will be  truncated .  If any consumers somehow did happen to remain on a  truncated  state, the reverse delta out of the truncated chain is still intact -- they could be manually pinned back to the restored state, then unpinned to get back up-to-date.", 
            "title": "Restoring At Startup"
        }, 
        {
            "location": "/producer-consumer/#rolling-back", 
            "text": "While producing a new state, if the  HollowProducer  encounters an error during data state population or validation fails, the current  data state  will be aborted and the underlying  state engine  will be rolled back to the previous data state.  Any delta produced on the next cycle will be from the last  successful  data state.", 
            "title": "Rolling Back"
        }, 
        {
            "location": "/producer-consumer/#validating-data", 
            "text": "It likely makes sense to perform some basic  validation  on your produced data states before announcing them to clients.  If you provide one or more  Validator s to the  HollowProducer , these will be automatically executed prior to announcement.  Validation rules will be specific to the semantics of the dataset, and may include some heuristics-based metrics based on expectations about the dataset.  If your  Validator  throws an Exception, the  HollowProducer  will automatically roll back the state engine and the  next successful  cycle will produce a delta from the prior successful state.", 
            "title": "Validating Data"
        }, 
        {
            "location": "/producer-consumer/#compacting-data", 
            "text": "It is possible to produce delta chains which extend over many thousands of states.  If during this delta chain an especially large delta happens for a specific type, it\u2019s possible that many ordinal holes will be present in that type.  If over time multiple types go through especially large deltas, this can have an impact on a dataset\u2019s heap footprint.  To reclaim heap space occupied by ordinal holes, a special  compaction cycle  can be run on the  HollowProducer .  During compaction, no record data will change, but identical records will be relocated off of the high end of the ordinal space into the ordinal holes.  This is accomplished by producing a new data state with no changes except for the more optimal ordinal assignments.  To run a  compaction cycle , call  runCompactionCycle(config)  on the  HollowProducer .  If this method returns a valid version identifier, then a compaction cycle occurred and produced a new data state.  If it returns  Long.MIN_VALUE , then the compaction criteria specified in the  CompactionConfig  was not met and no action was taken.  See the  HollowCompactor  javadoc for more details.", 
            "title": "Compacting Data"
        }, 
        {
            "location": "/producer-consumer/#the-hollowconsumer", 
            "text": "Data consumers keep their local copy of a dataset current by ensuring that their state engine is always at the latest  announced  data state. Consumers can arrive at a particular data state in a couple of different ways:   At initialization time, they will load a snapshot, which is an entire copy of the dataset to be forklifted into memory.  After initialization time, they will keep their local copy of the dataset current by applying delta transitions, which are the differences between adjacent data states.   The  HollowConsumer  encapsulates the details of initializing and keeping a dataset up to date.  In order to accomplish this task, a few infrastructure hooks should be injected:  HollowConsumer\n   .withBlobRetriever(blobRetriever)              /// required: a BlobRetriever\n   .withLocalBlobStore(localDiskDir)              /// optional: a local disk location\n   .withAnnouncementWatcher(announcementWatcher)  /// optional: a AnnouncementWatcher\n   .withRefreshListener(refreshListener)          /// optional: a RefreshListener\n   .withGeneratedAPIClass(MyGeneratedAPI.class)   /// optional: a generated client API class\n   .withFilterConfig(filterConfig)                /// optional: a HollowFilterConfig\n   .withDoubleSnapshotConfig(doubleSnapshotCfg)   /// optional: a DoubleSnapshotConfig\n   .withObjectLongevityConfig(objectLongevityCfg) /// optional: an ObjectLongevityConfig\n   .withObjectLongevityDetector(detector)         /// optional: an ObjectLongevityDetector\n   .withRefreshExecutor(refreshExecutor)          /// optional: an Executor\n   .build();  Let's examine each the injected hooks to the  HollowConsumer :   BlobRetriever : The interface to the blob store.  This is the only hook for which a custom implementation is required.  Each of the other hooks have default implementations which may be used.  The  BlobRetriever  may be omitted only if a previously-populated local blob store is specified.  Local blob store : A  File  which indicates where to record downloaded blobs and find previously downloaded blobs.  If specified along with a  BlobRetriever , the  HollowConsumer  will prefer to use previously downloaded blobs where applicable, and otherwise write newly downloaded blobs to the specified directory.  If specified  without  a  BlobRetriever , only previously downloaded blobs will be available.   AnnouncementWatcher : Provides an interface to the state announcement mechanism.  Often, announcement polling logic is encapsulated inside implementations.  RefreshListener : Provides hooks so that actions may be taken during and after updates (e.g. indexing).  Generated API Class : Specifies a  custom-generated Hollow API  to use.  HollowFilterConfig :   DoubleSnapshotConfig : Defines advanced settings related to  double snapshots .  ObjectLongevityConfig : Defines advanced settings related to  object longevity .  ObjectLongevityDetector : Implementations are notified when stale hollow object existence and usage is detected.  RefreshExecutor : An  Executor  to use when asynchronous updates are called via  triggerAsyncRefresh() .   Each time the identifier of the currently announced state changes,  triggerRefresh()  should be called on the  HollowConsumer .  This will bring the data up to date.  In general, the only requirement for getting Hollow consumers to work with your specific infrastructure is to implement a  BlobRetriever  and  AnnouncementWatcher , and use them with a  HollowConsumer .   Triggering Refresh  When implementing a  AnnouncementWatcher , you will need to implement the method  subscribeToUpdates(HollowConsumer consumer) .  When you\ncreate a  HollowConsumer  with an  AnnouncementWatcher , it will automatically call back to this method with itself as the argument.    You should track all  HollowConsumer s received by calls to this method.  When your announcement mechanism provides an updated value, \nyou should notify each  HollowConsumer  via the  triggerAsyncRefresh()  method.  In this way, your  HollowConsumer  injected with this  HollowAnnouncementWatcher  implementation will be automatically kept up-to-date.", 
            "title": "The HollowConsumer"
        }, 
        {
            "location": "/producer-consumer/#dataset-consistency", 
            "text": "If you have a long-running process which requires a consistent view of the dataset in a single state, you can prevent the  HollowConsumer  from updating while your process runs:  HollowConsumer consumer = ...\n\nconsumer.getRefreshLock().lock();\ntry {\n    /// run your process\n} finally {\n    consumer.getRefreshLock().unlock();\n}  The  getRefreshLock()  call returns the read lock in a  ReadWriteLock .  Refreshes use the write lock.", 
            "title": "Dataset Consistency"
        }, 
        {
            "location": "/tooling/", 
            "text": "Once your data is Hollow, you will be able to gain better insights into it.  Hollow ships with a number of useful tools for quickly gaining insights into your data, from broad patterns at high level, to zooming in to find and inspect specific individual records.\n\n\nInsight Tools\n\n\nHollow Explorer\n\n\nHollow ships with a UI which can be used to browse and search records within any dataset.\n\n\nExplorer Setup\n\n\nThe \nHollowExplorerUI\n class in the \nhollow-explorer-ui\n project is instantiated using either a \nHollowReadStateEngine\n or a \nHollowConsumer\n and a base URL path:\n\n\nHollowConsumer consumer = /// or your HollowReadStateEngine\n\nHollowExplorerUI ui = new HollowExplorerUI(\n, consumer);\n\n\n\n\nIncoming requests should be sent to the \nhandle\n method in your \nHollowExplorerUI\n instance:\n\n\npublic boolean handle(String target, \n                      HttpServletRequest req, \n                      HttpServletResponse resp) throws IOException\n\n\n\n\nThe \nHollowExplorerUI\n can be used in the context of an existing web container as shown above, \nor\n can be invoked via the included \nHollowExplorerUIServer\n, which uses the Jetty HTTP Servlet Server:\n\n\nHollowConsumer consumer = /// or your HollowReadStateEngine\n\nHollowExplorerUIServer server = new HollowExplorerUIServer(consumer, 8080);\n\nserver.start();\nserver.join();\n\n\n\n\nThe above call to \nserver.join()\n will block forever.  While the above code is running, you can point a browser to \nhttp://localhost:8080\n to explore your data.\n\n\n\n\nJetty: Optional Dependency\n\n\nIf using the \nHollowExplorerUIServer\n, you'll need to include a dependency on Jetty.  For example, with a Gradle build you may add the dependency: \n\n\ncompile 'org.eclipse.jetty:jetty-server:9.4.3.v20170317'\n\n\n\n\nExplorer Usage\n\n\nUpon opening your browser, you should see something like this:\n\n\n\n\nClick on a column header to sort by that column.  This view shows details about how many records exist for each type, and the approximate heap footprint of each type.\n\n\nClick on a type to browse records.  We'll arrive at a screen like the following:\n\n\n\n\nClicking on the record keys on the left will display the corresponding record contents in the display field.  \n\n\nNow imagine we wanted to search for movies in which Carrie-Anne Moss starred.  On this page, click the \nBrowse Schema\n link in the header to arrive at the following page:\n\n\n\n\nThe view on this page is a collapsible tree-view of the current type's \nschema\n.  Each searchable field in this view will contain a \nsearch\n link, which will prepopulate the \nsearch\n page with the type and field name.  We can navigate to the \nActor.actorName\n field and click \nsearch\n.  We will arrive on the \nsearch\n page with the type and field name prepopulated:\n\n\n\n\nWe can enter the field value we are looking for and click \nSubmit\n, and we will see the number of matching records of each type.  Search query matches are not limited to the directly matching records -- they are automatically rolled up to include any referencing records as well:\n\n\n\n\nIf we click on the type \nMovie\n on this page, we'll be presented with the browse view again, but this time filtered to matching records:\n\n\n\n\nSearch queries remain active in the browser session until cleared, and can be augmented to find the intersection of matches over multiple fields by entering multiple query parameters without clearing the existing ones in the session.  For example, if we want to find movies in which \nboth\n Carrie-Anne Moss and the Actor with ID \n1001\n starred, we can go back to the search page, and enter the appropriate criteria to augment our session's query.  The results will contain only records which match \nboth\n of these criteria:\n\n\n\n\nHistory tool\n\n\nHollow ships with a UI which can be used to browse and search changes in a dataset over time.  The history tool provides the ability to get a bird\u2019s eye view of all of the changes a dataset goes through over time, while simultaneously allowing for specific queries to see exactly how individual records change as the dataset transitions between states.  The history tool has proven to be enormously beneficial when investigating data issues in production scenarios.  When something looks incorrect, it\u2019s easy to pinpoint exactly what changed when, which can vastly expedite data corrections and eliminate hours of potential detective work.\n\n\nHistory Setup\n\n\nThe \nHollowHistoryUI\n class in the \nhollow-diff-ui\n project can be instantiated using a \nHollowConsumer\n and a base URL path:\n\n\nHollowConsumer consumer = /// or your HollowReadStateEngine\n\nHollowHistoryUI ui = new HollowHistoryUI(\n, consumer);\n\n\n\n\n\nThe \nHollowHistoryUI\n will by default be configured to track all of the types for which primary keys have been specified.  By default, it will track changes through the latest rolling 1024 states.  This default can be changed with another parameter in the constructor.\n\n\nIncoming requests should be sent to the \nhandle\n method in your \nHollowExplorerUI\n instance:\n\n\npublic boolean handle(String target, \n                      HttpServletRequest req, \n                      HttpServletResponse resp) throws IOException\n\n\n\n\nThe \nHollowHistoryUI\n can be used in the context of an existing web container as shown above, \nor\n can be invoked via the included \nHollowHistoryUIServer\n, which uses the Jetty HTTP Servlet Server:\n\n\nHollowConsumer consumer = ...\n\nHollowHistoryUIServer server = new HollowHistoryUIServer(consumer, 8080);\n\nserver.start();\nserver.join();\n\n\n\n\nThe above call to \nserver.join()\n will block forever.  While the above code is running, you can point a browser to \nhttp://localhost:8080\n to explore the history.\n\n\nHistory Usage\n\n\nUpon opening your browser, you will see something like this:\n\n\n\n\nThe history UI will track changes in types for which you have defined primary keys.  In this view, we're looking at the number of records which changed between \ndata states\n.  The changes in each state are broken down here into \nmodifications\n, \nadditions\n, and \nremovals\n.  Clicking on a state will show us a further breakdown of these changes by each top-level type:\n\n\n\n\nClicking on a type will show us the individual records which changed:\n\n\n\n\nIf we click on one of these records, we'll be able to inspect precisely what happened:\n\n\n\n\nThis is a collapsible tree-view in which the entire before/after state of the record is available, but only the differences are expanded by default.  Click on individual field names to expand/collapse them.\n\n\n\n\nPartially Expanded\n\n\nThe double-arrows in the example above mean the field is \npartially expanded\n to highlight the diffs.  Clicking on it will fully expand the field.\n\n\n\n\nRemovals show up as red, additions as green, and modifications as yellow.  The following example shows a modified actor name:\n\n\n\n\nIf this specific record key has changed in multiple states tracked by this history, those states will be highlighted on the left.  We can click back and forth through the changes to see how this record evolved over time.\n\n\nWe can also search for changes in specific records by their keys.  On each page in the history tool, a textbox is available in the header labeled \nLookup\n.   Plugging in \na single field\n of a key of any type into this field will find matching diffs through the history:\n\n\n\n\nClicking on individual records will bring us back to the object diff view page to see what was changed.\n\n\nDiff Tool\n\n\nJust as the Hollow history tool UI makes the differences between any two \nadjacent\n states in a delta chain readily accessible, the Hollow diff tool is used to investigate the differences between any two \narbitrary\n data states, even those which may exist in different delta chains. \n\n\nThis is especially useful as a step in a regular release cadence, as the differences between data states produced, for example, in a test environment and production environment can be evaluated at a glance.  Sometimes, unintended consequences of code updates may be discovered this way, which prevents production issues before they happen.\n\n\nInitiating a diff between two data states is accomplished by loading both states into separate \nHollowReadStateEngines\n in memory, and then instantiating a \nHollowDiff\n and configuring it with the primary keys of types to diff.  For our \nMovie\n/\nActor\n example:\n\n\nHollowConsumer testConsumer = /// load the test data\nHollowConsumer prodConsumer = /// load the prod data\n\nHollowReadStateEngine testData = testConsumer.getStateEngine(); \nHollowReadStateEngine prodData = prodConsumer.getStateEngine();\n\nHollowDiff diff = new HollowDiff(testData, prodData);\ndiff.addTypeDiff(\nMovie\n, \nid\n);\ndiff.addTypeDiff(\nActor\n, \nactorId\n);\n\ndiff.calculateDiffs();\n\n\n\n\nA diff is calculated by matching records of the same type based on defined primary keys.  The unmatched records in both states are tracked, and detailed differences between field values in matching pairs are also tracked.\n\n\n\n\nPrimary Keys\n\n\nThe \nHollowDiff\n will, by default, automatically configure any primary keys which are defined in the \nObject\n schemas of your dataset. \n\n\n\n\nHollow includes a ready-made UI which can be applied to a \nHollowDiff\n.    The \nHollowDiffUI\n class can be used in the context of an existing web container, or can be invoked via the \nHollowDiffUIServer\n, which uses the Jetty HTTP Servlet Server:\n\n\nHollowDiff diff = /// build the diff\n\nHollowDiffUIServer server = new HollowDiffUIServer(8080);\nserver.start();\n\nserver.addDiff(\ndiff\n, diff);\n\nserver.join();\n\n\n\n\nWhile the above code is running, you can point a browser to \nhttp://localhost:8080\n to explore the diff.\n\n\nHeap Usage Analysis\n\n\nOne of the most important considerations when dealing with in-memory datasets is the heap utilization of that dataset on consumer machines.  Hollow provides a number of methods to analyze this metric.\n\n\nGiven a loaded \nHollowReadStateEngine\n, it is possible to iterate over each type and gather statistics about its approximate heap usage.  This is done in the following example:\n\n\nHollowReadStateEngine stateEngine = /// a populated state engine\n\nlong totalApproximateHeapFootprint = 0;\n\nfor(HollowTypeReadState typeState : stateEngine.getTypeStates()) {\n    String typeName = typeState.getSchema().getName();\n    long heapCost = typeState.getApproximateHeapFootprintInBytes();\n    System.out.println(typeName + \n: \n + heapCost);\n    totalApproximateHeapFootprint += heapCost;\n}\n\nSystem.out.println(\nTOTAL: \n + totalApproximateHeapFootprint);\n\n\n\n\nAs shown above, information can be gathered about the total heap footprint, and also about the heap footprint of individual types.  This information can be helpful in identifying optimization targets.  This technique can also be used to identify how the heap cost of individual types changes over time, which can provide early warning signs about optimizations which should be targeted proactively.\n\n\nUsage Tracking\n\n\nHollow tracks usage, which can be investigated at runtime.  By default, this functionality is turned off, but it can be enabled by injecting a HollowSamplingDirector into a Hollow API in a running instance.  You can use the TimeSliceSamplingDirector implementation, which will by default record every access which happens during 1ms out of every second:\n\n\nMovieAPI api = /// a custom-generated API\n\nTimeSliceSamplingDirector samplingDirector = new TimeSliceSamplingDirector();\nsamplingDirector.startSampling();\n\napi.setSamplingDirector(samplingDirector);\n\n\n\n\nOnce this is enabled, and some time has passed for samples to be gathered, the results can be collected for analysis:\n\n\nfor(SampleResult result : api.getAccessSampleResults()) {\n    if(result.getNumSamples() \n 0)\n        System.out.println(result.getIdentifier() + \n: \n + \n                                                  result.getNumSamples());\n}\n\n\n\n\nTransitive Set Traverser\n\n\nThe \nTransitiveSetTraverser\n can be used to find children and parent references for a selected set of records.  We start with an initial set of selected records by ordinal, represented with a \nMap\nString, BitSet\n.  Entries in this map will indicate a type, plus the ordinals of the selected records:\n\n\nMap\nString, BitSet\n selection = new HashMap\nString, BitSet\n();\n\n/// select the movies with IDs 1 and 6.\nBitSet selectedMovies = new BitSet();\nselectedMovies.set(movieIdx.getMatchingOrdinal(1));\nselectedMovies.set(movieIdx.getMatchingOrdinal(6));\n\nselection.put(\nMovie\n, movies);\n\n\n\n\nWe can add the references, and the \ntransitive references\n of our selection.  After the following call returns, our selection will be augmented with these matches:\n\n\nTransitiveSetTraverser.addTransitiveMatches(readEngine, selection);\n\n\n\n\n\n\nTransitive References\n\n\nIf A references B, and B references C, then A transitively references C\n\n\n\n\nGiven a selection, we can also add any records which reference anything in the selection.  This is essentially the opposite operation as above; it can be said that \naddTransitiveMatches\n traverses down, while \naddReferencingOutsideClosure\n traverses up.  After the following call returns, our selection will be augmented with this selection:\n\n\nTransitiveSetTraverser.removedReferencedOutsideClosure(readEngine, selection);\n\n\n\n\nDataset Manipulation Tools\n\n\nFiltering\n\n\nSometimes, a dataset will be of interest to multiple different types of consumers, but not all consumers may be interested in all aspects of a dataset.  In these cases, it\u2019s possible to omit certain types and fields from a client\u2019s view of the data.  This is typically done to tailor a consumer\u2019s heap footprint and startup time costs based on their data needs.\n\n\nUsing our \nMovie\n/\nActor\n example above, if there was a consumer which was interested in \nMovie\n records, but not \nActor\n records, that consumer might construct a consumer-side data filter configuration in the following way:\n\n\nHollowFilterConfig config = new HollowFilterConfig(true);\nconfig.addField(\nMovie\n, \nactors\n);\nconfig.addType(\nListOfActor\n);\nconfig.addType(\nActor\n);\n\n\n\n\nThe boolean \ntrue\n parameter in the constructor above indicates that this is an exclusion filter.  We could accomplish the same goal using an inclusion filter:\n\n\nHollowFilterConfig config = new HollowFilterConfig(false);\nconfig.addField(\nMovie\n, \nid\n);\nconfig.addField(\nMovie\n, \ntitle\n);\nconfig.addField(\nMovie\n, \nreleaseYear\n);\nconfig.addType(\nString\n);\n\n\n\n\nThe difference between these two configurations is how the filter behaves as new types and fields are added to the data model.  The exclusion filter will not exclude them by default, whereas the inclusion filter will.\n\n\nA filter configuration is applied to a \nHollowConsumer\n at read time:\n\n\nHollowConsumer consumer = HollowConsumer.withBlobReader(reader)\n                                        .withFilterConfig(config)\n                                        .build();\n\n\n\n\nCombining\n\n\nThe \nHollowCombiner\n is used to copy data from one or more copies of hollow datasets in \nHollowReadStateEngine\ns into a single \nHollowWriteStateEngine\n.  If each of the inputs contain the same data model, the following is sufficient to combine them:\n\n\nHollowReadStateEngine input1 = /// an input\nHollowReadStateEngine input2 = /// another input\n\nHollowCombiner combiner = new HollowCombiner(input1, input2);\ncombiner.combine();\n\nHollowWriteStateEngine combined = combiner.getCombinedStateEngine();\n\n\n\n\nBy default, the combiner will copy all records from all types from the inputs to the output.  We can direct the combiner to exclude certain records from copying using a \nHollowCombinerCopyDirector\n.  The interface for a \nHollowCombinerCopyDirector\n allows for making decisions about copying individual records during a combine operation by implementing the following method:\n\n\npublic boolean shouldCopy(HollowTypeReadState typeState, int ordinal);\n\n\n\n\nIf this method returns false, then the copier will not attempt to directly copy the matching record.  However, if the matching record is referenced via another record for which this method returns true, then it will still be copied regardless of the return value of this method.\n\n\nThe most broadly useful provided implementation of the \nHollowCombinerCopyDirector\n is the \nHollowCombinerExcludePrimaryKeysCopyDirector\n, which can be used to specify record exclusions by primary key.  For example, if we wanted to create a copy of a state engine with the \nMovie\n records with ids 100 and 125 excluded:\n\n\nHollowReadStateEngine input = /// an input\nHollowPrimaryKeyIndex idx = new HollowPrimaryKeyIndex(input, \nMovie\n, \nid\n);\n\nHollowCombinerExcludePrimaryKeysCopyDirector director = \n                          new HollowCombinerExcludePrimaryKeysCopyDirector();\n\ndirector.excludeKey(idx, 100);\ndirector.excludeKey(idx, 125);\n\nHollowCombiner combiner = new HollowCombiner(director, input);\ncombiner.combine();\n\nHollowWriteStateEngine result = combiner.getCombineStateEngine();\n\n\n\n\nIt\u2019s possible that while combining two inputs, both may have a record of the same type with the same primary key.  This violation of the uniqueness constraint of a primary key can be avoided by informing the combiner of the primary keys in a data model prior to the combine operation:\n\n\nHollowCombiner combiner = new HollowCombiner(input1, input2);\n\ncombiner.setPrimaryKeys(\n        new PrimaryKey(\nMovie\n, \nid\n),\n        new PrimaryKey(\nActor\n, \nactorId\n)\n);\n\ncombiner.combine();\n\n\n\n\nIf multiple records exist in the inputs matching a single value for any of the supplied primary keys, then only one such record will be copied to the output.  The specific record which is copied will be the record from the input was supplied earliest in the constructor of the \nHollowCombiner\n.  Further, if any record references another record which was omitted because it would have been duplicate based on this rule, then that reference is remapped in the output state to the matching record which was chosen to be included.\n\n\nSplitting\n\n\nA single dataset can be sharded into multiple datasets using a \nHollowSplitter\n.  The \nHollowSplitter\n takes a \nHollowSplitterCopyDirector\n, which indicates:\n\n\n\n\ntop level\n types to split,\n\n\nthe number of shards to create, and \n\n\nwhich shard to send individual records.\n\n\n\n\n\n\nTop Level Types\n\n\nTop level types are those which are not referenced by any other types.  In our \nMovie\n/\nActor\n example, \nMovie\n is a top-level type, but \nActor\n is not.\n\n\n\n\nTwo default implementations of \nHollowSplitterCopyDirector\n are available: \n\n\n\n\nHollowSplitterOrdinalCopyDirector\n\n\nHollowSplitterPrimaryKeyCopyDirector\n.  \n\n\n\n\nThese directors will split top-level types among a specified number of shards either by ordinals or primary keys, respectively.  When splitting by ordinal, a record with a specific primary key may jump between shards when it is modified, while with the primary key director a specific primary key will consistently hash to the same shard.\n\n\nOur \nMovie\n/\nActor\n example may use the splitter to split a dataset into four shards with the following invocation:\n\n\nHollowReadStateEngine stateEngine = /// a state engine\n\nHollowSplitterCopyDirector director = \n                            new HollowSplitterOrdinalCopyDirector(4, \nMovie\n);\n\nHollowSplitter splitter = new HollowSplitter(director, stateEngine);\nsplitter.split();\n\n\nfor(int i=0; i\n4; i++) {\n    HollowWriteStateEngine shard = splitter.getOutputShardStateEngine(i);\n}\n\n\n\n\nState Manipulation Tools\n\n\nPatching\n\n\nUsing the \nHollowWriteStateEngine\n\u2019s restore capability, it\u2019s possible to produce deltas forever, so that consumers never have to load a snapshot after initialization.  However, if environmental hiccups cause a producer to fail to publish a delta, or if a delta is lost, or if it\u2019s desired to publish a delta between non-adjacent states, then the \nHollowStateDeltaPatcher\n may be used to produce deltas between two arbitrary states within the same delta chain.\n\n\nThe \nHollowStateDeltaPatcher\n must produce \ntwo\n delta transitions to create a transition between arbitrary states.  This is because non-adjacent states may have different records occupying the same ordinals.  Since no ordinal may be removed and added in adjacent states, the state patcher must create an intermediate state in which modified records do not share any ordinals.\n\n\nSee the \nHollowStateDeltaPatcher\n javadocs for usage details.", 
            "title": "Tooling"
        }, 
        {
            "location": "/tooling/#insight-tools", 
            "text": "", 
            "title": "Insight Tools"
        }, 
        {
            "location": "/tooling/#hollow-explorer", 
            "text": "Hollow ships with a UI which can be used to browse and search records within any dataset.", 
            "title": "Hollow Explorer"
        }, 
        {
            "location": "/tooling/#explorer-setup", 
            "text": "The  HollowExplorerUI  class in the  hollow-explorer-ui  project is instantiated using either a  HollowReadStateEngine  or a  HollowConsumer  and a base URL path:  HollowConsumer consumer = /// or your HollowReadStateEngine\n\nHollowExplorerUI ui = new HollowExplorerUI( , consumer);  Incoming requests should be sent to the  handle  method in your  HollowExplorerUI  instance:  public boolean handle(String target, \n                      HttpServletRequest req, \n                      HttpServletResponse resp) throws IOException  The  HollowExplorerUI  can be used in the context of an existing web container as shown above,  or  can be invoked via the included  HollowExplorerUIServer , which uses the Jetty HTTP Servlet Server:  HollowConsumer consumer = /// or your HollowReadStateEngine\n\nHollowExplorerUIServer server = new HollowExplorerUIServer(consumer, 8080);\n\nserver.start();\nserver.join();  The above call to  server.join()  will block forever.  While the above code is running, you can point a browser to  http://localhost:8080  to explore your data.   Jetty: Optional Dependency  If using the  HollowExplorerUIServer , you'll need to include a dependency on Jetty.  For example, with a Gradle build you may add the dependency:   compile 'org.eclipse.jetty:jetty-server:9.4.3.v20170317'", 
            "title": "Explorer Setup"
        }, 
        {
            "location": "/tooling/#explorer-usage", 
            "text": "Upon opening your browser, you should see something like this:   Click on a column header to sort by that column.  This view shows details about how many records exist for each type, and the approximate heap footprint of each type.  Click on a type to browse records.  We'll arrive at a screen like the following:   Clicking on the record keys on the left will display the corresponding record contents in the display field.    Now imagine we wanted to search for movies in which Carrie-Anne Moss starred.  On this page, click the  Browse Schema  link in the header to arrive at the following page:   The view on this page is a collapsible tree-view of the current type's  schema .  Each searchable field in this view will contain a  search  link, which will prepopulate the  search  page with the type and field name.  We can navigate to the  Actor.actorName  field and click  search .  We will arrive on the  search  page with the type and field name prepopulated:   We can enter the field value we are looking for and click  Submit , and we will see the number of matching records of each type.  Search query matches are not limited to the directly matching records -- they are automatically rolled up to include any referencing records as well:   If we click on the type  Movie  on this page, we'll be presented with the browse view again, but this time filtered to matching records:   Search queries remain active in the browser session until cleared, and can be augmented to find the intersection of matches over multiple fields by entering multiple query parameters without clearing the existing ones in the session.  For example, if we want to find movies in which  both  Carrie-Anne Moss and the Actor with ID  1001  starred, we can go back to the search page, and enter the appropriate criteria to augment our session's query.  The results will contain only records which match  both  of these criteria:", 
            "title": "Explorer Usage"
        }, 
        {
            "location": "/tooling/#history-tool", 
            "text": "Hollow ships with a UI which can be used to browse and search changes in a dataset over time.  The history tool provides the ability to get a bird\u2019s eye view of all of the changes a dataset goes through over time, while simultaneously allowing for specific queries to see exactly how individual records change as the dataset transitions between states.  The history tool has proven to be enormously beneficial when investigating data issues in production scenarios.  When something looks incorrect, it\u2019s easy to pinpoint exactly what changed when, which can vastly expedite data corrections and eliminate hours of potential detective work.", 
            "title": "History tool"
        }, 
        {
            "location": "/tooling/#history-setup", 
            "text": "The  HollowHistoryUI  class in the  hollow-diff-ui  project can be instantiated using a  HollowConsumer  and a base URL path:  HollowConsumer consumer = /// or your HollowReadStateEngine\n\nHollowHistoryUI ui = new HollowHistoryUI( , consumer);  The  HollowHistoryUI  will by default be configured to track all of the types for which primary keys have been specified.  By default, it will track changes through the latest rolling 1024 states.  This default can be changed with another parameter in the constructor.  Incoming requests should be sent to the  handle  method in your  HollowExplorerUI  instance:  public boolean handle(String target, \n                      HttpServletRequest req, \n                      HttpServletResponse resp) throws IOException  The  HollowHistoryUI  can be used in the context of an existing web container as shown above,  or  can be invoked via the included  HollowHistoryUIServer , which uses the Jetty HTTP Servlet Server:  HollowConsumer consumer = ...\n\nHollowHistoryUIServer server = new HollowHistoryUIServer(consumer, 8080);\n\nserver.start();\nserver.join();  The above call to  server.join()  will block forever.  While the above code is running, you can point a browser to  http://localhost:8080  to explore the history.", 
            "title": "History Setup"
        }, 
        {
            "location": "/tooling/#history-usage", 
            "text": "Upon opening your browser, you will see something like this:   The history UI will track changes in types for which you have defined primary keys.  In this view, we're looking at the number of records which changed between  data states .  The changes in each state are broken down here into  modifications ,  additions , and  removals .  Clicking on a state will show us a further breakdown of these changes by each top-level type:   Clicking on a type will show us the individual records which changed:   If we click on one of these records, we'll be able to inspect precisely what happened:   This is a collapsible tree-view in which the entire before/after state of the record is available, but only the differences are expanded by default.  Click on individual field names to expand/collapse them.   Partially Expanded  The double-arrows in the example above mean the field is  partially expanded  to highlight the diffs.  Clicking on it will fully expand the field.   Removals show up as red, additions as green, and modifications as yellow.  The following example shows a modified actor name:   If this specific record key has changed in multiple states tracked by this history, those states will be highlighted on the left.  We can click back and forth through the changes to see how this record evolved over time.  We can also search for changes in specific records by their keys.  On each page in the history tool, a textbox is available in the header labeled  Lookup .   Plugging in  a single field  of a key of any type into this field will find matching diffs through the history:   Clicking on individual records will bring us back to the object diff view page to see what was changed.", 
            "title": "History Usage"
        }, 
        {
            "location": "/tooling/#diff-tool", 
            "text": "Just as the Hollow history tool UI makes the differences between any two  adjacent  states in a delta chain readily accessible, the Hollow diff tool is used to investigate the differences between any two  arbitrary  data states, even those which may exist in different delta chains.   This is especially useful as a step in a regular release cadence, as the differences between data states produced, for example, in a test environment and production environment can be evaluated at a glance.  Sometimes, unintended consequences of code updates may be discovered this way, which prevents production issues before they happen.  Initiating a diff between two data states is accomplished by loading both states into separate  HollowReadStateEngines  in memory, and then instantiating a  HollowDiff  and configuring it with the primary keys of types to diff.  For our  Movie / Actor  example:  HollowConsumer testConsumer = /// load the test data\nHollowConsumer prodConsumer = /// load the prod data\n\nHollowReadStateEngine testData = testConsumer.getStateEngine(); \nHollowReadStateEngine prodData = prodConsumer.getStateEngine();\n\nHollowDiff diff = new HollowDiff(testData, prodData);\ndiff.addTypeDiff( Movie ,  id );\ndiff.addTypeDiff( Actor ,  actorId );\n\ndiff.calculateDiffs();  A diff is calculated by matching records of the same type based on defined primary keys.  The unmatched records in both states are tracked, and detailed differences between field values in matching pairs are also tracked.   Primary Keys  The  HollowDiff  will, by default, automatically configure any primary keys which are defined in the  Object  schemas of your dataset.    Hollow includes a ready-made UI which can be applied to a  HollowDiff .    The  HollowDiffUI  class can be used in the context of an existing web container, or can be invoked via the  HollowDiffUIServer , which uses the Jetty HTTP Servlet Server:  HollowDiff diff = /// build the diff\n\nHollowDiffUIServer server = new HollowDiffUIServer(8080);\nserver.start();\n\nserver.addDiff( diff , diff);\n\nserver.join();  While the above code is running, you can point a browser to  http://localhost:8080  to explore the diff.", 
            "title": "Diff Tool"
        }, 
        {
            "location": "/tooling/#heap-usage-analysis", 
            "text": "One of the most important considerations when dealing with in-memory datasets is the heap utilization of that dataset on consumer machines.  Hollow provides a number of methods to analyze this metric.  Given a loaded  HollowReadStateEngine , it is possible to iterate over each type and gather statistics about its approximate heap usage.  This is done in the following example:  HollowReadStateEngine stateEngine = /// a populated state engine\n\nlong totalApproximateHeapFootprint = 0;\n\nfor(HollowTypeReadState typeState : stateEngine.getTypeStates()) {\n    String typeName = typeState.getSchema().getName();\n    long heapCost = typeState.getApproximateHeapFootprintInBytes();\n    System.out.println(typeName +  :   + heapCost);\n    totalApproximateHeapFootprint += heapCost;\n}\n\nSystem.out.println( TOTAL:   + totalApproximateHeapFootprint);  As shown above, information can be gathered about the total heap footprint, and also about the heap footprint of individual types.  This information can be helpful in identifying optimization targets.  This technique can also be used to identify how the heap cost of individual types changes over time, which can provide early warning signs about optimizations which should be targeted proactively.", 
            "title": "Heap Usage Analysis"
        }, 
        {
            "location": "/tooling/#usage-tracking", 
            "text": "Hollow tracks usage, which can be investigated at runtime.  By default, this functionality is turned off, but it can be enabled by injecting a HollowSamplingDirector into a Hollow API in a running instance.  You can use the TimeSliceSamplingDirector implementation, which will by default record every access which happens during 1ms out of every second:  MovieAPI api = /// a custom-generated API\n\nTimeSliceSamplingDirector samplingDirector = new TimeSliceSamplingDirector();\nsamplingDirector.startSampling();\n\napi.setSamplingDirector(samplingDirector);  Once this is enabled, and some time has passed for samples to be gathered, the results can be collected for analysis:  for(SampleResult result : api.getAccessSampleResults()) {\n    if(result.getNumSamples()   0)\n        System.out.println(result.getIdentifier() +  :   + \n                                                  result.getNumSamples());\n}", 
            "title": "Usage Tracking"
        }, 
        {
            "location": "/tooling/#transitive-set-traverser", 
            "text": "The  TransitiveSetTraverser  can be used to find children and parent references for a selected set of records.  We start with an initial set of selected records by ordinal, represented with a  Map String, BitSet .  Entries in this map will indicate a type, plus the ordinals of the selected records:  Map String, BitSet  selection = new HashMap String, BitSet ();\n\n/// select the movies with IDs 1 and 6.\nBitSet selectedMovies = new BitSet();\nselectedMovies.set(movieIdx.getMatchingOrdinal(1));\nselectedMovies.set(movieIdx.getMatchingOrdinal(6));\n\nselection.put( Movie , movies);  We can add the references, and the  transitive references  of our selection.  After the following call returns, our selection will be augmented with these matches:  TransitiveSetTraverser.addTransitiveMatches(readEngine, selection);   Transitive References  If A references B, and B references C, then A transitively references C   Given a selection, we can also add any records which reference anything in the selection.  This is essentially the opposite operation as above; it can be said that  addTransitiveMatches  traverses down, while  addReferencingOutsideClosure  traverses up.  After the following call returns, our selection will be augmented with this selection:  TransitiveSetTraverser.removedReferencedOutsideClosure(readEngine, selection);", 
            "title": "Transitive Set Traverser"
        }, 
        {
            "location": "/tooling/#dataset-manipulation-tools", 
            "text": "", 
            "title": "Dataset Manipulation Tools"
        }, 
        {
            "location": "/tooling/#filtering", 
            "text": "Sometimes, a dataset will be of interest to multiple different types of consumers, but not all consumers may be interested in all aspects of a dataset.  In these cases, it\u2019s possible to omit certain types and fields from a client\u2019s view of the data.  This is typically done to tailor a consumer\u2019s heap footprint and startup time costs based on their data needs.  Using our  Movie / Actor  example above, if there was a consumer which was interested in  Movie  records, but not  Actor  records, that consumer might construct a consumer-side data filter configuration in the following way:  HollowFilterConfig config = new HollowFilterConfig(true);\nconfig.addField( Movie ,  actors );\nconfig.addType( ListOfActor );\nconfig.addType( Actor );  The boolean  true  parameter in the constructor above indicates that this is an exclusion filter.  We could accomplish the same goal using an inclusion filter:  HollowFilterConfig config = new HollowFilterConfig(false);\nconfig.addField( Movie ,  id );\nconfig.addField( Movie ,  title );\nconfig.addField( Movie ,  releaseYear );\nconfig.addType( String );  The difference between these two configurations is how the filter behaves as new types and fields are added to the data model.  The exclusion filter will not exclude them by default, whereas the inclusion filter will.  A filter configuration is applied to a  HollowConsumer  at read time:  HollowConsumer consumer = HollowConsumer.withBlobReader(reader)\n                                        .withFilterConfig(config)\n                                        .build();", 
            "title": "Filtering"
        }, 
        {
            "location": "/tooling/#combining", 
            "text": "The  HollowCombiner  is used to copy data from one or more copies of hollow datasets in  HollowReadStateEngine s into a single  HollowWriteStateEngine .  If each of the inputs contain the same data model, the following is sufficient to combine them:  HollowReadStateEngine input1 = /// an input\nHollowReadStateEngine input2 = /// another input\n\nHollowCombiner combiner = new HollowCombiner(input1, input2);\ncombiner.combine();\n\nHollowWriteStateEngine combined = combiner.getCombinedStateEngine();  By default, the combiner will copy all records from all types from the inputs to the output.  We can direct the combiner to exclude certain records from copying using a  HollowCombinerCopyDirector .  The interface for a  HollowCombinerCopyDirector  allows for making decisions about copying individual records during a combine operation by implementing the following method:  public boolean shouldCopy(HollowTypeReadState typeState, int ordinal);  If this method returns false, then the copier will not attempt to directly copy the matching record.  However, if the matching record is referenced via another record for which this method returns true, then it will still be copied regardless of the return value of this method.  The most broadly useful provided implementation of the  HollowCombinerCopyDirector  is the  HollowCombinerExcludePrimaryKeysCopyDirector , which can be used to specify record exclusions by primary key.  For example, if we wanted to create a copy of a state engine with the  Movie  records with ids 100 and 125 excluded:  HollowReadStateEngine input = /// an input\nHollowPrimaryKeyIndex idx = new HollowPrimaryKeyIndex(input,  Movie ,  id );\n\nHollowCombinerExcludePrimaryKeysCopyDirector director = \n                          new HollowCombinerExcludePrimaryKeysCopyDirector();\n\ndirector.excludeKey(idx, 100);\ndirector.excludeKey(idx, 125);\n\nHollowCombiner combiner = new HollowCombiner(director, input);\ncombiner.combine();\n\nHollowWriteStateEngine result = combiner.getCombineStateEngine();  It\u2019s possible that while combining two inputs, both may have a record of the same type with the same primary key.  This violation of the uniqueness constraint of a primary key can be avoided by informing the combiner of the primary keys in a data model prior to the combine operation:  HollowCombiner combiner = new HollowCombiner(input1, input2);\n\ncombiner.setPrimaryKeys(\n        new PrimaryKey( Movie ,  id ),\n        new PrimaryKey( Actor ,  actorId )\n);\n\ncombiner.combine();  If multiple records exist in the inputs matching a single value for any of the supplied primary keys, then only one such record will be copied to the output.  The specific record which is copied will be the record from the input was supplied earliest in the constructor of the  HollowCombiner .  Further, if any record references another record which was omitted because it would have been duplicate based on this rule, then that reference is remapped in the output state to the matching record which was chosen to be included.", 
            "title": "Combining"
        }, 
        {
            "location": "/tooling/#splitting", 
            "text": "A single dataset can be sharded into multiple datasets using a  HollowSplitter .  The  HollowSplitter  takes a  HollowSplitterCopyDirector , which indicates:   top level  types to split,  the number of shards to create, and   which shard to send individual records.    Top Level Types  Top level types are those which are not referenced by any other types.  In our  Movie / Actor  example,  Movie  is a top-level type, but  Actor  is not.   Two default implementations of  HollowSplitterCopyDirector  are available:    HollowSplitterOrdinalCopyDirector  HollowSplitterPrimaryKeyCopyDirector .     These directors will split top-level types among a specified number of shards either by ordinals or primary keys, respectively.  When splitting by ordinal, a record with a specific primary key may jump between shards when it is modified, while with the primary key director a specific primary key will consistently hash to the same shard.  Our  Movie / Actor  example may use the splitter to split a dataset into four shards with the following invocation:  HollowReadStateEngine stateEngine = /// a state engine\n\nHollowSplitterCopyDirector director = \n                            new HollowSplitterOrdinalCopyDirector(4,  Movie );\n\nHollowSplitter splitter = new HollowSplitter(director, stateEngine);\nsplitter.split();\n\n\nfor(int i=0; i 4; i++) {\n    HollowWriteStateEngine shard = splitter.getOutputShardStateEngine(i);\n}", 
            "title": "Splitting"
        }, 
        {
            "location": "/tooling/#state-manipulation-tools", 
            "text": "", 
            "title": "State Manipulation Tools"
        }, 
        {
            "location": "/tooling/#patching", 
            "text": "Using the  HollowWriteStateEngine \u2019s restore capability, it\u2019s possible to produce deltas forever, so that consumers never have to load a snapshot after initialization.  However, if environmental hiccups cause a producer to fail to publish a delta, or if a delta is lost, or if it\u2019s desired to publish a delta between non-adjacent states, then the  HollowStateDeltaPatcher  may be used to produce deltas between two arbitrary states within the same delta chain.  The  HollowStateDeltaPatcher  must produce  two  delta transitions to create a transition between arbitrary states.  This is because non-adjacent states may have different records occupying the same ordinals.  Since no ordinal may be removed and added in adjacent states, the state patcher must create an intermediate state in which modified records do not share any ordinals.  See the  HollowStateDeltaPatcher  javadocs for usage details.", 
            "title": "Patching"
        }, 
        {
            "location": "/data-modeling/", 
            "text": "Schemas\n\n\nA Hollow data model is a set of schemas, which are usually defined by the POJOs used on the producer to \npopulate the data\n.  This section will use POJOs as examples, but there are other ways to define schemas -- for example you could ingest a text file and use the \nschema parser\n.\n\n\n\n\nSchemas Define the Data Model\n\n\nA hollow dataset is comprised of one or more data \ntypes\n.  The \ndata model\n for a dataset is defined by the schemas describing those types.\n\n\n\n\nObject Schemas\n\n\nEach POJO class you define will result in an \nObject\n schema, which is a fixed set of strongly typed fields.  The fields will be based on the member variables in the class.  For example, the class \nMovie\n will define an \nObject\n schema with three fields:\n\n\npublic class Movie {\n    int movieId;\n    String title;\n    Set\nActor\n actors;\n}\n\n\n\n\n\nEach schema has a \ntype name\n.  The name of the type will default to the simple name of your POJO -- in this case \nMovie\n.  \n\n\nEach schema field has a \nfield name\n, which will default to the same name as the field in the POJO -- in this case \nmovieId\n, \ntitle\n, and \nactors\n.  Each field also has a \nfield type\n, which is in this case \nINT\n, \nREFERENCE\n, and \nREFERENCE\n, respectively.  Each \nREFERENCE\n field also indicates the \nreferenced type\n, which for our reference fields above default to \nString\n and \nSetOfActor\n.\n\n\nThe possible field types are:\n\n\n\n\nINT\n: An integer value up to 32-bits\n\n\nLONG\n: An integer value up to 64-bits\n\n\nFLOAT\n: A 32-bit floating-point value\n\n\nDOUBLE\n: A 64-bit floating-point value\n\n\nBOOLEAN\n: \ntrue\n or \nfalse\n\n\nSTRING\n: An array of characters\n\n\nBYTES\n: An array of bytes\n\n\nREFERENCE\n: A reference to another specific type.  The referenced type must be defined by the schema.\n\n\n\n\nNotice that since the reference type is \ndefined by the schema\n, data models must be strongly typed.  Each reference in your data model must point to a specific concrete implementation.  References to interfaces, abstract classes, or \njava.lang.Object\n are not supported.\n\n\nPrimary Keys\n\n\nObject\n schemas may specify a primary key.  This is accomplished by using the \n@HollowPrimaryKey\n annotation and specifying the fields.\n\n\n@HollowPrimaryKey(fields={\nmovieId\n})\npublic class Movie {\n    int movieId;\n    String title;\n    Set\nActor\n actors;\n}\n\n\n\n\nWhen defined in the schema, primary keys are a part of your data model and drive useful functionality and default configuration in the \nhollow explorer\n, \nhollow history\n, and \ndiff ui\n.  They also provide a shortcut when creating a \nprimary key index\n.\n\n\nPrimary keys defined in the schema follow the same convention as primary keys defined for indexes.  They consist of one or more \nfield paths\n, which will auto-expand if they terminate in a \nREFERENCE\n field.\n\n\nInlined vs Referenced Fields\n\n\nWe can \ninline\n some fields in our POJOs so that they are no longer \nREFERENCE\n fields, but instead encode their data directly in each record:\n\n\npublic class Movie {\n    int movieId;\n    @HollowInline String title;\n    Set\nActor\n actors;\n}\n\n\n\n\nIn the above example, our fields are now of type \nINT\n, \nSTRING\n, and \nREFERENCE\n.\n\n\nWhile modeling data, we choose whether or not to inline a field for efficiency.  Consider the following type:\n\n\npublic class Award {\n    String awardName;\n    long movieId;\n    long actorId;\n}\n\n\n\n\nIn this case, imagine \nawardName\n is something like \u201cBest Supporting Actress\u201d.  Over the years, many such awards will be given, so we\u2019ll have a lot of records which share that value.  If we use an \ninlined\n \nSTRING\n field, then the value \"Best Supporting Actress\" will be repeated for every such award record.  However, if we reference a separate record type, all such awards will reference the same child record with that value.  If the \nawardName\n values have a lot of repetition, then this can result in a significant savings.\n\n\n\n\nDeduplication\n\n\nRecord deduplication happens automatically at the \nrecord\n granularity in Hollow.  Try to model your data such that when there is a lot of repetition in records, the repetitive fields are encapsulated into their own types.\n\n\n\n\nTo consider the opposite case, let\u2019s examine the following \nActor\n type:\n\n\npublic class Actor {\n    long id;\n    @HollowInline String actorName;\n}\n\n\n\n\nThe \nactorName\n is unlikely to be repeated often.  In this case, if we reference a separate record type, we have to retain roughly the same number of unique character strings \nplus\n we need to retain references to those records.  In this case, we end up saving space by using an inlined \nSTRING\n field instead of a reference to a separate type.\n\n\n\n\nReference Costs\n\n\nA \nREFERENCE\n field isn't free, and therefore we shouldn't necessarily try to encapsulate fields inside their own record types where we won't benefit from deduplication.  These fields should instead be \ninlined\n.\n\n\n\n\nWe refer to fields which are defined with native Hollow types as \ninlined\n fields, and fields which are defined as references to types with a single field as \nreferenced\n fields.\n\n\nNamespaced Record Type Names\n\n\nIn order to be very efficient, referenced types sometimes should be \nnamespaced\n so that fields with like values may reference the same \nrecord type\n, but reference fields of the same \nprimitive type\n elsewhere in the data model use different \nrecord types\n.  For example, consider our \nAward\n type again, but this time, we\u2019ll reference a type called \nAwardName\n, instead of \nString\n.  We can explicitly name the \ntype\n of a field with the \n@HollowTypeName\n annotation:\n\n\npublic class Award {\n    @HollowTypeName(name=\nAwardName\n)\n    String awardName;\n    long movieId;\n    long actorId;\n}\n\n\n\n\n\nOther types in our data model which reference award names can reuse the \nAwardName\n type.  Other referenced string fields in our data model, which are unrelated to award names, should use different types corresponding to the semantics of their values.  \n\n\nNamespacing fields saves space because references to types with a lower cardinality use fewer bits than references to types with a higher cardinality.  The reason for this can be gleaned from the \nIn-Memory Data Layout\n topic underneath the \nAdvanced Topics\n section.\n\n\nNamespacing fields is also useful if some consumers don't need the contents of a specific referenced field.  If a type is namespaced, it can be selectively \nfiltered\n, whereas if it is grouped with other fields which \nare\n needed by all consumers, then it cannot be selected for filtering.\n\n\n\n\nNamespacing Reduces Reference Costs\n\n\nUsing an appropriately \nnamespaced\n type reduces the heap footprint cost of \nREFERENCE\n fields.\n\n\n\n\n\n\nChanging default \ntype names\n\n\nThe \n@HollowTypeName\n annotation can also be used at the class level to select a default type name for a class other than its simple name.\n\n\n\n\nGrouping Associated Fields\n\n\nReferencing fields can save space because the same field values do not have to be repeated for every record in which they occur.  Similarly, we can \ngroup\n fields which have covarying values, and pull these out from larger objects as their own types.  For example, imagine we started with a \nMovie\n type which included the following fields:\n\n\npublic class Movie {\n    long id;\n    String title;\n    String maturityRating;\n    String advisories;\n}\n\n\n\n\nWe might notice that the \nmaturityRating\n and \nadvisories\n fields vary together, and are often the repeated across many \nMovie\n records.  We can pull out a separate type for these fields:\n\n\npublic class Movie {\n    long id;\n    String title;\n    MaturityRating maturityRating;\n}\n\npublic class MaturityRating {\n    string rating;\n    string advisories;\n}\n\n\n\n\nWe could have referenced these fields separately.  If we had done so, each \nMovie\n record, of which there are probably many, would have had to contain two separate references for these fields.  Instead, by recognizing that these fields were associated and pulling them together, space is saved because each \nMovie\n record now only contains one reference for this data.\n\n\nList Schemas\n\n\nYou can define \nList\n schemas by adding a member variable of type \nList\n in your data model.  For example:\n\n\npublic class Movie {\n    long id;\n    String title;\n    List\nAward\n awardsReceived;\n}\n\n\n\n\n\nThe \nList\n must explicitly define its parameterized element type.  The default \ntype name\n of the above \nList\n schema will be \nListOfAward\n.  \n\n\nA \nList\n schema indicates a record type which is an ordered collection of \nREFERENCE\n fields.  Each record will have a variable number of references.  The referenced type must be defined by the schema, and all references in all records will encode only the \nordinals\n of the referenced records as the values for these references.\n\n\nSet Schemas\n\n\nYou can define \nSet\n schemas by adding a member variable of type \nSet\n in your data model.  The \nSet\n must explicitly define its parameterized element type.\n\n\nA \nSet\n schema indicates a record type which is an unordered collection of \nREFERENCE\n fields.  Each record will have a variable number of references, and the referenced type must be defined by the schema.  Within a single set record, each reference must be unique.  \n\n\nReferences in \nSet\n records can be hashed by some specific element fields for O(1) retrieval.  In order to enable this feature, a \nSet\n schema will define an optional \nhash key\n, which defines how its elements are hashed/indexed.\n\n\nMap Schemas\n\n\nYou can define \nMap\n schemas by adding a member variable of type \nMap\n in your data model.  The \nMap\n must explicitly define it parameterized key and values types.  \n\n\nA \nMap\n schema indicates a record type which is an unordered collection of pairs of \nREFERENCE\n fields, used to represent a key/value mapping.  Each record will have a variable number of key/value pairs.  Both the key reference type and the value reference type must be defined by the schema.  The key reference type does not have to be the same as the value reference type.  Within a single map record, each key reference must be unique.  \n\n\nEntries in \nMap\n records can be hashed by some specific key fields for O(1) retrieval of the keys, values, and/or entries.  In order to enable this feature, a \nMap\n schema will define an optional \nhash key\n, which defines how its entries are hashed/indexed.\n\n\nHash Keys\n\n\nEach \nMap\n and \nSet\n schema may optionally define a \nhash key\n.  A \nhash key\n specifies one or more user-defined fields used to hash entries into the collection.  When a hash key is defined on a \nSet\n, each set record becomes like a primary key index; records in the set can be efficiently retrieved by matching the specified \nhash key\n fields.  Similarly, when a hash key is defined on a \nMap\n, each map record becomes like an index over the keys in the key/value pairs contained in the map record.\n\n\nSee \nHash Keys\n for a detailed discussion of hash keys.\n\n\nCircular References\n\n\nCircular references are not allowed in Hollow.  A type may not reference itself, either directly or transitively.\n\n\nObject Memory Layout\n\n\nOn consumers, \nINT\n and \nLONG\n fields are each represented by a number of bits exactly sufficient to represent the maximum value for the field across all records.  \nFLOAT\n, \nDOUBLE\n, and \nBOOLEAN\n fields are represented by 32, 64, and 2 bits, respectively.  \nSTRING\n and \nBYTES\n fields use a variable number of bytes for each record.  \nREFERENCE\n fields encode the \nordinal\n of referenced records, and are represented by a number of bits exactly sufficient to encode the maximum ordinal of the referenced type.  See \nIn-memory Data Layout\n for more details.\n\n\n\n\nAvoid Outlier Values\n\n\nTry to model your data such that there aren't any outlier values for \nINT\n and \nLONG\n fields.  Also, avoid \nFLOAT\n and \nDOUBLE\n fields where possible, since these field types are relatively expensive.\n\n\n\n\nMaintaining Backwards Compatibility\n\n\nA data model will evolve over time.  The following operations will not impact the interoperability between existing clients and new data:\n\n\n\n\nAdding a new type\n\n\nRemoving an existing type\n\n\nAdding a new field to an existing type\n\n\nRemoving an existing field from an existing type.\n\n\n\n\nWhen adding new fields or types, existing generated client APIs will ignore the new fields, and all of the fields which existed at the time of API generation will still be visible using the same methods.  When removing fields, existing generated client APIs will see null values if the methods corresponding to the removed fields are called.  When removing types, existing generated client APIs will see removed types as having no records.\n\n\nIt is not backwards compatible to change the type of an existing field.  The client behavior when calling a method corresponding to a field with a changed type is undefined.\n\n\nIt is not backwards compatible to change the primary key or hash key for any type.\n\n\nBeyond the specification of Hollow itself, backwards compatibility often has a lot to do with the use case and semantics of the data. Hollow will always behave in the stated way for evolving data models, but it\u2019s possible that consumers require a field which starts returning null once it gets removed.  For this reason, additional caution should be exercised when removing types and fields.\n\n\n\n\nBackwards-incompatible data remodeling\n\n\nEvery so often, it may be required or desirable to make changes to the data model which are incompatible with prior versions.  In this case, an older producer, which produces the older data model, should run in parallel with the newer producer, producing the newer incompatible data model.  Each producer should write its blobs to a different \nnamespace\n, so that older consumers can read from the old data model, and newer consumers can read from the newer data model.  Once all consumers are upgraded and reading from the newer data model, the older producer can be decommissioned.", 
            "title": "Data Modeling"
        }, 
        {
            "location": "/data-modeling/#schemas", 
            "text": "A Hollow data model is a set of schemas, which are usually defined by the POJOs used on the producer to  populate the data .  This section will use POJOs as examples, but there are other ways to define schemas -- for example you could ingest a text file and use the  schema parser .   Schemas Define the Data Model  A hollow dataset is comprised of one or more data  types .  The  data model  for a dataset is defined by the schemas describing those types.", 
            "title": "Schemas"
        }, 
        {
            "location": "/data-modeling/#object-schemas", 
            "text": "Each POJO class you define will result in an  Object  schema, which is a fixed set of strongly typed fields.  The fields will be based on the member variables in the class.  For example, the class  Movie  will define an  Object  schema with three fields:  public class Movie {\n    int movieId;\n    String title;\n    Set Actor  actors;\n}  Each schema has a  type name .  The name of the type will default to the simple name of your POJO -- in this case  Movie .    Each schema field has a  field name , which will default to the same name as the field in the POJO -- in this case  movieId ,  title , and  actors .  Each field also has a  field type , which is in this case  INT ,  REFERENCE , and  REFERENCE , respectively.  Each  REFERENCE  field also indicates the  referenced type , which for our reference fields above default to  String  and  SetOfActor .  The possible field types are:   INT : An integer value up to 32-bits  LONG : An integer value up to 64-bits  FLOAT : A 32-bit floating-point value  DOUBLE : A 64-bit floating-point value  BOOLEAN :  true  or  false  STRING : An array of characters  BYTES : An array of bytes  REFERENCE : A reference to another specific type.  The referenced type must be defined by the schema.   Notice that since the reference type is  defined by the schema , data models must be strongly typed.  Each reference in your data model must point to a specific concrete implementation.  References to interfaces, abstract classes, or  java.lang.Object  are not supported.", 
            "title": "Object Schemas"
        }, 
        {
            "location": "/data-modeling/#primary-keys", 
            "text": "Object  schemas may specify a primary key.  This is accomplished by using the  @HollowPrimaryKey  annotation and specifying the fields.  @HollowPrimaryKey(fields={ movieId })\npublic class Movie {\n    int movieId;\n    String title;\n    Set Actor  actors;\n}  When defined in the schema, primary keys are a part of your data model and drive useful functionality and default configuration in the  hollow explorer ,  hollow history , and  diff ui .  They also provide a shortcut when creating a  primary key index .  Primary keys defined in the schema follow the same convention as primary keys defined for indexes.  They consist of one or more  field paths , which will auto-expand if they terminate in a  REFERENCE  field.", 
            "title": "Primary Keys"
        }, 
        {
            "location": "/data-modeling/#inlined-vs-referenced-fields", 
            "text": "We can  inline  some fields in our POJOs so that they are no longer  REFERENCE  fields, but instead encode their data directly in each record:  public class Movie {\n    int movieId;\n    @HollowInline String title;\n    Set Actor  actors;\n}  In the above example, our fields are now of type  INT ,  STRING , and  REFERENCE .  While modeling data, we choose whether or not to inline a field for efficiency.  Consider the following type:  public class Award {\n    String awardName;\n    long movieId;\n    long actorId;\n}  In this case, imagine  awardName  is something like \u201cBest Supporting Actress\u201d.  Over the years, many such awards will be given, so we\u2019ll have a lot of records which share that value.  If we use an  inlined   STRING  field, then the value \"Best Supporting Actress\" will be repeated for every such award record.  However, if we reference a separate record type, all such awards will reference the same child record with that value.  If the  awardName  values have a lot of repetition, then this can result in a significant savings.   Deduplication  Record deduplication happens automatically at the  record  granularity in Hollow.  Try to model your data such that when there is a lot of repetition in records, the repetitive fields are encapsulated into their own types.   To consider the opposite case, let\u2019s examine the following  Actor  type:  public class Actor {\n    long id;\n    @HollowInline String actorName;\n}  The  actorName  is unlikely to be repeated often.  In this case, if we reference a separate record type, we have to retain roughly the same number of unique character strings  plus  we need to retain references to those records.  In this case, we end up saving space by using an inlined  STRING  field instead of a reference to a separate type.   Reference Costs  A  REFERENCE  field isn't free, and therefore we shouldn't necessarily try to encapsulate fields inside their own record types where we won't benefit from deduplication.  These fields should instead be  inlined .   We refer to fields which are defined with native Hollow types as  inlined  fields, and fields which are defined as references to types with a single field as  referenced  fields.", 
            "title": "Inlined vs Referenced Fields"
        }, 
        {
            "location": "/data-modeling/#namespaced-record-type-names", 
            "text": "In order to be very efficient, referenced types sometimes should be  namespaced  so that fields with like values may reference the same  record type , but reference fields of the same  primitive type  elsewhere in the data model use different  record types .  For example, consider our  Award  type again, but this time, we\u2019ll reference a type called  AwardName , instead of  String .  We can explicitly name the  type  of a field with the  @HollowTypeName  annotation:  public class Award {\n    @HollowTypeName(name= AwardName )\n    String awardName;\n    long movieId;\n    long actorId;\n}  Other types in our data model which reference award names can reuse the  AwardName  type.  Other referenced string fields in our data model, which are unrelated to award names, should use different types corresponding to the semantics of their values.    Namespacing fields saves space because references to types with a lower cardinality use fewer bits than references to types with a higher cardinality.  The reason for this can be gleaned from the  In-Memory Data Layout  topic underneath the  Advanced Topics  section.  Namespacing fields is also useful if some consumers don't need the contents of a specific referenced field.  If a type is namespaced, it can be selectively  filtered , whereas if it is grouped with other fields which  are  needed by all consumers, then it cannot be selected for filtering.   Namespacing Reduces Reference Costs  Using an appropriately  namespaced  type reduces the heap footprint cost of  REFERENCE  fields.    Changing default  type names  The  @HollowTypeName  annotation can also be used at the class level to select a default type name for a class other than its simple name.", 
            "title": "Namespaced Record Type Names"
        }, 
        {
            "location": "/data-modeling/#grouping-associated-fields", 
            "text": "Referencing fields can save space because the same field values do not have to be repeated for every record in which they occur.  Similarly, we can  group  fields which have covarying values, and pull these out from larger objects as their own types.  For example, imagine we started with a  Movie  type which included the following fields:  public class Movie {\n    long id;\n    String title;\n    String maturityRating;\n    String advisories;\n}  We might notice that the  maturityRating  and  advisories  fields vary together, and are often the repeated across many  Movie  records.  We can pull out a separate type for these fields:  public class Movie {\n    long id;\n    String title;\n    MaturityRating maturityRating;\n}\n\npublic class MaturityRating {\n    string rating;\n    string advisories;\n}  We could have referenced these fields separately.  If we had done so, each  Movie  record, of which there are probably many, would have had to contain two separate references for these fields.  Instead, by recognizing that these fields were associated and pulling them together, space is saved because each  Movie  record now only contains one reference for this data.", 
            "title": "Grouping Associated Fields"
        }, 
        {
            "location": "/data-modeling/#list-schemas", 
            "text": "You can define  List  schemas by adding a member variable of type  List  in your data model.  For example:  public class Movie {\n    long id;\n    String title;\n    List Award  awardsReceived;\n}  The  List  must explicitly define its parameterized element type.  The default  type name  of the above  List  schema will be  ListOfAward .    A  List  schema indicates a record type which is an ordered collection of  REFERENCE  fields.  Each record will have a variable number of references.  The referenced type must be defined by the schema, and all references in all records will encode only the  ordinals  of the referenced records as the values for these references.", 
            "title": "List Schemas"
        }, 
        {
            "location": "/data-modeling/#set-schemas", 
            "text": "You can define  Set  schemas by adding a member variable of type  Set  in your data model.  The  Set  must explicitly define its parameterized element type.  A  Set  schema indicates a record type which is an unordered collection of  REFERENCE  fields.  Each record will have a variable number of references, and the referenced type must be defined by the schema.  Within a single set record, each reference must be unique.    References in  Set  records can be hashed by some specific element fields for O(1) retrieval.  In order to enable this feature, a  Set  schema will define an optional  hash key , which defines how its elements are hashed/indexed.", 
            "title": "Set Schemas"
        }, 
        {
            "location": "/data-modeling/#map-schemas", 
            "text": "You can define  Map  schemas by adding a member variable of type  Map  in your data model.  The  Map  must explicitly define it parameterized key and values types.    A  Map  schema indicates a record type which is an unordered collection of pairs of  REFERENCE  fields, used to represent a key/value mapping.  Each record will have a variable number of key/value pairs.  Both the key reference type and the value reference type must be defined by the schema.  The key reference type does not have to be the same as the value reference type.  Within a single map record, each key reference must be unique.    Entries in  Map  records can be hashed by some specific key fields for O(1) retrieval of the keys, values, and/or entries.  In order to enable this feature, a  Map  schema will define an optional  hash key , which defines how its entries are hashed/indexed.", 
            "title": "Map Schemas"
        }, 
        {
            "location": "/data-modeling/#hash-keys", 
            "text": "Each  Map  and  Set  schema may optionally define a  hash key .  A  hash key  specifies one or more user-defined fields used to hash entries into the collection.  When a hash key is defined on a  Set , each set record becomes like a primary key index; records in the set can be efficiently retrieved by matching the specified  hash key  fields.  Similarly, when a hash key is defined on a  Map , each map record becomes like an index over the keys in the key/value pairs contained in the map record.  See  Hash Keys  for a detailed discussion of hash keys.", 
            "title": "Hash Keys"
        }, 
        {
            "location": "/data-modeling/#circular-references", 
            "text": "Circular references are not allowed in Hollow.  A type may not reference itself, either directly or transitively.", 
            "title": "Circular References"
        }, 
        {
            "location": "/data-modeling/#object-memory-layout", 
            "text": "On consumers,  INT  and  LONG  fields are each represented by a number of bits exactly sufficient to represent the maximum value for the field across all records.   FLOAT ,  DOUBLE , and  BOOLEAN  fields are represented by 32, 64, and 2 bits, respectively.   STRING  and  BYTES  fields use a variable number of bytes for each record.   REFERENCE  fields encode the  ordinal  of referenced records, and are represented by a number of bits exactly sufficient to encode the maximum ordinal of the referenced type.  See  In-memory Data Layout  for more details.   Avoid Outlier Values  Try to model your data such that there aren't any outlier values for  INT  and  LONG  fields.  Also, avoid  FLOAT  and  DOUBLE  fields where possible, since these field types are relatively expensive.", 
            "title": "Object Memory Layout"
        }, 
        {
            "location": "/data-modeling/#maintaining-backwards-compatibility", 
            "text": "A data model will evolve over time.  The following operations will not impact the interoperability between existing clients and new data:   Adding a new type  Removing an existing type  Adding a new field to an existing type  Removing an existing field from an existing type.   When adding new fields or types, existing generated client APIs will ignore the new fields, and all of the fields which existed at the time of API generation will still be visible using the same methods.  When removing fields, existing generated client APIs will see null values if the methods corresponding to the removed fields are called.  When removing types, existing generated client APIs will see removed types as having no records.  It is not backwards compatible to change the type of an existing field.  The client behavior when calling a method corresponding to a field with a changed type is undefined.  It is not backwards compatible to change the primary key or hash key for any type.  Beyond the specification of Hollow itself, backwards compatibility often has a lot to do with the use case and semantics of the data. Hollow will always behave in the stated way for evolving data models, but it\u2019s possible that consumers require a field which starts returning null once it gets removed.  For this reason, additional caution should be exercised when removing types and fields.   Backwards-incompatible data remodeling  Every so often, it may be required or desirable to make changes to the data model which are incompatible with prior versions.  In this case, an older producer, which produces the older data model, should run in parallel with the newer producer, producing the newer incompatible data model.  Each producer should write its blobs to a different  namespace , so that older consumers can read from the old data model, and newer consumers can read from the newer data model.  Once all consumers are upgraded and reading from the newer data model, the older producer can be decommissioned.", 
            "title": "Maintaining Backwards Compatibility"
        }, 
        {
            "location": "/diving-deeper/", 
            "text": "The following section details some lower-level concepts which will provide the backdrop for some more advanced usage of Hollow.  If we didn't have a \nHollowProducer\n or \nHollowConsumer\n, this section details how we would use Hollow.\n\n\nState Engines\n\n\nBoth the \nHollowProducer\n and \nHollowConsumer\n handle datasets with a \nstate engine\n.  A state engine can be transitioned between data states.  A producer uses a \nwrite state engine\n and a consumer uses a \nread state engine\n.  \n\n\n\n\nA \nHollowReadStateEngine\n can be obtained from a \nHollowConsumer\n via the method \ngetStateEngine()\n.\n\n\nA \nHollowWriteStateEngine\n can be obtained from a \nHollowProducer\n via the method \ngetWriteEngine()\n.\n\n\n\n\nOrdinals\n\n\nEach record in a Hollow data state is assigned to a specific \nordinal\n, which is an integer value. An \nordinal\n:\n\n\n\n\nis a unique identifier of the record within a type.\n\n\nis sufficient to locate the record within a type.\n\n\n\n\nOrdinals are automatically assigned by Hollow. They lie in the range of 0-n, where n is generally not much larger than the total number of records for the type.  In lower-level usage of Hollow, ordinals are often used as proxies for handles to specific records.\n\n\nGiven a \nHollowReadStateEngine\n, you can retrieve the set of currently populated ordinals using the call \nstateEngine.getTypeState(\"TypeName\").getPopulatedOrdinals()\n.  A \nBitSet\n containing all of the populated ordinals is returned.  Similarly, the ordinals which were populated prior to the last delta transition can be obtained using \nstateEngine.getTypeState(\"TypeName\").getPreviousOrdinals()\n.\n\n\n\n\nPopulated Ordinals\n\n\nNever modify the \nBitSet\n returned from \ngetPopulatedOrdinals()\n or \ngetPreviousOrdinals()\n.  Modifying these may corrupt the data store.\n\n\n\n\nIt's useful to note that records in Hollow are immutable.  They will never be \nmodified\n, only removed and added.  A \nmodification\n probably means that within the same delta there was a removal of a record keyed by some value and an addition of a new record keyed by the same value. \n\n\nOrdinals have some useful properties:\n\n\n\n\nIt is guaranteed that if an exactly equivalent record exists in two adjacent states, then that record will retain the same ordinal. If, on the other hand, a record does not have an exact equivalent in an adjacent state, then its ordinal will not be populated in the state in which it does not exist.\n\n\nAfter a single delta transition has been applied which removes a record, that record will be marked as not populated, but the data for that record will still be accessible at that ordinal until the \nnext\n delta transition.  We call these records \nghost records\n.\n\n\n\n\nWriting a Data Snapshot (low-level)\n\n\nLet's assume we have a POJO class \nMovie\n:\n\n\npublic class Movie {\n    long id;\n    String title;\n    int releaseYear;\n\n    public Movie(long id, String title, int releaseYear) {\n        this.id = id;\n        this.title = title;\n        this.releaseYear = releaseYear;\n    }\n}\n\n\n\n\nIn order to create a new data state and write it to disk, we can use a \nHollowWriteStateEngine\n directly:\n\n\nHollowWriteStateEngine writeEngine = new HollowWriteStateEngine();\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\n\nfor(Movie movie : movies)\n    mapper.addObject(movie);\n\nOutputStream os = ...; /// where to write the blob\nHollowBlobWriter writer = new HollowBlobWriter(writeEngine);\nwriter.writeSnapshot(os);\n\n\n\n\nA \nHollowWriteStateEngine\n is the main handle to a Hollow dataset for a data producer.  A \nHollowObjectMapper\n is one of a few different ways to populate a \nHollowWriteStateEngine\n with data.  When starting with POJOs, it's the easiest way.\n\n\nWe'll use a \nHollowBlobWriter\n to write the current state of a \nHollowWriteStateEngine\n to an \nOutputStream\n.  We call the data which gets written to the \nOutputStream\n a \nblob\n.  \n\n\nReading a Data Snapshot (low-level)\n\n\nA data consumer can load a snapshot created by the producer into memory:\n\n\nHollowReadStateEngine readEngine = new HollowReadStateEngine();\nHollowBlobReader reader = new HollowBlobReader(readEngine);\n\nInputStream is = /// where to load the snapshot from\nreader.readSnapshot(is);\n\n\n\n\nA \nHollowReadStateEngine\n is our main handle to a Hollow dataset as a consumer.  A \nHollowBlobReader\n is used to consume blobs into a \nHollowReadStateEngine\n.  Above, we're consuming a snapshot blob in order to initialize our state engine.  \n\n\nOnce this dataset is loaded into memory, we can access the data for any records using our \ngenerated API\n:\n\n\nMovieAPI movieApi = new MovieAPI(readEngine);\n\nfor(Movie movie : movieApi.getAllMovieHollow()) {\n    /// do something for each Movie record\n}\n\n\n\n\nWriting a Delta (low-level)\n\n\nSome time has passed and the dataset has evolved.  The producer, with the same \nHollowWriteStateEngine\n in memory, needs to communicate this updated dataset to consumers.  The data for the new state must be added to the state engine, after which a transition from the previous state to the new state can be written as a \ndelta\n blob:\n\n\nwriteEngine.prepareForNextCycle();\n\nfor(Movie movie : movies)\n    mapper.addObject(movie);\n\nOutputStream os = ....; /// where to write the delta blob\nwriter.writeDelta(os);\n\n\n\n\nLet's take a closer look at what the above code does.  The same \nHollowWriteStateEngine\n which was used to produce the \nsnapshot\n blob is used -- it already knows everything about the prior state and can be transitioned to the next state.  We call \nprepareForNextCycle()\n to inform the state engine that the writing of blobs from the prior state is complete, and populating data into the next state is about to begin.  When creating a new state, all of the movies currently in our dataset are re-added again.  It's not necessary to figure out which records were added, removed, or modified -- that's Hollow's job.\n\n\nWe can (but don't have to) use the same \nHollowObjectMapper\n and/or \nHollowBlobWriter\n as we used in the prior \ncycle\n to create the initial snapshot.  \n\n\nThe call to \nwriteDelta()\n records a \ndelta\n blob to the \nOutputStream\n.  Encoded into the delta is a set of instructions to update a consumer\u2019s read state engine from the previous state to the current state.\n\n\n\n\nReverse Deltas\n\n\nJust as you can call \nwriteDelta()\n to write a delta from one state to the next, you can also call \nwriteReverseDelta()\n to write the reverse operation which will take you from the next state to the prior state.\n\n\n\n\nReading a Delta (low-level)\n\n\nOnce a delta is available the HollowReadStateEngine can be updated on the client:\n\n\nInputStream is = /// where to load the delta from\nHollowBlobReader reader = new HollowBlobReader(readEngine);\nreader.applyDelta(is);\n\n\n\n\nThe same \nHollowReadStateEngine\n into which our snapshot was consumed must be reused to consume a \ndelta\n blob.  This state engine knows everything about the current state and can use the instructions in a delta to transition to the next state.  We can (but don't have to) reuse the same \nHollowBlobReader\n.\n\n\nAfter this delta has been applied, the read state engine is at the new state.  \n\n\n\n\nThread Safety\n\n\nIt is safe to use the HollowReadStateEngine to retrieve data while a delta transition is in progress.\n\n\n\n\n\n\nDelta Mismatch\n\n\nIf a delta application is attempted onto a \nHollowReadStateEngine\n which is at a state from which the delta did not originate, then an exception is thrown and the state engine remains safely unchanged.\n\n\n\n\nIndexing Data for Retrieval\n\n\nIn prior examples the generated Hollow API was used by the data consumer to iterate over all \nMovie\n records in the dataset.  Most often, however, it isn\u2019t desirable to iterate over the entire dataset \u2014 instead, specific records will be accessed based on some known key.  Let\u2019s assume that the \nMovie\n\u2019s id is a known key.\n\n\nAfter consumers have populated a \nHollowReadStateEngine\n, the data can be indexed:\n\n\nHollowPrimaryKeyIndex idx = \n                      new HollowPrimaryKeyIndex(readEngine, \nMovie\n, \nid\n);\n\nidx.listenForDeltaUpdates();\n\n\n\n\nThis index can be held in memory and then used in conjunction with the generated Hollow API to retrieve Movie records by id:\n\n\nint movieOrdinal = idx.getMatchingOrdinal(2);\nif(movieOrdinal != -1) {\n    MovieHollow movie = movieApi.getMovieHollow(movieOrdinal);\n    System.out.println(\nFound Movie: \n + movie._getTitle()._getValue());\n}\n\n\n\n\nWhich outputs:\n\n\nFound Movie: Beasts of No Nation\n\n\n\n\n\n\nKeeping an Index Up To Date\n\n\nThe call to \nlistenForDeltaUpdates()\n will cause a \nHollowPrimaryKeyIndex\n to automatically stay updated when deltas are applied to the indexed \nHollowReadStateEngine\n, but this should only be called if you intend to keep the index around.  See the Indexing / Querying section for usage details.\n\n\n\n\n\n\nThread Safety\n\n\nRetrievals from a \nHollowPrimaryKeyIndex\n are thread-safe.  It is safe to use a \nHollowPrimaryKeyIndex\n from multiple threads, and it is safe to query while a transition is in progress.\n\n\n\n\n\n\nOrdinals\n\n\nSee \nordinals\n for a discussion about ordinals.\n\n\n\n\nHollowPrimaryKeyIndex\n\n\nIn the above example, the primary key is defined for \nMovie\n as its \nid\n field.  A primary key can also be defined over multiple and/or hierarchical fields.  Imagine that \nMovie\n additionally had a \ncountry\n field defined in its schema, and that across countries, \nMovie\n \nid\ns may be duplicated, but that there will never exist two \nMovie\n records with the same id and country:\n\n\n@HollowPrimaryKey(fields={\nid\n, \ncountry.id\n})\npublic class Movie {\n    long id;\n    Country country;\n    ...\n}\n\n@HollowPrimaryKey(fields={\nid\n})\npublic class Country {\n    String id;\n    String name;\n}\n\n\n\n\nA \nHollowPrimaryKeyIndex\n can be defined with a primary key consisting of both fields:\n\n\nHollowPrimaryKeyIndex idx = \n            new HollowPrimaryKeyIndex(readEngine, \nMovie\n, \nid\n, \ncountry.id.value\n);\nidx.listenForDeltaUpdates();\n\n\n\n\nAnd to query for a \nMovie\n based on its id and country:\n\n\nint movieOrdinal = idx.getMatchingOrdinal(2, \nUS\n);\nif(movieOrdinal != -1) {\n    Movie movie = movieApi.getMovie(movieOrdinal);\n    System.out.println(\nFound Movie: \n + movie.getTitle().getValue());\n}\n\n\n\n\nNotice that \nMovie\n\u2019s country field in the above example is actually a \nREFERENCE\n field.  The defined key includes the id of the movie, and the value of the id String of the referenced country.  We denote this traversal using dot notation in the primary key definition.  The field definitions can be multiple references deep.\n\n\nThe requirement for a primary key definition is that no duplicates should exist for the defined combination of fields.  If this rule is violated, an arbitrary match will be returned for queries when multiple matches exist.  \n\n\n\n\nPrimary Key Violations\n\n\nViolations of the \"no duplicate\" primary key rule can be detected using the \ngetDuplicateKeys()\n method on a \nHollowPrimaryKeyIndex\n, which returns a \nCollection\nObject[]\n.  If no duplicate keys exist, the returned Collection will be empty.  If they do, the returned values will indicate the keys for which duplicate records exist.\n\n\n\n\nIf a \nHollowPrimaryKeyIndex\n will be retained for a long duration, they should be kept updated as deltas are applied to the underlying \nHollowReadStateEngine\n.  This is accomplished with a single call after instantiation to the \nlistenForDeltaUpdates()\n method. \n\n\n\n\nDetaching Primary Key Indexes\n\n\nIf \nlistenForDeltaUpdates()\n is called on a primary key index, then it cannot be garbage collected.  If you intend to drop an index which is listening for updates, first call \ndetachFromDeltaUpdates()\n to prevent a memory leak.\n\n\n\n\nIndexes which are listening for delta updates are updated after a dataset is updated.  In the brief interim time between when a dataset is updated and the index is updated, the index will point to the \nghost records\n located at tombstoned ordinals.  This helps guarantee that all in-flight operations will observe correct data.\n\n\nHollowHashIndex\n\n\nIt is sometimes desirable to index records by fields other than primary keys.  The \nHollowHashIndex\n allows for indexing records by fields or combinations of fields for which values may match multiple records, and records may match multiple values.\n\n\nIn our \nMovie\n/\nActor\n example, we may want to index movies by their starring actors:\n\n\nHollowHashIndex idx = \n            new HollowHashIndex(readEngine, \nMovie\n, \n, \ncast.element.actor.actorId\n);\n\n\n\n\nThe \nHollowHashIndex\n expects in its constructor arguments a query start type, a select field, and a set of match fields.  The constructor arguments above indicate that queries will start with the \nMovie\n type, select the root of the query (indicated by the empty string), and match the id of any \nActor\n record in the actors list.\n\n\nTo query this index:\n\n\nHollowHashIndexResult result = idx.findMatches(102);\n\nif(result != null) {\n    System.out.println(\nFound matches: \n + result.numResults());\n\n    HollowOrdinalIterator iter = result.iterator();\n    int matchedOrdinal = iter.next();\n    while(matchedOrdinal != HollowOrdinalIterator.NO_MORE_ORDINALS) {\n        Movie movie = api.getMovie(matchedOrdinal);\n        System.out.println(\nStarred in: \n + movie.getTitle().getValue());\n        matchedOrdinal = iter.next();\n    }\n}\n\n\n\n\nAlternatively, if the data model included the nationality of actors, and we needed to index actors by nationality and the titles of movies in which they starred:\n\n\nHollowHashIndex idx = \n            new HollowHashIndex(readEngine, \nMovie\n, \ncast.element.actor\n,\n                                            \ntitle.value\n,\n                                            \ncast.element.actor.nationality.id.value\n);\n\n\n\n\nIn this case, the query start type is still \nMovie\n, but we\u2019re selecting related \nActor\n records.  Matches are selected based on the \nMovie\n\u2019s title, and the actor\u2019s nationality.  Using this index, one can query for Brazilian actors who starred in movies titled \u201cNarcos\u201d:\n\n\nHollowHashIndexResult result = idx.findMatches(\nNarcos\n, \nBR\n);\n\nif(result != null) {\n    HollowOrdinalIterator iter = result.iterator();\n    int matchedOrdinal = iter.next();\n    while(matchedOrdinal != HollowOrdinalIterator.NO_MORE_ORDINALS) {\n        Actor actor = api.getMovie(matchedOrdinal);\n        System.out.println(\nMatched actor: \n + \n                                      actor.getActorName().getValue());\n        matchedOrdinal = iter.next();\n    }\n}\n\n\n\n\nThe \nHollowHashIndex\n does not yet have a facility for listening for delta updates.  If an index is necessary across multiple states, currently the index must be recreated on each update.", 
            "title": "Diving Deeper"
        }, 
        {
            "location": "/diving-deeper/#state-engines", 
            "text": "Both the  HollowProducer  and  HollowConsumer  handle datasets with a  state engine .  A state engine can be transitioned between data states.  A producer uses a  write state engine  and a consumer uses a  read state engine .     A  HollowReadStateEngine  can be obtained from a  HollowConsumer  via the method  getStateEngine() .  A  HollowWriteStateEngine  can be obtained from a  HollowProducer  via the method  getWriteEngine() .", 
            "title": "State Engines"
        }, 
        {
            "location": "/diving-deeper/#ordinals", 
            "text": "Each record in a Hollow data state is assigned to a specific  ordinal , which is an integer value. An  ordinal :   is a unique identifier of the record within a type.  is sufficient to locate the record within a type.   Ordinals are automatically assigned by Hollow. They lie in the range of 0-n, where n is generally not much larger than the total number of records for the type.  In lower-level usage of Hollow, ordinals are often used as proxies for handles to specific records.  Given a  HollowReadStateEngine , you can retrieve the set of currently populated ordinals using the call  stateEngine.getTypeState(\"TypeName\").getPopulatedOrdinals() .  A  BitSet  containing all of the populated ordinals is returned.  Similarly, the ordinals which were populated prior to the last delta transition can be obtained using  stateEngine.getTypeState(\"TypeName\").getPreviousOrdinals() .   Populated Ordinals  Never modify the  BitSet  returned from  getPopulatedOrdinals()  or  getPreviousOrdinals() .  Modifying these may corrupt the data store.   It's useful to note that records in Hollow are immutable.  They will never be  modified , only removed and added.  A  modification  probably means that within the same delta there was a removal of a record keyed by some value and an addition of a new record keyed by the same value.   Ordinals have some useful properties:   It is guaranteed that if an exactly equivalent record exists in two adjacent states, then that record will retain the same ordinal. If, on the other hand, a record does not have an exact equivalent in an adjacent state, then its ordinal will not be populated in the state in which it does not exist.  After a single delta transition has been applied which removes a record, that record will be marked as not populated, but the data for that record will still be accessible at that ordinal until the  next  delta transition.  We call these records  ghost records .", 
            "title": "Ordinals"
        }, 
        {
            "location": "/diving-deeper/#writing-a-data-snapshot-low-level", 
            "text": "Let's assume we have a POJO class  Movie :  public class Movie {\n    long id;\n    String title;\n    int releaseYear;\n\n    public Movie(long id, String title, int releaseYear) {\n        this.id = id;\n        this.title = title;\n        this.releaseYear = releaseYear;\n    }\n}  In order to create a new data state and write it to disk, we can use a  HollowWriteStateEngine  directly:  HollowWriteStateEngine writeEngine = new HollowWriteStateEngine();\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\n\nfor(Movie movie : movies)\n    mapper.addObject(movie);\n\nOutputStream os = ...; /// where to write the blob\nHollowBlobWriter writer = new HollowBlobWriter(writeEngine);\nwriter.writeSnapshot(os);  A  HollowWriteStateEngine  is the main handle to a Hollow dataset for a data producer.  A  HollowObjectMapper  is one of a few different ways to populate a  HollowWriteStateEngine  with data.  When starting with POJOs, it's the easiest way.  We'll use a  HollowBlobWriter  to write the current state of a  HollowWriteStateEngine  to an  OutputStream .  We call the data which gets written to the  OutputStream  a  blob .", 
            "title": "Writing a Data Snapshot (low-level)"
        }, 
        {
            "location": "/diving-deeper/#reading-a-data-snapshot-low-level", 
            "text": "A data consumer can load a snapshot created by the producer into memory:  HollowReadStateEngine readEngine = new HollowReadStateEngine();\nHollowBlobReader reader = new HollowBlobReader(readEngine);\n\nInputStream is = /// where to load the snapshot from\nreader.readSnapshot(is);  A  HollowReadStateEngine  is our main handle to a Hollow dataset as a consumer.  A  HollowBlobReader  is used to consume blobs into a  HollowReadStateEngine .  Above, we're consuming a snapshot blob in order to initialize our state engine.    Once this dataset is loaded into memory, we can access the data for any records using our  generated API :  MovieAPI movieApi = new MovieAPI(readEngine);\n\nfor(Movie movie : movieApi.getAllMovieHollow()) {\n    /// do something for each Movie record\n}", 
            "title": "Reading a Data Snapshot (low-level)"
        }, 
        {
            "location": "/diving-deeper/#writing-a-delta-low-level", 
            "text": "Some time has passed and the dataset has evolved.  The producer, with the same  HollowWriteStateEngine  in memory, needs to communicate this updated dataset to consumers.  The data for the new state must be added to the state engine, after which a transition from the previous state to the new state can be written as a  delta  blob:  writeEngine.prepareForNextCycle();\n\nfor(Movie movie : movies)\n    mapper.addObject(movie);\n\nOutputStream os = ....; /// where to write the delta blob\nwriter.writeDelta(os);  Let's take a closer look at what the above code does.  The same  HollowWriteStateEngine  which was used to produce the  snapshot  blob is used -- it already knows everything about the prior state and can be transitioned to the next state.  We call  prepareForNextCycle()  to inform the state engine that the writing of blobs from the prior state is complete, and populating data into the next state is about to begin.  When creating a new state, all of the movies currently in our dataset are re-added again.  It's not necessary to figure out which records were added, removed, or modified -- that's Hollow's job.  We can (but don't have to) use the same  HollowObjectMapper  and/or  HollowBlobWriter  as we used in the prior  cycle  to create the initial snapshot.    The call to  writeDelta()  records a  delta  blob to the  OutputStream .  Encoded into the delta is a set of instructions to update a consumer\u2019s read state engine from the previous state to the current state.   Reverse Deltas  Just as you can call  writeDelta()  to write a delta from one state to the next, you can also call  writeReverseDelta()  to write the reverse operation which will take you from the next state to the prior state.", 
            "title": "Writing a Delta (low-level)"
        }, 
        {
            "location": "/diving-deeper/#reading-a-delta-low-level", 
            "text": "Once a delta is available the HollowReadStateEngine can be updated on the client:  InputStream is = /// where to load the delta from\nHollowBlobReader reader = new HollowBlobReader(readEngine);\nreader.applyDelta(is);  The same  HollowReadStateEngine  into which our snapshot was consumed must be reused to consume a  delta  blob.  This state engine knows everything about the current state and can use the instructions in a delta to transition to the next state.  We can (but don't have to) reuse the same  HollowBlobReader .  After this delta has been applied, the read state engine is at the new state.     Thread Safety  It is safe to use the HollowReadStateEngine to retrieve data while a delta transition is in progress.    Delta Mismatch  If a delta application is attempted onto a  HollowReadStateEngine  which is at a state from which the delta did not originate, then an exception is thrown and the state engine remains safely unchanged.", 
            "title": "Reading a Delta (low-level)"
        }, 
        {
            "location": "/diving-deeper/#indexing-data-for-retrieval", 
            "text": "In prior examples the generated Hollow API was used by the data consumer to iterate over all  Movie  records in the dataset.  Most often, however, it isn\u2019t desirable to iterate over the entire dataset \u2014 instead, specific records will be accessed based on some known key.  Let\u2019s assume that the  Movie \u2019s id is a known key.  After consumers have populated a  HollowReadStateEngine , the data can be indexed:  HollowPrimaryKeyIndex idx = \n                      new HollowPrimaryKeyIndex(readEngine,  Movie ,  id );\n\nidx.listenForDeltaUpdates();  This index can be held in memory and then used in conjunction with the generated Hollow API to retrieve Movie records by id:  int movieOrdinal = idx.getMatchingOrdinal(2);\nif(movieOrdinal != -1) {\n    MovieHollow movie = movieApi.getMovieHollow(movieOrdinal);\n    System.out.println( Found Movie:   + movie._getTitle()._getValue());\n}  Which outputs:  Found Movie: Beasts of No Nation   Keeping an Index Up To Date  The call to  listenForDeltaUpdates()  will cause a  HollowPrimaryKeyIndex  to automatically stay updated when deltas are applied to the indexed  HollowReadStateEngine , but this should only be called if you intend to keep the index around.  See the Indexing / Querying section for usage details.    Thread Safety  Retrievals from a  HollowPrimaryKeyIndex  are thread-safe.  It is safe to use a  HollowPrimaryKeyIndex  from multiple threads, and it is safe to query while a transition is in progress.    Ordinals  See  ordinals  for a discussion about ordinals.", 
            "title": "Indexing Data for Retrieval"
        }, 
        {
            "location": "/diving-deeper/#hollowprimarykeyindex", 
            "text": "In the above example, the primary key is defined for  Movie  as its  id  field.  A primary key can also be defined over multiple and/or hierarchical fields.  Imagine that  Movie  additionally had a  country  field defined in its schema, and that across countries,  Movie   id s may be duplicated, but that there will never exist two  Movie  records with the same id and country:  @HollowPrimaryKey(fields={ id ,  country.id })\npublic class Movie {\n    long id;\n    Country country;\n    ...\n}\n\n@HollowPrimaryKey(fields={ id })\npublic class Country {\n    String id;\n    String name;\n}  A  HollowPrimaryKeyIndex  can be defined with a primary key consisting of both fields:  HollowPrimaryKeyIndex idx = \n            new HollowPrimaryKeyIndex(readEngine,  Movie ,  id ,  country.id.value );\nidx.listenForDeltaUpdates();  And to query for a  Movie  based on its id and country:  int movieOrdinal = idx.getMatchingOrdinal(2,  US );\nif(movieOrdinal != -1) {\n    Movie movie = movieApi.getMovie(movieOrdinal);\n    System.out.println( Found Movie:   + movie.getTitle().getValue());\n}  Notice that  Movie \u2019s country field in the above example is actually a  REFERENCE  field.  The defined key includes the id of the movie, and the value of the id String of the referenced country.  We denote this traversal using dot notation in the primary key definition.  The field definitions can be multiple references deep.  The requirement for a primary key definition is that no duplicates should exist for the defined combination of fields.  If this rule is violated, an arbitrary match will be returned for queries when multiple matches exist.     Primary Key Violations  Violations of the \"no duplicate\" primary key rule can be detected using the  getDuplicateKeys()  method on a  HollowPrimaryKeyIndex , which returns a  Collection Object[] .  If no duplicate keys exist, the returned Collection will be empty.  If they do, the returned values will indicate the keys for which duplicate records exist.   If a  HollowPrimaryKeyIndex  will be retained for a long duration, they should be kept updated as deltas are applied to the underlying  HollowReadStateEngine .  This is accomplished with a single call after instantiation to the  listenForDeltaUpdates()  method.    Detaching Primary Key Indexes  If  listenForDeltaUpdates()  is called on a primary key index, then it cannot be garbage collected.  If you intend to drop an index which is listening for updates, first call  detachFromDeltaUpdates()  to prevent a memory leak.   Indexes which are listening for delta updates are updated after a dataset is updated.  In the brief interim time between when a dataset is updated and the index is updated, the index will point to the  ghost records  located at tombstoned ordinals.  This helps guarantee that all in-flight operations will observe correct data.", 
            "title": "HollowPrimaryKeyIndex"
        }, 
        {
            "location": "/diving-deeper/#hollowhashindex", 
            "text": "It is sometimes desirable to index records by fields other than primary keys.  The  HollowHashIndex  allows for indexing records by fields or combinations of fields for which values may match multiple records, and records may match multiple values.  In our  Movie / Actor  example, we may want to index movies by their starring actors:  HollowHashIndex idx = \n            new HollowHashIndex(readEngine,  Movie ,  ,  cast.element.actor.actorId );  The  HollowHashIndex  expects in its constructor arguments a query start type, a select field, and a set of match fields.  The constructor arguments above indicate that queries will start with the  Movie  type, select the root of the query (indicated by the empty string), and match the id of any  Actor  record in the actors list.  To query this index:  HollowHashIndexResult result = idx.findMatches(102);\n\nif(result != null) {\n    System.out.println( Found matches:   + result.numResults());\n\n    HollowOrdinalIterator iter = result.iterator();\n    int matchedOrdinal = iter.next();\n    while(matchedOrdinal != HollowOrdinalIterator.NO_MORE_ORDINALS) {\n        Movie movie = api.getMovie(matchedOrdinal);\n        System.out.println( Starred in:   + movie.getTitle().getValue());\n        matchedOrdinal = iter.next();\n    }\n}  Alternatively, if the data model included the nationality of actors, and we needed to index actors by nationality and the titles of movies in which they starred:  HollowHashIndex idx = \n            new HollowHashIndex(readEngine,  Movie ,  cast.element.actor ,\n                                             title.value ,\n                                             cast.element.actor.nationality.id.value );  In this case, the query start type is still  Movie , but we\u2019re selecting related  Actor  records.  Matches are selected based on the  Movie \u2019s title, and the actor\u2019s nationality.  Using this index, one can query for Brazilian actors who starred in movies titled \u201cNarcos\u201d:  HollowHashIndexResult result = idx.findMatches( Narcos ,  BR );\n\nif(result != null) {\n    HollowOrdinalIterator iter = result.iterator();\n    int matchedOrdinal = iter.next();\n    while(matchedOrdinal != HollowOrdinalIterator.NO_MORE_ORDINALS) {\n        Actor actor = api.getMovie(matchedOrdinal);\n        System.out.println( Matched actor:   + \n                                      actor.getActorName().getValue());\n        matchedOrdinal = iter.next();\n    }\n}  The  HollowHashIndex  does not yet have a facility for listening for delta updates.  If an index is necessary across multiple states, currently the index must be recreated on each update.", 
            "title": "HollowHashIndex"
        }, 
        {
            "location": "/interacting-with-a-dataset/", 
            "text": "Generated Object API\n\n\nEach of the examples provided thus far have focused on interaction with the Hollow data set via the generated Hollow Objects API.  \n\n\nHollow Objects\n are instantiated at the time they are requested.  Each Hollow Object references a single record of a specific type, and holds two things:\n\n\n\n\nA reference to the Hollow data store for the type\n\n\nAn \nordinal\n\n\n\n\nA Hollow Object contains methods to retrieve each of its type's fields in the data model from which the API was generated.  Each time a field retrieval method is called on a Hollow Object, the data is retrieved directly from the Hollow data store.\n\n\n\n\nHollow Objects are 'hollow'\n\n\nThe name Hollow is derived from the fact that these objects are 'hollow' -- they \nappear\n to contain field accessors, but those are just facades which access the underlying data store.\n\n\n\n\nGenerated Type API\n\n\nAt times, using the Hollow Object API can result in a high rate of Object allocation.  All generated Hollow APIs also provide a way to interact with the data without creating Objects.  This is accomplished by using record \nordinals\n to query the data store directly.  For example:\n\n\nMovieTypeAPI movieTypeAPI = movieAPI.getMovieTypeAPI();\nListOfActorTypeAPI listOfActorTypeAPI = movieAPI.getListOfActorTypeAPI();\nActorTypeAPI actorTypeAPI = movieAPI.getActorTypeAPI();\nStringTypeAPI stringTypeAPI = movieAPI.getStringTypeAPI();\n\nint movieOrdinal = movieIdx.getMatchingOrdinal(6);\nint listOfActorsOrdinal = movieTypeAPI.getActors(movieOrdinal);\n\nint numActors = listOfActorTypeAPI.size(listOfActorsOrdinal);\n\nfor(int i=0; i\nnumActors; i++) {\n   int actorOrdinal = \n                 listOfActorTypeAPI.getElementOrdinal(listOfActorsOrdinal, i);\n   int stringOrdinal = actorTypeAPI.getActorNameOrdinal(actorOrdinal);\n   System.out.println(\nStarring \n + stringAPI.getValue(stringOrdinal));\n}\n\n\n\n\nIn extremely tight loops, it may be more efficient to use the Type API rather than the Object API.\n\n\n\n\nAvoid Premature Optimization\n\n\nIn all but the tightest, most frequently executed loops, usage of the Type API will be unnecessary.  Its usage should be applied judiciously, since the pattern can be more difficult to maintain.\n\n\n\n\nGeneric Object API\n\n\nHollow also includes a generic Hollow Object API which, if sufficient for consumers, obviates the need to provide generated code:\n\n\nint movieOrdinal = movieIdx.getMatchingOrdinal(1);\n\nGenericHollowObject movie = new GenericHollowObject(readEngine, \nMovie\n, movieOrdinal);\n\nString title = movie.getObject(\ntitle\n).getString(\nvalue\n);\n\nfor(GenericHollowObject actor : movie.getList(\nactors\n).objects()) {\n    String actorName = actor.getObject(\nactorName\n).getString(\nvalue\n);\n    System.out.println(\nStarring \n + actorName);\n}\n\n\n\n\nWorking with the Generic Object API can become cumbersome \u2014 unlike a generated Hollow API, the IDE type assist cannot provide a guide to the data model.  However, for simple data models and explorational tasks the Generic Object API can be useful.", 
            "title": "Interacting with a Hollow Dataset"
        }, 
        {
            "location": "/interacting-with-a-dataset/#generated-object-api", 
            "text": "Each of the examples provided thus far have focused on interaction with the Hollow data set via the generated Hollow Objects API.    Hollow Objects  are instantiated at the time they are requested.  Each Hollow Object references a single record of a specific type, and holds two things:   A reference to the Hollow data store for the type  An  ordinal   A Hollow Object contains methods to retrieve each of its type's fields in the data model from which the API was generated.  Each time a field retrieval method is called on a Hollow Object, the data is retrieved directly from the Hollow data store.   Hollow Objects are 'hollow'  The name Hollow is derived from the fact that these objects are 'hollow' -- they  appear  to contain field accessors, but those are just facades which access the underlying data store.", 
            "title": "Generated Object API"
        }, 
        {
            "location": "/interacting-with-a-dataset/#generated-type-api", 
            "text": "At times, using the Hollow Object API can result in a high rate of Object allocation.  All generated Hollow APIs also provide a way to interact with the data without creating Objects.  This is accomplished by using record  ordinals  to query the data store directly.  For example:  MovieTypeAPI movieTypeAPI = movieAPI.getMovieTypeAPI();\nListOfActorTypeAPI listOfActorTypeAPI = movieAPI.getListOfActorTypeAPI();\nActorTypeAPI actorTypeAPI = movieAPI.getActorTypeAPI();\nStringTypeAPI stringTypeAPI = movieAPI.getStringTypeAPI();\n\nint movieOrdinal = movieIdx.getMatchingOrdinal(6);\nint listOfActorsOrdinal = movieTypeAPI.getActors(movieOrdinal);\n\nint numActors = listOfActorTypeAPI.size(listOfActorsOrdinal);\n\nfor(int i=0; i numActors; i++) {\n   int actorOrdinal = \n                 listOfActorTypeAPI.getElementOrdinal(listOfActorsOrdinal, i);\n   int stringOrdinal = actorTypeAPI.getActorNameOrdinal(actorOrdinal);\n   System.out.println( Starring   + stringAPI.getValue(stringOrdinal));\n}  In extremely tight loops, it may be more efficient to use the Type API rather than the Object API.   Avoid Premature Optimization  In all but the tightest, most frequently executed loops, usage of the Type API will be unnecessary.  Its usage should be applied judiciously, since the pattern can be more difficult to maintain.", 
            "title": "Generated Type API"
        }, 
        {
            "location": "/interacting-with-a-dataset/#generic-object-api", 
            "text": "Hollow also includes a generic Hollow Object API which, if sufficient for consumers, obviates the need to provide generated code:  int movieOrdinal = movieIdx.getMatchingOrdinal(1);\n\nGenericHollowObject movie = new GenericHollowObject(readEngine,  Movie , movieOrdinal);\n\nString title = movie.getObject( title ).getString( value );\n\nfor(GenericHollowObject actor : movie.getList( actors ).objects()) {\n    String actorName = actor.getObject( actorName ).getString( value );\n    System.out.println( Starring   + actorName);\n}  Working with the Generic Object API can become cumbersome \u2014 unlike a generated Hollow API, the IDE type assist cannot provide a guide to the data model.  However, for simple data models and explorational tasks the Generic Object API can be useful.", 
            "title": "Generic Object API"
        }, 
        {
            "location": "/data-ingestion/", 
            "text": "Hollow includes a few ready-made data ingestion mechanisms.  Additionally, custom data ingestion mechanisms can be created relatively easily using the \nLow Level Input API\n.\n\n\nHollowObjectMapper\n\n\nWhen using a \nHollowProducer\n, each call to \nstate.add(obj)\n is delegated to a \nHollowObjectMapper\n.  The \nHollowObjectMapper\n is used to add POJOs into a \nHollowWriteStateEngine\n:\n\n\nHollowWriteStateEngine engine = /// a state engine\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\n\nfor(Movie movie : movies)\n    writeEngine.add(movie);\n\n\n\n\nThe \nHollowObjectMapper\n can also be used to initialize the data model of a \nHollowWriteStateEngine\n without adding any actual data:\n\n\nHollowWriteStateEngine engine = /// a state engine\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\n\nmapper.initializeTypeState(Movie.class);\nmapper.initializeTypeState(Show.class);\n\n\n\n\nSchemas will be assumed based on the field and type names in the POJOs.  Any referenced types will also be traversed and included in the derived data model.\n\n\n\n\nThread Safety\n\n\nThe \nHollowObjectMapper\n is thread-safe; multiple threads may add Objects at the same time.\n\n\n\n\nSpecifying type names in the HollowObjectMapper\n\n\nBy default, type names are equal to the names of classes added to the \nHollowObjectMapper\n.  Alternatively, the name of a type may be explicitly defined by using the \n@HollowTypeName\n annotation.  This annotation can be added at either the class or field level.\n\n\nThe following example \nAward\n class will reference a type \nAwardName\n, which will contain a single string value:\n\n\npublic class Award {\n    long id;\n\n    @HollowTypeName(name=\nAwardName\n)\n    String name;\n}\n\n\n\n\nThe following example \nCategory\n class will be added as the type \nGenre\n, where not otherwise specified by referencing fields:\n\n\n@HollowTypeName(name=\nGenre\n)\npublic class Category {\n   ...\n}\n\n\n\n\n\n\nNamespaced fields\n\n\nUsing the \n@HollowTypeName\n attribute is a convenient way to add appropriate \nrecord type namespacing\n into your data model.\n\n\n\n\nInlining fields in the HollowObjectMapper\n\n\nYou can \ninline\n fields in the \nHollowObjectMapper\n by annotating them with \n@HollowInline\n.  The following example \nCreator\n class inlines the field \ncreatorName\n:\n\n\npublic class Creator {\n    long id;\n\n    @HollowInline\n    String creatorName;\n}\n\n\n\n\nThe following \njava.lang.*\n types can be inlined:\n\n\n\n\nString\n\n\nBoolean\n\n\nInteger\n\n\nLong\n\n\nDouble\n\n\nFloat\n\n\nShort\n\n\nByte\n\n\nCharacter\n\n\n\n\nMemoizing POJOs in the HollowObjectMapper\n\n\nIf a long field named \n__assigned_ordinal\n is defined in a POJO class, then \nHollowObjectMapper\n will use this field to record the assigned ordinal when Objects of this class are added to the state engine.  \n\n\nWhen the \nHollowObjectMapper\n sees this POJO again, it will short-circuit writing to the state engine and discovering or assigning an ordinal -- it will instead return the previously recorded ordinal.  If during processing you can reuse duplicate referenced POJOs, then you can use this effect to greatly speed up adding records to the state engine.\n\n\nIf the \n__assigned_ordinal\n field is present, it should be initialized to -1.  The field may be (but does not have to be) private and/or final.\n\n\nThe following example \nDirector\n class uses the \n__assigned_ordinal\n optimization:\n\n\npublic class Director {\n    long id;\n    String directorName;\n\n    private final int __assigned_ordinal = -1L;\n}\n\n\n\n\n\n\n\nWarning\n\n\nIf the \n__assigned_ordinal\n optimization is used, POJOs should \nnot be modified\n after they are added to the state engine.  Any modifications after the first time a memoized POJO is added to the state engine will be ignored and any references to these POJOs will always point to the \noriginally\n added record.\n\n\n\n\nJSON to Hollow Adapter\n\n\nThe project \nhollow-jsonadapter\n contains a component which will automatically parse json into a \nHollowWriteStateEngine\n.  The expected format of the json will be defined by the schemas in the \nHollowWriteStateEngine\n.  The data model must be pre-initialized.  See the \nSchema Parser\n topic in this document for an easy way to configure the schemas with a text document.\n\n\nThe \nHollowJsonAdapter\n class is used to populate records of a single type from a json file.  A single record:\n\n\n{ \n  \nid\n: 1,\n  \nreleaseYear\n: 1999,\n  \nactors\n: [\n     {\n        \nid\n: 101,\n        \nactorName\n: \nKeanu Reeves\n\n     },\n     {\n        \nid\n: 102,\n        \nactorName\n: \nLaurence Fishburne\n\n     }\n  ]\n}\n\n\n\n\nCan be parsed with the following code:\n\n\nString json = /// the record above\n\nHollowJsonAdapter jsonAdapter = new HollowJsonAdapter(writeEngine, \nMovie\n);\n\njsonAdapter.processRecord(json);\n\n\n\n\nIf a field defined in the schema is not encountered in the json data, the value will be null in the corresponding Hollow record.  If a field is encountered in the json data which is not defined in the schema, the field will be ignored.\n\n\nA large number of records in a single file can also be processed:\n\n\nReader reader = /// a reader for the json file \n\nHollowJsonAdapter jsonAdapter = new HollowJsonAdapter(writeEngine, \nMovie\n);\n\njsonAdapter.populate(reader);\n\n\n\n\nWhen processing an entire file, it is expected that the file contains only a single json array of records of the expected type.  The records will be processed in parallel.\n\n\nZeno to Hollow Adapter\n\n\nThe project \nhollow-zenoadapter\n has an adapter which can be used with Hollow\u2019s predecessor, Zeno.  We used this as part of our migration path from Zeno to Hollow, and it is provided for current users of Zeno who would like to migrate to Hollow as well.  Start with the \nHollowStateEngineCreator\n.", 
            "title": "Data Ingestion"
        }, 
        {
            "location": "/data-ingestion/#hollowobjectmapper", 
            "text": "When using a  HollowProducer , each call to  state.add(obj)  is delegated to a  HollowObjectMapper .  The  HollowObjectMapper  is used to add POJOs into a  HollowWriteStateEngine :  HollowWriteStateEngine engine = /// a state engine\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\n\nfor(Movie movie : movies)\n    writeEngine.add(movie);  The  HollowObjectMapper  can also be used to initialize the data model of a  HollowWriteStateEngine  without adding any actual data:  HollowWriteStateEngine engine = /// a state engine\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\n\nmapper.initializeTypeState(Movie.class);\nmapper.initializeTypeState(Show.class);  Schemas will be assumed based on the field and type names in the POJOs.  Any referenced types will also be traversed and included in the derived data model.   Thread Safety  The  HollowObjectMapper  is thread-safe; multiple threads may add Objects at the same time.", 
            "title": "HollowObjectMapper"
        }, 
        {
            "location": "/data-ingestion/#specifying-type-names-in-the-hollowobjectmapper", 
            "text": "By default, type names are equal to the names of classes added to the  HollowObjectMapper .  Alternatively, the name of a type may be explicitly defined by using the  @HollowTypeName  annotation.  This annotation can be added at either the class or field level.  The following example  Award  class will reference a type  AwardName , which will contain a single string value:  public class Award {\n    long id;\n\n    @HollowTypeName(name= AwardName )\n    String name;\n}  The following example  Category  class will be added as the type  Genre , where not otherwise specified by referencing fields:  @HollowTypeName(name= Genre )\npublic class Category {\n   ...\n}   Namespaced fields  Using the  @HollowTypeName  attribute is a convenient way to add appropriate  record type namespacing  into your data model.", 
            "title": "Specifying type names in the HollowObjectMapper"
        }, 
        {
            "location": "/data-ingestion/#inlining-fields-in-the-hollowobjectmapper", 
            "text": "You can  inline  fields in the  HollowObjectMapper  by annotating them with  @HollowInline .  The following example  Creator  class inlines the field  creatorName :  public class Creator {\n    long id;\n\n    @HollowInline\n    String creatorName;\n}  The following  java.lang.*  types can be inlined:   String  Boolean  Integer  Long  Double  Float  Short  Byte  Character", 
            "title": "Inlining fields in the HollowObjectMapper"
        }, 
        {
            "location": "/data-ingestion/#memoizing-pojos-in-the-hollowobjectmapper", 
            "text": "If a long field named  __assigned_ordinal  is defined in a POJO class, then  HollowObjectMapper  will use this field to record the assigned ordinal when Objects of this class are added to the state engine.    When the  HollowObjectMapper  sees this POJO again, it will short-circuit writing to the state engine and discovering or assigning an ordinal -- it will instead return the previously recorded ordinal.  If during processing you can reuse duplicate referenced POJOs, then you can use this effect to greatly speed up adding records to the state engine.  If the  __assigned_ordinal  field is present, it should be initialized to -1.  The field may be (but does not have to be) private and/or final.  The following example  Director  class uses the  __assigned_ordinal  optimization:  public class Director {\n    long id;\n    String directorName;\n\n    private final int __assigned_ordinal = -1L;\n}   Warning  If the  __assigned_ordinal  optimization is used, POJOs should  not be modified  after they are added to the state engine.  Any modifications after the first time a memoized POJO is added to the state engine will be ignored and any references to these POJOs will always point to the  originally  added record.", 
            "title": "Memoizing POJOs in the HollowObjectMapper"
        }, 
        {
            "location": "/data-ingestion/#json-to-hollow-adapter", 
            "text": "The project  hollow-jsonadapter  contains a component which will automatically parse json into a  HollowWriteStateEngine .  The expected format of the json will be defined by the schemas in the  HollowWriteStateEngine .  The data model must be pre-initialized.  See the  Schema Parser  topic in this document for an easy way to configure the schemas with a text document.  The  HollowJsonAdapter  class is used to populate records of a single type from a json file.  A single record:  { \n   id : 1,\n   releaseYear : 1999,\n   actors : [\n     {\n         id : 101,\n         actorName :  Keanu Reeves \n     },\n     {\n         id : 102,\n         actorName :  Laurence Fishburne \n     }\n  ]\n}  Can be parsed with the following code:  String json = /// the record above\n\nHollowJsonAdapter jsonAdapter = new HollowJsonAdapter(writeEngine,  Movie );\n\njsonAdapter.processRecord(json);  If a field defined in the schema is not encountered in the json data, the value will be null in the corresponding Hollow record.  If a field is encountered in the json data which is not defined in the schema, the field will be ignored.  A large number of records in a single file can also be processed:  Reader reader = /// a reader for the json file \n\nHollowJsonAdapter jsonAdapter = new HollowJsonAdapter(writeEngine,  Movie );\n\njsonAdapter.populate(reader);  When processing an entire file, it is expected that the file contains only a single json array of records of the expected type.  The records will be processed in parallel.", 
            "title": "JSON to Hollow Adapter"
        }, 
        {
            "location": "/data-ingestion/#zeno-to-hollow-adapter", 
            "text": "The project  hollow-zenoadapter  has an adapter which can be used with Hollow\u2019s predecessor, Zeno.  We used this as part of our migration path from Zeno to Hollow, and it is provided for current users of Zeno who would like to migrate to Hollow as well.  Start with the  HollowStateEngineCreator .", 
            "title": "Zeno to Hollow Adapter"
        }, 
        {
            "location": "/advanced-topics/", 
            "text": "Schema Parser\n\n\nHollow schemas can be serialized as Java Strings.  Calling \ntoString()\n on a \nHollowSchema\n will produce a human-readable representation of the schema.  The following shows the String representations of all of the schemas from a \nMovie\n/\nActor\n example data model:\n\n\nMovie @PrimaryKey(id) {\n    long id;\n    int releaseYear;\n    string title;\n    SetOfActor actors;\n}\n\nSetOfActor Set\nActor\n @HashKey(firstname, surname);\n\nActor {\n    long id;\n    String firstname;\n    String surname;\n}\n\nString {\n    string value;\n}\n\n\n\n\nThese representations can be parsed using the \nHollowSchemaParser\n, and in turn can be used to initialize the state of a \nHollowWriteStateEngine\n:\n\n\nString allSchemas = /// a String containing all schemas\n\nList\nHollowSchema\n schemas = \n             HollowSchemaParser.parseCollectionOfSchemas(allSchemas);\n\nHollowWriteStateEngine initializedWriteEngine = \n             HollowWriteStateCreator.createWithSchemas(schemas);\n\n\n\n\n\n\nGuiding Data Ingestion with a Data Model\n\n\nFor a generic data ingestion mechanism, loading the schemas from a text representation comes in handy.  For example, the \nJSON to Hollow adapter\n requires a \nHollowWriteStateEngine\n which is preinitialized with a data model.  The data model can be configured in a text file, and loaded with the \nHollowSchemaParser\n.\n\n\n\n\nObject schema definitions take the following form:\n\n\nObjectTypeName @PrimaryKey(fieldName1, fieldNameN) {\n   FieldType1 fieldName1;\n   FieldType2 fieldName2;\n   ...\n   FieldTypeN fieldNameN;\n}\n\n\n\n\nThe primary key definition is optional and should be omitted if no primary key should be defined for a type.\n\n\nObject schemas may define any of the following field types: \nint\n, \nlong\n, \nfloat\n, \ndouble\n, \nboolean\n, \nstring\n, \nbytes\n.  If a field has a type other than these, the field will be interpreted as a \nREFERENCE\n to another type of that name.\n\n\n\n\nLowercase Field Type Declarations\n\n\nNote that the declarations for each of the inline field types are all lowercase (including \nstring\n).  An uppercase letter in any of these types will be interpreted as a \nREFERENCE\n field to a separate type.\n\n\n\n\nList\n, \nSet\n, and \nMap\n types use the following notation:\n\n\n\n\nListTypeName List\nElementTypeName\n;\n\n\nSetTypeName Set\nElementTypeName\n @HashKey(elementFieldOne, elementFieldTwo);\n\n\nMapTypeName Map\nKeyTypeName, ValueTypeName\n @HashKey(keyFieldOne, keyFieldTwo);\n\n\n\n\nSet\n and \nMap\n types may optionally define a \nhash key\n.  The hash key definition should be omitted if no hash key should be defined for a type.\n\n\nElements, keys, and values in collection record types cannot be inlined.  Whitespace is unimportant when parsing schema definitions.\n\n\nLow Level Input API\n\n\nAlthough Hollow includes a few ready-made data ingestion utilities, other data ingestion utilities can be created.  Adding data into Hollow starts with a \nHollowWriteStateEngine\n.  We need to initialize a type state for each schema in our data model:\n\n\nHollowWriteStateEngine writeEngine = new HollowWriteStateEngine();\n\nHollowObjectSchema movieSchema = new HollowObjectSchema(\nMovie\n, 3);\nmovieSchema.addField(\nid\n, FieldType.LONG);\nmovieSchema.addField(\ntitle\n, FieldType.REFERENCE, \nString\n);\nmovieSchema.addField(\nreleaseYear\n, FieldType.INT);\n\nHollowObjectTypeWriteState movieState = new HollowObjectTypeWriteState(movieSchema);\n\nwriteEngine.addTypeState(movieState);\n\n\n\n\nOnce we\u2019ve initialized our type states, we can add data into our state engine using HollowWriteRecords:\n\n\nHollowObjectSchema stringSchema = /// the String schema\nHollowObjectSchema movieSchema = /// the Movie schema\n\nHollowObjectWriteRecord titleRec = new HollowObjectWriteRecord(stringSchema);\nHollowObjectWriteRecord movieRec = new HollowObjectWriteRecord(movieSchema);\n\ntitleRec.setString(\nvalue\n, \nThe Matrix\n);\n\nint titleOrdinal = writeEngine.addObject(\nString\n, titleRec);\n\nmovieRec.setLong(\nid\n, 1);\nmovieRec.setReference(\ntitle\n, titleOrdinal);\nmovieRec.setInt(\nreleaseYear\n, 1999);\n\nwriteEngine.addObject(\nMovie\n, movieRec);\n\n\n\n\nNote that referenced records must be added prior to referencing records in order to obtain the referenced ordinals.\n\n\n\n\nReusing HollowWriteRecords\n\n\nHollowWriteRecord\ns can be reused -- just be sure to call \nreset()\n before populating data from the next record.\n\n\n\n\nEach schema type has its own \nHollowTypeWriteState\n and \nHollowWriteRecord\n implementation:\n\n\nHollowObjectSchema objectSchema = /// an Object schema\nHollowListSchema listSchema = /// a List schema\nHollowSetSchema setSchema = /// a Set schema\nHollowMapSchema mapSchema = /// a Map schema\n\nHollowObjectTypeWriteState objectTypeState = new HollowObjectTypeWriteState(objectSchema);\nHollowListTypeWriteState listTypeState = new HollowListTypeWriteState(listSchema);\nHollowSetTypeWriteState setTypeState = new HollowSetTypeWriteState(setSchema);\nHollowMapTypeWritestate mapTypeState = new HollowMapTypeWriteState(mapSchema);\n\nHollowObjectWriteRecord objectRec = new HollowObjectWriteRecord(objectSchema);\nHollowListWriteRecord listRec = new HollowListWriteRecord();\nHollowSetWriteRecord setRec = new HollowSetWriteRecord();\nHollowMapWriteRecord mapRec = new HollowMapWriteRecord();\n\n\n\n\nUsing this API, it is possible to create a generic data ingestion mechanism from \nany\n type of input source. \n\n\nDetermining Populated Ordinals\n\n\nMany lower-level operations require knowledge of the currently populated ordinals (i.e. those which are not holes), or knowledge of ordinals which were populated in state prior to the last delta application.  We can determine this from any \ntype state\n in a \nHollowReadStateEngine\n.  Both of these are useful, for example, if consumers wish to inspect exactly what has changed.\n\n\nEach \nHollowTypeReadState\n underneath a \nHollowReadStateEngine\n contains the methods \ngetPopulatedOrdinals()\n and \ngetPreviousOrdinals()\n.  Each of these methods returns a \njava.util.BitSet\n.  The contents of returned \nBitSet\ns may be inspected, but should never be modified.\n\n\nCaching\n\n\nAlthough a lot of effort has gone into minimizing the cost to read Hollow data, there is still inevitably a performance difference between accessing a field in a POJO and accessing a field in Hollow.  In general, this will not be a performance bottleneck, but in some rare cases the performance of tight inner loops may suffer on consumers.  \n\n\nWhen there is a type with a low cardinality, we can instantiate and cache a POJO implementation for each ordinal, which can be used by consumers in tight inner loops.  This is accomplished by simply passing a \nSet\nString\n as the second constructor argument when instantiating a custom-generated Hollow API.  The elements in the Set should be the types to cache.\n\n\n\n\nAvoid Premature Optimization\n\n\nCaching should be used judiciously.  In all but the tightest of loops, caching will be unnecessary, and can even be detrimental to performance for types with a large cardinality.\n\n\n\n\nHollow Heap Effects\n\n\nDouble Snapshots\n\n\nAt times, a new state may be produced to which there is no available delta from the previous state.  When this \nbroken delta chain\n scenario occurs, consumers will by default attempt to load a snapshot to the latest state.  If a consumer loads a snapshot when it currently has a state loaded for the same dataset, we call this a \ndouble snapshot\n.  Double snapshots result in a doubling of the heap usage of your dataset.  This is because Hollow assumes that consumers may be actively using the current state, and it therefore must retain the current state to provide data while the next state is loaded.  Only after the next state is fully loaded can the old state be dropped.\n\n\nIf using the \nHollow Consumer\n, you can configure consumers to never attempt a double snapshot.  This is accomplished with a custom \nDoubleSnapshotConfig\n.  In this case, you should check for 'stuck' clients in a \nRefreshListener\n, so that you may take the appropriate operational action.\n\n\nIf a dataset is large, double snapshots should be avoided for performance reasons.  Double snapshots can be entirely avoided by \nrestoring\n a \nHollowWriteStateEngine\n at producer initialization time, and, if necessary, using the \nHollowStateDeltaPatcher\n to operationally fill lost or missing deltas in the event an unforeseen issue occurs.  \n\n\nType Sharding\n\n\nDuring a delta transition each individual record type builds the entire next state of the type in memory while the current state is retained to answer requests.  This means that the memory pool Hollow reserves needs to be large enough to compose an extra copy of the largest type.  This is a miniaturized version of the \ndouble snapshot\n problem.\n\n\nTo address this problem, large types can be transparently sharded into smaller individual chunks, each of which updates independently.  The effect of this feature is that Hollow only needs to retain a large enough memory pool to update the largest \nchunk\n across all types.\n\n\nType sharding can be specified in two ways:  \n\n\n\n\nThe recommended way is to specify a target max type shard size via a call to \nsetTargetMaxTypeShardSize(long bytes)\n on a \nHollowWriteStateEngine\n prior to writing the first snapshot.  With this call, the number of shards will be automatically calculated based on the target excess memory pool size.\n\n\nAdditionally, the number of shards for a type can be explicitly specified by annotating a POJO with the \n@HollowShardLargeType\n annotation when using the \nHollowObjectMapper\n for data ingestion.  This can be useful if rapid growth is anticipated in a type.\n\n\n\n\nWithin a continuous delta chain, the type sharding configuration cannot be changed.  When a producer \nrestores\n the previously produced state at startup, then the restored \nHollowWriteStateEngine\n will always retain the sharding configuration of the prior state rather than recalculating based on the current size of each type.  Consequently, if the changes in a dataset over time results in a type sharding configuration which is highly suboptimal, it is recommended to start a new delta chain, which may require a double snapshot on all consumers, a simultaneous restart of all consumers, or a new \nblob namespace\n to which consumers can migrate over a period of time.\n\n\n\n\nBackwards Compatibility\n\n\nType sharding is new in v2.1.0.  Consumers can read blobs produced by producers v2.1.0 and later \nas long as type sharding is disabled\n. \nIf you are sure that all consumers are using v2.1.0 or later, it is safe to turn on type sharding.\n\n\nIn order to avoid causing issues for early adopters, the default target max type shard size is currently set to \nLong.MAX_VALUE\n.  At a later time, \nthis default will be changed to 25MB.\n\n\n\n\nObject Longevity\n\n\nA Hollow object returned from a generated API contains a reference to the Hollow data store, and an ordinal.  For this reason, if a reference to a Hollow object is retained by the consumer for an extended period of time, and the underlying record changes unexpected results may begin to be returned from these references.  We call Hollow objects which were obtained from a no longer current state stale references.\n\n\nIt is best practice to \nnever\n cache Hollow objects.  However, if it somehow \ncannot\n be guaranteed that Hollow Objects will never be cached at the consumer, and guaranteed protection against accidentally cached objects is necessary, then \nobject longevity\n can be enabled.\n\n\nWith object longevity, Hollow objects will, after an update, be backed by a reserved copy of the data at the time the reference was created.  This guarantees that even if a reference is held for a long time, it will continue to return the same data when interrogated.\n\n\nWhen object longevity is defined, Two durations are defined: \n\n\n\n\na grace period, and \n\n\na usage detection period.  \n\n\n\n\nThe grace period is defined by its duration, in milliseconds, after a reference becomes stale.  During the grace period, usage of stale references is acceptable.  The usage detection period is defined by its duration, in milliseconds, after the grace period has expired.  During the usage detection period, usage of stale references is unexpected, but will not result in failed interrogations of Hollow objects.  After the usage detection period expires, data will be dropped if no usage was detected in the usage detection period.  If stale references are interrogated \nafter\n their backing data is dropped, then Exceptions will be thrown.\n\n\nThe \nObjectLongevityConfig\n, injected into the \nHollowConsumer\n constructor, contains a few methods which are used to configure object longevity behavior:\n\n\n\n\nboolean enableLongLivedObjectSupport()\n: Whether or not object longevity is enabled.\n\n\nlong gracePeriodMillis()\n: If object longevity is enabled, this returns the number of milliseconds before the usage of stale objects gets flagged.\n\n\nlong usageDetectionPeriodMillis()\n: If long-lived object support is enabled, this defines the number of milliseconds, after the grace period, during which data is still available in stale references, but usage will be flagged.\n\n\nboolean dropDataAutomatically()\n: Returns whether or not to drop data behind stale references after the grace period + usage detection period has elapsed, as long as no usage was detected during the usage detection period.  This can be used to avoid memory leaks when long lived object support is enabled and stale references are cached, but unused.\n\n\nboolean forceDropData()\n: Returns whether or not to drop data behind stale references after the grace period + usage detection period has elapsed, even if usage was detected during the usage detection period.  This can be used to avoid memory leaks when long lived object support is enabled and stale references are cached and used.\n\n\n\n\nYour implementation of the \nObjectLongevityConfig\n may be backed by a dynamic configuration and safely change on live running consumers.  For example, \nforceDropData()\n may be operationally useful for boxes that are exhibiting memory leaks due to non-critical cached Hollow objects.\n\n\n\n\nHow It Works\n\n\nWhen enabled, object longevity is achieved using the \nHollowHistory\n data structure, which results in a minimal heap overhead. \n\n\n\n\nMemory Pooling\n\n\nHollow pools and reuses memory to minimize GC effects while updating data.  This pool of memory is kept arrays on the heap.  Each array in the pool has a fixed length.  When a long array or a byte array is required in Hollow, it will stitch together pooled array segments as a \nSegmentedByteArray\n or \nSegmentedLongArray\n.  These classes encapsulate the details of treating segmented arrays as contiguous ranges of values.\n\n\nDelta-Based Producer Input\n\n\nThe \nGetting Started\n section of this documentation describes a producer which every so often reads the \nentire dataset\n from some source of truth, re-adds all records to a \nHollowWriteStateEngine\n, then produces a delta based on the automatically discovered differences in the dataset since the prior cycle.  It is possible, however, that a producer may \nreceive\n an incoming stream of events which directly indicate the changes to a dataset, obviating the need to scan through the entire source of truth and re-add the entire dataset on each cycle.\n\n\nIf desired, a \nHollowWriteStateEngine\n\u2019s state can be explicitly modified, rather than recreated, each cycle.  We start such a cycle by re-adding all of the records from the previous cycle to the state engine:\n\n\nHollowWriteStateEngine writeEngine = /// the state engine\n\nwriteEngine.prepareForNextCycle();\nwriteEngine.addAllObjectsFromPreviousCycle();\n\n\n\n\nWe\u2019ll also need an indexed \nHollowReadStateEngine\n, which is updated in lock-step with the \nHollowWriteStateEngine\n.  The index can be used to retrieve the ordinals of records to be replaced.  These ordinals can be removed from the \nHollowWriteStateEngine\n:\n\n\nHollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\nHollowPrimaryKeyIndex idx = \n                       new HollowPrimaryKeyIndex(readEngine, \nMovie\n, \nid\n);\n\nHollowTypeWriteState movieTypeState = writeEngine.getTypeState(\nMovie\n);\n\nList\nMovieUpdateEvent\n eventBatch = /// a batch of events\n\nfor(MovieUpdateEvent event : eventBatch) {\n    int oldOrdinal = idx.getMatchingOrdinal(event.getMovie().getId());\n    movieTypeState.removeOrdinalFromThisCycle(oldOrdinal);\n    mapper.addObject(event.getMovie());\n}\n\n\n\n\n\n\nWatch out for Duplicates\n\n\nBe careful, this process assumes that no two events will have the same movie ID in the same cycle.  You'll want to dedup the \neventBatch\n.\n\n\n\n\nThis process may leave orphaned records around, since the call to \nremoveOrdinalFromThisCycle()\n doesn\u2019t remove any referenced records.  To solve this, the \nTransitiveSetTraverser\n can be used:\n\n\nList\nMovieUpdateEvent\n eventBatch = /// a batch of events\n\n/// find the Movie ordinals to remove\nBitSet removedMovieOrdinals = new BitSet();\nfor(MovieUpdateEvent event : eventBatch) {\n \u00a0\u00a0\u00a0int oldOrdinal = idx.getMatchingOrdinal(event.getMovie().getId());\n \u00a0\u00a0\u00a0removedMovieOrdinals.set(oldOrdinal);\n}\n\n/// initially the removal selection includes just Movies\nMap\nString, BitSet\n removeRecords = new HashMap\n();\nremoveRecords.put(\nMovie\n, removedMovieOrdinals);\n\n/// expand the selection to include any records referenced by selected Movies\nTransitiveSetTraverser.addTransitiveMatches(readEngine, removeRecords);\n/// but don't include records which are also referenced by unselected movies\nTransitiveSetTraverser.removeReferencedOutsideClosure(readEngine, removeRecords);\n\n/// remove everything in the selection\nfor(Map.Entry\nString, BitSet\n entry : removeRecords) {\n \u00a0\u00a0\u00a0HollowTypeWriteState typeState = writeEngine.getTypeState(entry.getKey());\n \u00a0\u00a0\u00a0int removeOrdinal = entry.getValue().nextSetBit(0);\n \u00a0\u00a0\u00a0while(removeOrdinal != -1) {\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0typeState.removeOrdinalFromThisCycle(removeOrdinal);\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0removeOrdinal = entry.getValue().nextSetBit(removeOrdinal + 1);\n \u00a0\u00a0\u00a0}\n}\n\n\n\n\nIn the above code, each of the \nMovie\n ordinals to be replaced are added to a \nBitSet\n.  Then, the \nTransitiveSetTraverser\n is used to expand the collection of selected records by adding any records referenced by the selected \nMovies\n.  Then, the \nTransitiveSetTraverser\n is again used to deselect any child records which are also referenced by other Movies which were not selected for removal.  Finally, the selection is actually removed from the \nHollowWriteStateEngine\n.\n\n\nIn-Memory Data Layout\n\n\nEach record in Hollow begins with a fixed-length number of bits.  At the lowest level, these bits are held in long arrays using the class \nFixedLengthElementArray\n.  This class allows for storage and retrieval of fixed-length data in a range of bits.  For example, if a \nFixedLengthElementArray\n was queried for the 6-bit value starting at bit 7 in the following example range of bits:\n\n\n\n\nThe value \n100100\n in binary, or \n36\n in base 10, would be returned.\n\n\nObject Layout\n\n\nAn \nOBJECT\n record is a fixed set of strongly typed fields.  Each field is represented by a fixed-length number of bits.  Each record is represented by a fixed length number of bits equal to the sum of the bits required to represent each fields.  For each type, all fields of all records are packed into a single \nFixedLengthElementArray\n.  No bookkeeping data structures are required to locate a record -- the start bit for each record can is located by simply multiplying the number of bits per record times the record\u2019s ordinal.\n\n\n\n\nThe number of bits used to represent a field which is one of the types (\nINT\n, \nLONG\n, \nREFERENCE\n) is exactly equal to the number of bits required to represent the maximum value contained in the field across all records.  The values for \nINT\n and \nLONG\n fields are represented using zig-zag encoding, so that smaller absolute values require fewer bits.  The values for \nREFERENCE\n fields are encoded as the referenced record\u2019s ordinal, which along with the referenced type (from the schema) is sufficient to identify and locate the referenced record.\n\n\n32 bits are used to represent a \nFLOAT\n, and 64 bits are used to represent a \nDOUBLE\n.\n\n\nSTRING\n and \nBYTES\n fields each get a separate byte array, into which the values for all records are packed.  The fixed-length value in these fields are offsets into the field\u2019s byte array where the record\u2019s value ends.  In order to determine the begin byte for the record with ordinal n, the offset encoded into the record with ordinal (n-1) is read.  The number of fixed length bits used to represent the offsets is exactly equal to the number of number of bits required to represent the maximum offset, plus one.\n\n\nEach field type may be assigned a null value.  For \nINT\n, \nLONG\n, and \nREFERENCE\n fields, null is encoded as a value with all ones.  For \nFLOAT\n and \nDOUBLE\n fields, null is encoded as special bit sequences.  For \nSTRING\n and \nBYTES\n fields, null is encoded by setting a designated null bit at the beginning of each field, followed by the end offset of the last populated value for that field.\n\n\nList Layout\n\n\nA \nLIST\n is an ordered collection of records of a specific type.  \nLIST\n types are represented with two FixedLengthElementArrays.  We can refer to these arrays as the \noffset array\n and the \nelement array\n.\n\n\nEach \nLIST\n type contains a single \nelement array\n into which the references to elements for all records are packed.  References are encoded as the ordinals of the element records, which is sufficient to identify and locate the record.  Each reference is represented using a fixed number of bits equal to the number of bits required to represent the maximum referenced ordinal across all records.  Each record is represented with a contiguous range of elements in the \nelement array\n.  \n\n\nThe \noffset array\n contains fixed-length offsets into the element array where the record\u2019s elements end.  In order to determine the begin element for the record with ordinal n, the end value for the element (n-1) is read.  \n\n\n\n\nElements in a \nLIST\n record may not be null.\n\n\nSet Layout\n\n\nA \nSET\n is an unordered collection of records of a specific type.  The records for \nSET\n elements are hashed into an open-addressed hash table.  \nSET\n types are represented with two \nFixedLengthElementArrays\n.  We can refer to these arrays as the \noffset array\n and the \nbucket array\n.\n\n\nEach \nSET\n element contains a single \nbucket array\n into which the references to elements for all records are packed.  Each record is represented with a contiguous range of buckets in the \nbucket array\n.  The range of buckets for each record will contain an open-addressed hash table, with a linear probing hash collision resolution strategy.  The number of buckets for each record will be a power of two, and will be large enough such that all elements for the record will fit into those buckets with a load factor no greater than 70%.  Each bucket is represented using a fixed number of bits equal to the number of bits required to represent the maximum referenced ordinal across all records.  A populated bucket is encoded as the ordinal of the referenced record in that bucket.  To represent empty buckets, a sentinel value equal to all ones in binary is reserved.\n\n\nThe \noffset array\n contains two fixed-length fields per record:  the size of the set, and the offset to the bucket where the record\u2019s data ends.  Each of these fields is encoded using a fixed number of bits equal to the number of bits required to represent the field\u2019s maximum value across all records.\n\n\n\n\nElements in a \nSET\n record may not be null.\n\n\nMap Layout\n\n\nA \nMAP\n is an unordered collection of key/value pairs, where each key is a specific type, and each value type is a specific type.  The records for MAP elements are hashed into an open-addressed hash table.  \nMAP\n types are represented with two \nFixedLengthElementArrays\n.  We can refer to these arrays as the \noffset array\n and the \nbucket array\n.\n\n\nEach \nMAP\n type contains a single bucket array into which the references to keys and values for all records are packed.  Each record is represented with a contiguous range of buckets in the \nbucket array\n.  The range of buckets for each record will contain an open-addressed hash table, with a linear probing hash collision resolution strategy.  The number of buckets for each record will be a power of two, and will be large enough such that all key/value pairs for the record will fit into those buckets with a load factor no greater than 70%.  Each bucket is represented using a fixed number of bits equal to the number of bits required to represent the maximum referenced key ordinal, plus the number of bits required to represent the maximum referenced value ordinal.  A populated bucket contains two fixed length fields: the first field contains the ordinal of the referenced key, and the second field contains the ordinal of the referenced value.  Empty buckets are represented with a key field containing a reserved sentinel value equal to all ones in binary.\n\n\nThe \noffset array\n contains two fixed-length fields per record:  the size of the map, and the offset to the bucket where the record\u2019s data ends.  Each of these fields is encoded using a fixed number of bits equal to the number of bits required to represent the field\u2019s maximum value across all records.\n\n\n\n\nA \nMAP\n cannot contain null keys or values.\n\n\nPrimary Key Index Layout\n\n\nA primary key index is a single \nFixedLengthElementArray\n, which represents an open-addressed hash table of pointers to records of the given type.  The hash of each record is derived based on the fields designated in the primary key.  Each bucket in the hash table is represented using a fixed number of bits equal to the number of bits required to represent the maximum ordinal of the indexed type.  A populated bucket is encoded as the ordinal of the referenced record in that bucket.  To represent empty buckets, a sentinel value equal to all ones in binary is reserved.\n\n\nWhen queried with a key, the index will hash the key, look in the corresponding bucket to find the ordinal of a record which also hashes to this key, then compare the referenced record\u2019s key to the query.  When a matching record is found, the ordinal at that bucket is returned as the match.  Collisions are resolved with linear probing.  If after linear probing an empty bucket is encountered, no such record exists in the dataset.\n\n\nHash Index Layout\n\n\nA hash index is uses two \nFixedLengthElementArrays\n.  These arrays can be referred to as the \nmatch array\n and the \nselect array\n.  The \nmatch array\n is an open-addressed hash table and contains buckets.  Like a primary key index, the match array does not re-encode the values of keys.  Instead, it retains pointers to existing records which may be used to retrieve the hashed keys.\n\n\nUnlike a primary key index, it may not be sufficient to retain a single pointer to the indexed type and extract the hashed key from that record -- each record in a hash index may match multiple keys.  Instead, we retain 1-n pointers.  Each pointer will indicate a record through which one or more of the key fields may be unambiguously retrieved.  If multiple key fields may be unambiguously traversed to via a single type, then only a single pointer for the field group will be retained per bucket.  \n\n\nConsider a scenario in which a hash index is used to index Movies by the nationalities and birth year of actors.  For the key \n[\u201cBritish\u201d, 1972]\n, the corresponding entry in the match array may contain a pointer to the \nActor\n record for Idris Elba.  Although the record points to a specific \nActor\n, the matching records for this entry will contain movies starring any British Actor born in 1972.\n\n\nEach pointer field in the match array bucket references a specific type, and is encoded as the ordinal to which the bucket refers.  Each is represented using a fixed number of bits equal to the maximum ordinal in the referenced type.\n\n\nIn addition to pointers which allow us to look up the matching key, each bucket in the \nmatch array\n includes the number of matching records, and an offset into the \nselect array\n.  The \nselect array\n contains lists of ordinals to matching records.\n\n\nWhen queried with a key, the index will hash the key, then look to the corresponding bucket in the \nmatch array\n.  The match pointers are used to compare the queried key with the matching key.  If a match is found, then the corresponding entries in the select array are returned as a \nHollowHashIndexResult\n.  Collisions are resolved with linear probing.  If after linear probing an empty bucket is encountered in the match array, no such record exists in the dataset.", 
            "title": "Advanced Topics"
        }, 
        {
            "location": "/advanced-topics/#schema-parser", 
            "text": "Hollow schemas can be serialized as Java Strings.  Calling  toString()  on a  HollowSchema  will produce a human-readable representation of the schema.  The following shows the String representations of all of the schemas from a  Movie / Actor  example data model:  Movie @PrimaryKey(id) {\n    long id;\n    int releaseYear;\n    string title;\n    SetOfActor actors;\n}\n\nSetOfActor Set Actor  @HashKey(firstname, surname);\n\nActor {\n    long id;\n    String firstname;\n    String surname;\n}\n\nString {\n    string value;\n}  These representations can be parsed using the  HollowSchemaParser , and in turn can be used to initialize the state of a  HollowWriteStateEngine :  String allSchemas = /// a String containing all schemas\n\nList HollowSchema  schemas = \n             HollowSchemaParser.parseCollectionOfSchemas(allSchemas);\n\nHollowWriteStateEngine initializedWriteEngine = \n             HollowWriteStateCreator.createWithSchemas(schemas);   Guiding Data Ingestion with a Data Model  For a generic data ingestion mechanism, loading the schemas from a text representation comes in handy.  For example, the  JSON to Hollow adapter  requires a  HollowWriteStateEngine  which is preinitialized with a data model.  The data model can be configured in a text file, and loaded with the  HollowSchemaParser .   Object schema definitions take the following form:  ObjectTypeName @PrimaryKey(fieldName1, fieldNameN) {\n   FieldType1 fieldName1;\n   FieldType2 fieldName2;\n   ...\n   FieldTypeN fieldNameN;\n}  The primary key definition is optional and should be omitted if no primary key should be defined for a type.  Object schemas may define any of the following field types:  int ,  long ,  float ,  double ,  boolean ,  string ,  bytes .  If a field has a type other than these, the field will be interpreted as a  REFERENCE  to another type of that name.   Lowercase Field Type Declarations  Note that the declarations for each of the inline field types are all lowercase (including  string ).  An uppercase letter in any of these types will be interpreted as a  REFERENCE  field to a separate type.   List ,  Set , and  Map  types use the following notation:   ListTypeName List ElementTypeName ;  SetTypeName Set ElementTypeName  @HashKey(elementFieldOne, elementFieldTwo);  MapTypeName Map KeyTypeName, ValueTypeName  @HashKey(keyFieldOne, keyFieldTwo);   Set  and  Map  types may optionally define a  hash key .  The hash key definition should be omitted if no hash key should be defined for a type.  Elements, keys, and values in collection record types cannot be inlined.  Whitespace is unimportant when parsing schema definitions.", 
            "title": "Schema Parser"
        }, 
        {
            "location": "/advanced-topics/#low-level-input-api", 
            "text": "Although Hollow includes a few ready-made data ingestion utilities, other data ingestion utilities can be created.  Adding data into Hollow starts with a  HollowWriteStateEngine .  We need to initialize a type state for each schema in our data model:  HollowWriteStateEngine writeEngine = new HollowWriteStateEngine();\n\nHollowObjectSchema movieSchema = new HollowObjectSchema( Movie , 3);\nmovieSchema.addField( id , FieldType.LONG);\nmovieSchema.addField( title , FieldType.REFERENCE,  String );\nmovieSchema.addField( releaseYear , FieldType.INT);\n\nHollowObjectTypeWriteState movieState = new HollowObjectTypeWriteState(movieSchema);\n\nwriteEngine.addTypeState(movieState);  Once we\u2019ve initialized our type states, we can add data into our state engine using HollowWriteRecords:  HollowObjectSchema stringSchema = /// the String schema\nHollowObjectSchema movieSchema = /// the Movie schema\n\nHollowObjectWriteRecord titleRec = new HollowObjectWriteRecord(stringSchema);\nHollowObjectWriteRecord movieRec = new HollowObjectWriteRecord(movieSchema);\n\ntitleRec.setString( value ,  The Matrix );\n\nint titleOrdinal = writeEngine.addObject( String , titleRec);\n\nmovieRec.setLong( id , 1);\nmovieRec.setReference( title , titleOrdinal);\nmovieRec.setInt( releaseYear , 1999);\n\nwriteEngine.addObject( Movie , movieRec);  Note that referenced records must be added prior to referencing records in order to obtain the referenced ordinals.   Reusing HollowWriteRecords  HollowWriteRecord s can be reused -- just be sure to call  reset()  before populating data from the next record.   Each schema type has its own  HollowTypeWriteState  and  HollowWriteRecord  implementation:  HollowObjectSchema objectSchema = /// an Object schema\nHollowListSchema listSchema = /// a List schema\nHollowSetSchema setSchema = /// a Set schema\nHollowMapSchema mapSchema = /// a Map schema\n\nHollowObjectTypeWriteState objectTypeState = new HollowObjectTypeWriteState(objectSchema);\nHollowListTypeWriteState listTypeState = new HollowListTypeWriteState(listSchema);\nHollowSetTypeWriteState setTypeState = new HollowSetTypeWriteState(setSchema);\nHollowMapTypeWritestate mapTypeState = new HollowMapTypeWriteState(mapSchema);\n\nHollowObjectWriteRecord objectRec = new HollowObjectWriteRecord(objectSchema);\nHollowListWriteRecord listRec = new HollowListWriteRecord();\nHollowSetWriteRecord setRec = new HollowSetWriteRecord();\nHollowMapWriteRecord mapRec = new HollowMapWriteRecord();  Using this API, it is possible to create a generic data ingestion mechanism from  any  type of input source.", 
            "title": "Low Level Input API"
        }, 
        {
            "location": "/advanced-topics/#determining-populated-ordinals", 
            "text": "Many lower-level operations require knowledge of the currently populated ordinals (i.e. those which are not holes), or knowledge of ordinals which were populated in state prior to the last delta application.  We can determine this from any  type state  in a  HollowReadStateEngine .  Both of these are useful, for example, if consumers wish to inspect exactly what has changed.  Each  HollowTypeReadState  underneath a  HollowReadStateEngine  contains the methods  getPopulatedOrdinals()  and  getPreviousOrdinals() .  Each of these methods returns a  java.util.BitSet .  The contents of returned  BitSet s may be inspected, but should never be modified.", 
            "title": "Determining Populated Ordinals"
        }, 
        {
            "location": "/advanced-topics/#caching", 
            "text": "Although a lot of effort has gone into minimizing the cost to read Hollow data, there is still inevitably a performance difference between accessing a field in a POJO and accessing a field in Hollow.  In general, this will not be a performance bottleneck, but in some rare cases the performance of tight inner loops may suffer on consumers.    When there is a type with a low cardinality, we can instantiate and cache a POJO implementation for each ordinal, which can be used by consumers in tight inner loops.  This is accomplished by simply passing a  Set String  as the second constructor argument when instantiating a custom-generated Hollow API.  The elements in the Set should be the types to cache.   Avoid Premature Optimization  Caching should be used judiciously.  In all but the tightest of loops, caching will be unnecessary, and can even be detrimental to performance for types with a large cardinality.", 
            "title": "Caching"
        }, 
        {
            "location": "/advanced-topics/#hollow-heap-effects", 
            "text": "", 
            "title": "Hollow Heap Effects"
        }, 
        {
            "location": "/advanced-topics/#double-snapshots", 
            "text": "At times, a new state may be produced to which there is no available delta from the previous state.  When this  broken delta chain  scenario occurs, consumers will by default attempt to load a snapshot to the latest state.  If a consumer loads a snapshot when it currently has a state loaded for the same dataset, we call this a  double snapshot .  Double snapshots result in a doubling of the heap usage of your dataset.  This is because Hollow assumes that consumers may be actively using the current state, and it therefore must retain the current state to provide data while the next state is loaded.  Only after the next state is fully loaded can the old state be dropped.  If using the  Hollow Consumer , you can configure consumers to never attempt a double snapshot.  This is accomplished with a custom  DoubleSnapshotConfig .  In this case, you should check for 'stuck' clients in a  RefreshListener , so that you may take the appropriate operational action.  If a dataset is large, double snapshots should be avoided for performance reasons.  Double snapshots can be entirely avoided by  restoring  a  HollowWriteStateEngine  at producer initialization time, and, if necessary, using the  HollowStateDeltaPatcher  to operationally fill lost or missing deltas in the event an unforeseen issue occurs.", 
            "title": "Double Snapshots"
        }, 
        {
            "location": "/advanced-topics/#type-sharding", 
            "text": "During a delta transition each individual record type builds the entire next state of the type in memory while the current state is retained to answer requests.  This means that the memory pool Hollow reserves needs to be large enough to compose an extra copy of the largest type.  This is a miniaturized version of the  double snapshot  problem.  To address this problem, large types can be transparently sharded into smaller individual chunks, each of which updates independently.  The effect of this feature is that Hollow only needs to retain a large enough memory pool to update the largest  chunk  across all types.  Type sharding can be specified in two ways:     The recommended way is to specify a target max type shard size via a call to  setTargetMaxTypeShardSize(long bytes)  on a  HollowWriteStateEngine  prior to writing the first snapshot.  With this call, the number of shards will be automatically calculated based on the target excess memory pool size.  Additionally, the number of shards for a type can be explicitly specified by annotating a POJO with the  @HollowShardLargeType  annotation when using the  HollowObjectMapper  for data ingestion.  This can be useful if rapid growth is anticipated in a type.   Within a continuous delta chain, the type sharding configuration cannot be changed.  When a producer  restores  the previously produced state at startup, then the restored  HollowWriteStateEngine  will always retain the sharding configuration of the prior state rather than recalculating based on the current size of each type.  Consequently, if the changes in a dataset over time results in a type sharding configuration which is highly suboptimal, it is recommended to start a new delta chain, which may require a double snapshot on all consumers, a simultaneous restart of all consumers, or a new  blob namespace  to which consumers can migrate over a period of time.   Backwards Compatibility  Type sharding is new in v2.1.0.  Consumers can read blobs produced by producers v2.1.0 and later  as long as type sharding is disabled . \nIf you are sure that all consumers are using v2.1.0 or later, it is safe to turn on type sharding.  In order to avoid causing issues for early adopters, the default target max type shard size is currently set to  Long.MAX_VALUE .  At a later time, \nthis default will be changed to 25MB.", 
            "title": "Type Sharding"
        }, 
        {
            "location": "/advanced-topics/#object-longevity", 
            "text": "A Hollow object returned from a generated API contains a reference to the Hollow data store, and an ordinal.  For this reason, if a reference to a Hollow object is retained by the consumer for an extended period of time, and the underlying record changes unexpected results may begin to be returned from these references.  We call Hollow objects which were obtained from a no longer current state stale references.  It is best practice to  never  cache Hollow objects.  However, if it somehow  cannot  be guaranteed that Hollow Objects will never be cached at the consumer, and guaranteed protection against accidentally cached objects is necessary, then  object longevity  can be enabled.  With object longevity, Hollow objects will, after an update, be backed by a reserved copy of the data at the time the reference was created.  This guarantees that even if a reference is held for a long time, it will continue to return the same data when interrogated.  When object longevity is defined, Two durations are defined:    a grace period, and   a usage detection period.     The grace period is defined by its duration, in milliseconds, after a reference becomes stale.  During the grace period, usage of stale references is acceptable.  The usage detection period is defined by its duration, in milliseconds, after the grace period has expired.  During the usage detection period, usage of stale references is unexpected, but will not result in failed interrogations of Hollow objects.  After the usage detection period expires, data will be dropped if no usage was detected in the usage detection period.  If stale references are interrogated  after  their backing data is dropped, then Exceptions will be thrown.  The  ObjectLongevityConfig , injected into the  HollowConsumer  constructor, contains a few methods which are used to configure object longevity behavior:   boolean enableLongLivedObjectSupport() : Whether or not object longevity is enabled.  long gracePeriodMillis() : If object longevity is enabled, this returns the number of milliseconds before the usage of stale objects gets flagged.  long usageDetectionPeriodMillis() : If long-lived object support is enabled, this defines the number of milliseconds, after the grace period, during which data is still available in stale references, but usage will be flagged.  boolean dropDataAutomatically() : Returns whether or not to drop data behind stale references after the grace period + usage detection period has elapsed, as long as no usage was detected during the usage detection period.  This can be used to avoid memory leaks when long lived object support is enabled and stale references are cached, but unused.  boolean forceDropData() : Returns whether or not to drop data behind stale references after the grace period + usage detection period has elapsed, even if usage was detected during the usage detection period.  This can be used to avoid memory leaks when long lived object support is enabled and stale references are cached and used.   Your implementation of the  ObjectLongevityConfig  may be backed by a dynamic configuration and safely change on live running consumers.  For example,  forceDropData()  may be operationally useful for boxes that are exhibiting memory leaks due to non-critical cached Hollow objects.   How It Works  When enabled, object longevity is achieved using the  HollowHistory  data structure, which results in a minimal heap overhead.", 
            "title": "Object Longevity"
        }, 
        {
            "location": "/advanced-topics/#memory-pooling", 
            "text": "Hollow pools and reuses memory to minimize GC effects while updating data.  This pool of memory is kept arrays on the heap.  Each array in the pool has a fixed length.  When a long array or a byte array is required in Hollow, it will stitch together pooled array segments as a  SegmentedByteArray  or  SegmentedLongArray .  These classes encapsulate the details of treating segmented arrays as contiguous ranges of values.", 
            "title": "Memory Pooling"
        }, 
        {
            "location": "/advanced-topics/#delta-based-producer-input", 
            "text": "The  Getting Started  section of this documentation describes a producer which every so often reads the  entire dataset  from some source of truth, re-adds all records to a  HollowWriteStateEngine , then produces a delta based on the automatically discovered differences in the dataset since the prior cycle.  It is possible, however, that a producer may  receive  an incoming stream of events which directly indicate the changes to a dataset, obviating the need to scan through the entire source of truth and re-add the entire dataset on each cycle.  If desired, a  HollowWriteStateEngine \u2019s state can be explicitly modified, rather than recreated, each cycle.  We start such a cycle by re-adding all of the records from the previous cycle to the state engine:  HollowWriteStateEngine writeEngine = /// the state engine\n\nwriteEngine.prepareForNextCycle();\nwriteEngine.addAllObjectsFromPreviousCycle();  We\u2019ll also need an indexed  HollowReadStateEngine , which is updated in lock-step with the  HollowWriteStateEngine .  The index can be used to retrieve the ordinals of records to be replaced.  These ordinals can be removed from the  HollowWriteStateEngine :  HollowObjectMapper mapper = new HollowObjectMapper(writeEngine);\nHollowPrimaryKeyIndex idx = \n                       new HollowPrimaryKeyIndex(readEngine,  Movie ,  id );\n\nHollowTypeWriteState movieTypeState = writeEngine.getTypeState( Movie );\n\nList MovieUpdateEvent  eventBatch = /// a batch of events\n\nfor(MovieUpdateEvent event : eventBatch) {\n    int oldOrdinal = idx.getMatchingOrdinal(event.getMovie().getId());\n    movieTypeState.removeOrdinalFromThisCycle(oldOrdinal);\n    mapper.addObject(event.getMovie());\n}   Watch out for Duplicates  Be careful, this process assumes that no two events will have the same movie ID in the same cycle.  You'll want to dedup the  eventBatch .   This process may leave orphaned records around, since the call to  removeOrdinalFromThisCycle()  doesn\u2019t remove any referenced records.  To solve this, the  TransitiveSetTraverser  can be used:  List MovieUpdateEvent  eventBatch = /// a batch of events\n\n/// find the Movie ordinals to remove\nBitSet removedMovieOrdinals = new BitSet();\nfor(MovieUpdateEvent event : eventBatch) {\n \u00a0\u00a0\u00a0int oldOrdinal = idx.getMatchingOrdinal(event.getMovie().getId());\n \u00a0\u00a0\u00a0removedMovieOrdinals.set(oldOrdinal);\n}\n\n/// initially the removal selection includes just Movies\nMap String, BitSet  removeRecords = new HashMap ();\nremoveRecords.put( Movie , removedMovieOrdinals);\n\n/// expand the selection to include any records referenced by selected Movies\nTransitiveSetTraverser.addTransitiveMatches(readEngine, removeRecords);\n/// but don't include records which are also referenced by unselected movies\nTransitiveSetTraverser.removeReferencedOutsideClosure(readEngine, removeRecords);\n\n/// remove everything in the selection\nfor(Map.Entry String, BitSet  entry : removeRecords) {\n \u00a0\u00a0\u00a0HollowTypeWriteState typeState = writeEngine.getTypeState(entry.getKey());\n \u00a0\u00a0\u00a0int removeOrdinal = entry.getValue().nextSetBit(0);\n \u00a0\u00a0\u00a0while(removeOrdinal != -1) {\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0typeState.removeOrdinalFromThisCycle(removeOrdinal);\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0removeOrdinal = entry.getValue().nextSetBit(removeOrdinal + 1);\n \u00a0\u00a0\u00a0}\n}  In the above code, each of the  Movie  ordinals to be replaced are added to a  BitSet .  Then, the  TransitiveSetTraverser  is used to expand the collection of selected records by adding any records referenced by the selected  Movies .  Then, the  TransitiveSetTraverser  is again used to deselect any child records which are also referenced by other Movies which were not selected for removal.  Finally, the selection is actually removed from the  HollowWriteStateEngine .", 
            "title": "Delta-Based Producer Input"
        }, 
        {
            "location": "/advanced-topics/#in-memory-data-layout", 
            "text": "Each record in Hollow begins with a fixed-length number of bits.  At the lowest level, these bits are held in long arrays using the class  FixedLengthElementArray .  This class allows for storage and retrieval of fixed-length data in a range of bits.  For example, if a  FixedLengthElementArray  was queried for the 6-bit value starting at bit 7 in the following example range of bits:   The value  100100  in binary, or  36  in base 10, would be returned.", 
            "title": "In-Memory Data Layout"
        }, 
        {
            "location": "/advanced-topics/#object-layout", 
            "text": "An  OBJECT  record is a fixed set of strongly typed fields.  Each field is represented by a fixed-length number of bits.  Each record is represented by a fixed length number of bits equal to the sum of the bits required to represent each fields.  For each type, all fields of all records are packed into a single  FixedLengthElementArray .  No bookkeeping data structures are required to locate a record -- the start bit for each record can is located by simply multiplying the number of bits per record times the record\u2019s ordinal.   The number of bits used to represent a field which is one of the types ( INT ,  LONG ,  REFERENCE ) is exactly equal to the number of bits required to represent the maximum value contained in the field across all records.  The values for  INT  and  LONG  fields are represented using zig-zag encoding, so that smaller absolute values require fewer bits.  The values for  REFERENCE  fields are encoded as the referenced record\u2019s ordinal, which along with the referenced type (from the schema) is sufficient to identify and locate the referenced record.  32 bits are used to represent a  FLOAT , and 64 bits are used to represent a  DOUBLE .  STRING  and  BYTES  fields each get a separate byte array, into which the values for all records are packed.  The fixed-length value in these fields are offsets into the field\u2019s byte array where the record\u2019s value ends.  In order to determine the begin byte for the record with ordinal n, the offset encoded into the record with ordinal (n-1) is read.  The number of fixed length bits used to represent the offsets is exactly equal to the number of number of bits required to represent the maximum offset, plus one.  Each field type may be assigned a null value.  For  INT ,  LONG , and  REFERENCE  fields, null is encoded as a value with all ones.  For  FLOAT  and  DOUBLE  fields, null is encoded as special bit sequences.  For  STRING  and  BYTES  fields, null is encoded by setting a designated null bit at the beginning of each field, followed by the end offset of the last populated value for that field.", 
            "title": "Object Layout"
        }, 
        {
            "location": "/advanced-topics/#list-layout", 
            "text": "A  LIST  is an ordered collection of records of a specific type.   LIST  types are represented with two FixedLengthElementArrays.  We can refer to these arrays as the  offset array  and the  element array .  Each  LIST  type contains a single  element array  into which the references to elements for all records are packed.  References are encoded as the ordinals of the element records, which is sufficient to identify and locate the record.  Each reference is represented using a fixed number of bits equal to the number of bits required to represent the maximum referenced ordinal across all records.  Each record is represented with a contiguous range of elements in the  element array .    The  offset array  contains fixed-length offsets into the element array where the record\u2019s elements end.  In order to determine the begin element for the record with ordinal n, the end value for the element (n-1) is read.     Elements in a  LIST  record may not be null.", 
            "title": "List Layout"
        }, 
        {
            "location": "/advanced-topics/#set-layout", 
            "text": "A  SET  is an unordered collection of records of a specific type.  The records for  SET  elements are hashed into an open-addressed hash table.   SET  types are represented with two  FixedLengthElementArrays .  We can refer to these arrays as the  offset array  and the  bucket array .  Each  SET  element contains a single  bucket array  into which the references to elements for all records are packed.  Each record is represented with a contiguous range of buckets in the  bucket array .  The range of buckets for each record will contain an open-addressed hash table, with a linear probing hash collision resolution strategy.  The number of buckets for each record will be a power of two, and will be large enough such that all elements for the record will fit into those buckets with a load factor no greater than 70%.  Each bucket is represented using a fixed number of bits equal to the number of bits required to represent the maximum referenced ordinal across all records.  A populated bucket is encoded as the ordinal of the referenced record in that bucket.  To represent empty buckets, a sentinel value equal to all ones in binary is reserved.  The  offset array  contains two fixed-length fields per record:  the size of the set, and the offset to the bucket where the record\u2019s data ends.  Each of these fields is encoded using a fixed number of bits equal to the number of bits required to represent the field\u2019s maximum value across all records.   Elements in a  SET  record may not be null.", 
            "title": "Set Layout"
        }, 
        {
            "location": "/advanced-topics/#map-layout", 
            "text": "A  MAP  is an unordered collection of key/value pairs, where each key is a specific type, and each value type is a specific type.  The records for MAP elements are hashed into an open-addressed hash table.   MAP  types are represented with two  FixedLengthElementArrays .  We can refer to these arrays as the  offset array  and the  bucket array .  Each  MAP  type contains a single bucket array into which the references to keys and values for all records are packed.  Each record is represented with a contiguous range of buckets in the  bucket array .  The range of buckets for each record will contain an open-addressed hash table, with a linear probing hash collision resolution strategy.  The number of buckets for each record will be a power of two, and will be large enough such that all key/value pairs for the record will fit into those buckets with a load factor no greater than 70%.  Each bucket is represented using a fixed number of bits equal to the number of bits required to represent the maximum referenced key ordinal, plus the number of bits required to represent the maximum referenced value ordinal.  A populated bucket contains two fixed length fields: the first field contains the ordinal of the referenced key, and the second field contains the ordinal of the referenced value.  Empty buckets are represented with a key field containing a reserved sentinel value equal to all ones in binary.  The  offset array  contains two fixed-length fields per record:  the size of the map, and the offset to the bucket where the record\u2019s data ends.  Each of these fields is encoded using a fixed number of bits equal to the number of bits required to represent the field\u2019s maximum value across all records.   A  MAP  cannot contain null keys or values.", 
            "title": "Map Layout"
        }, 
        {
            "location": "/advanced-topics/#primary-key-index-layout", 
            "text": "A primary key index is a single  FixedLengthElementArray , which represents an open-addressed hash table of pointers to records of the given type.  The hash of each record is derived based on the fields designated in the primary key.  Each bucket in the hash table is represented using a fixed number of bits equal to the number of bits required to represent the maximum ordinal of the indexed type.  A populated bucket is encoded as the ordinal of the referenced record in that bucket.  To represent empty buckets, a sentinel value equal to all ones in binary is reserved.  When queried with a key, the index will hash the key, look in the corresponding bucket to find the ordinal of a record which also hashes to this key, then compare the referenced record\u2019s key to the query.  When a matching record is found, the ordinal at that bucket is returned as the match.  Collisions are resolved with linear probing.  If after linear probing an empty bucket is encountered, no such record exists in the dataset.", 
            "title": "Primary Key Index Layout"
        }, 
        {
            "location": "/advanced-topics/#hash-index-layout", 
            "text": "A hash index is uses two  FixedLengthElementArrays .  These arrays can be referred to as the  match array  and the  select array .  The  match array  is an open-addressed hash table and contains buckets.  Like a primary key index, the match array does not re-encode the values of keys.  Instead, it retains pointers to existing records which may be used to retrieve the hashed keys.  Unlike a primary key index, it may not be sufficient to retain a single pointer to the indexed type and extract the hashed key from that record -- each record in a hash index may match multiple keys.  Instead, we retain 1-n pointers.  Each pointer will indicate a record through which one or more of the key fields may be unambiguously retrieved.  If multiple key fields may be unambiguously traversed to via a single type, then only a single pointer for the field group will be retained per bucket.    Consider a scenario in which a hash index is used to index Movies by the nationalities and birth year of actors.  For the key  [\u201cBritish\u201d, 1972] , the corresponding entry in the match array may contain a pointer to the  Actor  record for Idris Elba.  Although the record points to a specific  Actor , the matching records for this entry will contain movies starring any British Actor born in 1972.  Each pointer field in the match array bucket references a specific type, and is encoded as the ordinal to which the bucket refers.  Each is represented using a fixed number of bits equal to the maximum ordinal in the referenced type.  In addition to pointers which allow us to look up the matching key, each bucket in the  match array  includes the number of matching records, and an offset into the  select array .  The  select array  contains lists of ordinals to matching records.  When queried with a key, the index will hash the key, then look to the corresponding bucket in the  match array .  The match pointers are used to compare the queried key with the matching key.  If a match is found, then the corresponding entries in the select array are returned as a  HollowHashIndexResult .  Collisions are resolved with linear probing.  If after linear probing an empty bucket is encountered in the match array, no such record exists in the dataset.", 
            "title": "Hash Index Layout"
        }, 
        {
            "location": "/glossary/", 
            "text": "adjacent state\n\n\n\n\nIf state \nA\n is connected via a single delta to state \nB\n, then \nA\n and \nB\n are adjacent to each other.\n\n\n\n\nannounce\n\n\n\n\nAfter the blobs for a state have been published to a blob store by a producer, the state must be \nannounced\n to consumers.  The announcement signals to consumers that they should transition to the announced state.\n\n\n\n\nblob\n\n\n\n\nA blob is a file used by consumers to update their dataset.  A blob will be either a snapshot, delta, or reverse delta\n\n\n\n\nblob store\n\n\n\n\nA blob store is a file store to which blobs can be published by a producer and retrieved by a consumer. \n\n\n\n\nbroken delta chain\n\n\n\n\nWhen a blob namespace contains a state which is not adjacent to any prior states, the delta chain is said to be broken.  In this scenario, consumers may need to load a double snapshot.\n\n\n\n\nconsumer\n\n\n\n\nOne of many machines on which a dataset is made accessible.  Consumers are updated in lock-step based on the actions of the producer.\n\n\n\n\ncycle\n\n\n\n\nA producer runs in an infinite loop.  Each exection of the loop is called a cycle.  Each cycle produces a single data state.\n\n\n\n\ndata model\n\n\n\n\nA data model defines the structure of a dataset.  It is specified with a set of schemas.\n\n\n\n\ndata state\n\n\n\n\nA dataset changes over time.  The timeline for a changing dataset can be broken down into discrete data states, each of which is a complete snapshot of the data at a particular point in time.\n\n\n\n\ndeduplication\n\n\n\n\nTwo records which have identical data in Hollow will be consolidated into a single record.  Any references to duplicate records will be mapped to the canonical one when a dataset is represented with Hollow.\n\n\n\n\ndelta\n\n\n\n\nA set of encoded instructions to transition from one data state to an adjacent state.  Deltas are encoded as a set of ordinals to remove and a set of ordinals to add, along with the accompanying data to add.  'Delta' may refer specifically to a transition between an earlier state and a later state, contrasted with 'reverse delta', which specifically refers to a transition between a later state and an earlier state.\n\n\n\n\ndelta chain\n\n\n\n\nA series of states which are all connected via contiguous deltas.\n\n\n\n\ndiff\n\n\n\n\nA comprehensive accounting for the differences between two data states.\n\n\n\n\ndouble snapshot\n\n\n\n\nWhen a consumer already has an initialized state and an announcement signals to move to a new state for which a path of deltas is not available, the consumer may transition to that state via a snapshot.  In this scenario two full copies of the dataset must be loaded in memory.\n\n\n\n\nfield\n\n\n\n\nA single value encoded inside of a Hollow record.\n\n\n\n\nhash key\n\n\n\n\nA user-defined specification of one or more fields used to hash elements into a set or entries into a map.\n\n\n\n\ningestion\n\n\n\n\nGathering data from a source of truth and importing it into Hollow.\n\n\n\n\ninline\n\n\n\n\nA field for which the value is encoded directly into a record, as opposed to referenced via another record.\n\n\n\n\nnamespace (blobs)\n\n\n\n\nAn addressable, logical separation of both published artifacts in a blob store and announcement location.  Used to allow multiple publishers to communicate on separate channels to specific groups of consumers.\n\n\n\n\nnamespace (references)\n\n\n\n\nThe deliberate creation of a type to hold a specific referenced field's data in order to reduce the cardinality of the referenced records.\n\n\n\n\nobject longevity\n\n\n\n\nA technique used to ensure that stale references to Hollow Objects always return the same data they did initially upon creation.  Configured via the \nHollowObjectMemoryConfig\n.\n\n\n\n\nordinal\n\n\n\n\nAn integer value uniquely identifying a record within a type.  Because records are represented with a fixed-length number of bits, the only necessary information to locate a record in memory is the record's type and ordinal.  Ordinals are automatically assigned by Hollow, and are recycled as records are removed and added.  Consequently, they lie in the range of 0-n, where n is generally not much larger than the total number of records for the type.\n\n\n\n\npatch (states)\n\n\n\n\nCreating a series of two deltas between states in a delta chain.\n\n\n\n\npinning\n\n\n\n\nOverriding the state version announcement from the producer, to force clients to go back to or stay at an older state.\n\n\n\n\nprimary key\n\n\n\n\nA user-defined specification of one or more fields used to uniquely identify a record within a type.\n\n\n\n\nproducer\n\n\n\n\nA single machine that retrieves all data from a source of truth and produces a delta chain.\n\n\n\n\npublish\n\n\n\n\nWriting blobs to a blob store.\n\n\n\n\nread state engine\n\n\n\n\nA \nHollowReadStateEngine\n, the root handle to a Hollow dataset as a consumer.\n\n\n\n\nrecord\n\n\n\n\nA strongly-typed collection of fields or references, the structure of which is specified by a schema.\n\n\n\n\nreference\n\n\n\n\nA field type which indicates a pointer to another field.  Can also refer to the technique of pulling out a specific field into a record type of its own to deliberately allow Hollow to deduplicate the values.\n\n\n\n\nrestore\n\n\n\n\nInitializing a \nHollowWriteStateEngine\n with data from a previously produced state so that a delta may be created during a producer's first cycle.\n\n\n\n\nreverse delta\n\n\n\n\nA delta from a later state to an earlier state.  Generally used during pinning scenarios.\n\n\n\n\nschema\n\n\n\n\nMetadata about a Hollow type which defines the structure of the records.\n\n\n\n\nsnapshot\n\n\n\n\nA blob type which contains a serialization of all of the records in a type.  Consumed during initialization, and possibly in a broken delta chain scenario.\n\n\n\n\nstate\n\n\n\n\nSee \ndata state\n.\n\n\n\n\nstate version\n\n\n\n\nA unique identifier for a state.  Should by monotonically increasing as time passes.\n\n\n\n\nstate engine\n\n\n\n\nBoth the producer and consumers handle datasets with a state engine.  A state engine can be transitioned between data states.  A producer uses a \nwrite state engine\n and a consumer uses a \nread state engine\n\n\n\n\ntype\n\n\n\n\nA collection of records all conforming to a specific schema.\n\n\n\n\nwrite state engine\n\n\n\n\nA \nHollowWriteStateEngine\n, the root handle to a Hollow dataset as a consumer.", 
            "title": "Glossary"
        }, 
        {
            "location": "/community/", 
            "text": "Getting Support\n\n\nFor bug reports and feature requests, please file a \nGitHub issue\n.  \n\n\nIf you have a question that isn't covered in this documentation, please reach out for help either on Stack Overflow or \nGitter\n\n\nStack Overflow\n\n\nThe Platform Data Technologies team at Netflix will monitor posts tagged with \nhollow\n.\n\n\nGitter\n\n\nThe Platform Data Technologies team at Netflix is often available for chat via \nGitter\n.  We hope that you'll stick around and pay it forward by answering other users' questions when they arise.\n\n\nContributing to Hollow\n\n\nWe'll gladly review and accept pull requests for Hollow.  If you want to have a design discussion for your changes, please reach out to us on Gitter.\n\n\nBackwards Compatibility\n\n\nNew features in Hollow should always be added in a way that is backwards compatible, except in \nextremely\n rare cases when a major version is released.\n\n\nIf you would like to make a contribution which breaks backwards compatibility, please contact us so we can evaluate alternate ways to achieve the desired result, and/or whether to schedule the change for an upcoming major version release.\n\n\nDependencies\n\n\nThe core project \nhollow\n should have zero \ncompile\n dependencies, and should only depend on one library (jUnit) as a \ntest\n dependency.  We believe this provides long-term stability for users, reduces licensing concerns, and eliminates the possibility that \nother\n project dependencies will be compiled against incompatible versions of dependent libraries.\n\n\nIf you would like to make a contribution which requires a third-party dependency, please contact us before proceeding so we can discuss the appropriate location for the addition.\n\n\nsun.misc.Unsafe\n\n\nThe core project \nhollow\n utilizes \nsun.misc.Unsafe\n. Your IDE may treat this as an error. See \nIssue #5\n for how to compile without errors.", 
            "title": "Community"
        }, 
        {
            "location": "/community/#getting-support", 
            "text": "For bug reports and feature requests, please file a  GitHub issue .    If you have a question that isn't covered in this documentation, please reach out for help either on Stack Overflow or  Gitter", 
            "title": "Getting Support"
        }, 
        {
            "location": "/community/#stack-overflow", 
            "text": "The Platform Data Technologies team at Netflix will monitor posts tagged with  hollow .", 
            "title": "Stack Overflow"
        }, 
        {
            "location": "/community/#gitter", 
            "text": "The Platform Data Technologies team at Netflix is often available for chat via  Gitter .  We hope that you'll stick around and pay it forward by answering other users' questions when they arise.", 
            "title": "Gitter"
        }, 
        {
            "location": "/community/#contributing-to-hollow", 
            "text": "We'll gladly review and accept pull requests for Hollow.  If you want to have a design discussion for your changes, please reach out to us on Gitter.", 
            "title": "Contributing to Hollow"
        }, 
        {
            "location": "/community/#backwards-compatibility", 
            "text": "New features in Hollow should always be added in a way that is backwards compatible, except in  extremely  rare cases when a major version is released.  If you would like to make a contribution which breaks backwards compatibility, please contact us so we can evaluate alternate ways to achieve the desired result, and/or whether to schedule the change for an upcoming major version release.", 
            "title": "Backwards Compatibility"
        }, 
        {
            "location": "/community/#dependencies", 
            "text": "The core project  hollow  should have zero  compile  dependencies, and should only depend on one library (jUnit) as a  test  dependency.  We believe this provides long-term stability for users, reduces licensing concerns, and eliminates the possibility that  other  project dependencies will be compiled against incompatible versions of dependent libraries.  If you would like to make a contribution which requires a third-party dependency, please contact us before proceeding so we can discuss the appropriate location for the addition.", 
            "title": "Dependencies"
        }, 
        {
            "location": "/community/#sunmiscunsafe", 
            "text": "The core project  hollow  utilizes  sun.misc.Unsafe . Your IDE may treat this as an error. See  Issue #5  for how to compile without errors.", 
            "title": "sun.misc.Unsafe"
        }, 
        {
            "location": "/acknowledgements/", 
            "text": "Hollow is originally created by Drew Koszewnik with the advice and support of fellow members of the Data Platform Technologies team at Netflix:\n\n\n\n\nDavid Su\n\n\nDeva Jayaraman\n\n\nJatin Shah\n\n\nKinesh Satiya\n\n\nLavanya Kanchanapalli\n\n\nRamin Forood\n\n\nRohit Kaul\n\n\nTim Taylor\n\n\n\n\nHollow is maintained by the Data Platform Technologies team at Netflix.\n\n\nHollow makes use of an \nimplementation\n of the MurmurHash3 algorithm authored by Yonik Seeley.\n\n\nHollow makes use of Thomas Wang's well known 32-bit mix function, which is based on an original suggestion by Robert Jenkins.\n\n\nThe \nhollow-diff-ui\n object diff view was inspired in part by Chas Emerick's \njsdifflib\n, and borrows some of the .css from that project.\n\n\nThis documentation was created with \nMkDocs\n, using a modified theme from \nreadthedocs\n.", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/license/", 
            "text": "Copyright 2016 Netflix, Inc.\n\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.", 
            "title": "License"
        }
    ]
}